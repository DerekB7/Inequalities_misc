{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex 1.49 in Korner's *A Companion to Analysis*  (page 21)  \n",
    "part (ii) is of considerable interest    \n",
    "because it allows us recover mean value inequality based on using compactness to prove an increasing function...   \n",
    "\n",
    "In terms of technique, the approach used here reminds your author of the technique used in ex 6.10 of *Cauchy Schwarz Masterclass*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i)  \n",
    "in some open set $(\\alpha, \\beta) \\in \\mathbb R$, supposing $K\\geq 0$ and that $a,b \\in U$ with $b \\gt a$ if \n",
    "$f: U\\longrightarrow \\mathbb R$ is differentiable with $f'(t) \\leq K$ for all $t \\in U$, then  \n",
    "\n",
    "$f(b) - f(a) \\leq (b-a)K$  \n",
    "**this is the mean value inequality**  \n",
    "\n",
    "\n",
    "(ii)  \n",
    "we are to show that the mean value inequality is implied by  \n",
    "in some open set $(\\alpha, \\beta) \\in \\mathbb R$, supposing $K\\geq 0$ and that $a,b \\in U$ with $b \\gt a$ if \n",
    "$g: U\\longrightarrow \\mathbb R$ is differentiable with $f'(t) \\geq 0 $ for all $t \\in U$, then\n",
    "\n",
    "$0 \\leq g(b) - g(a) $  \n",
    "This result can be proven independently via compactness / extreme value theorem arguments (see below cells).  \n",
    "\n",
    "that is, if we select \n",
    "\n",
    "$g(t):= Kt - f(t)$  \n",
    "observing that  \n",
    "$g'(t):= K - f'(t)\\geq 0$  \n",
    "where as before we assume $f'(t) \\leq K$  \n",
    "hence we know  \n",
    "$0 \\leq g(b) - g(a) $  holds  \n",
    "\n",
    "then compute the difference:  \n",
    "$0 \\leq g(b) -g(a)= Kb - f(b) - \\big( Ka - f(a)\\big)  = K(b-a) - f(b) + f(a) $  \n",
    "or  \n",
    "$f(b) - f(a) \\leq  K(b-a) $  \n",
    "\n",
    "- - - - -  \n",
    "a *slightly* different approach would be to make a nicer argument using a strict inequality, so for any $\\epsilon \\gt 0$  \n",
    "and any $K \\gt 0$  \n",
    "\n",
    "$g(t):= (1+\\epsilon) Kt - f(t)$  \n",
    "which implies $g'(t) \\gt 0$  \n",
    "and hence  \n",
    "\n",
    "$0 \\lt g(b) -g(a)= (1+\\epsilon) K(b-a) - f(b) + f(a) $  \n",
    "\n",
    "or  \n",
    "\n",
    "$f(b) - f(a) \\lt (1+\\epsilon) K(b-a)  $  \n",
    "or  \n",
    "$f(b) - f(a) \\leq K(b-a)  $  \n",
    "as desired  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof that positive first deriviative implies increasing function**  \n",
    "\n",
    "since $\\alpha \\lt a \\lt b \\lt \\beta$  \n",
    "and the function $g$ is differentiable in this open set, with strictly positive derivative, it is enough to consider the mapping  \n",
    "\n",
    "$g: \\big[a,b\\big] \\longrightarrow [\\text{minimum}, \\text{maximum}]$  \n",
    "\n",
    "our interest is in the minimum value taken on by the function over this closed interval.  \n",
    "\n",
    "It's enough to consider two cases:  either\n",
    "\n",
    "$\\text{(1)} g(a) \\lt g(x)$ for $x \\in (a,b]$  i.e. the left boundary is mapped to the the unique minimum in this closed set   \n",
    "$\\text{(2)}$ there is (at least one) $x^* \\in(a,b]$ where $ g(x^*) \\leq g(x)$ for all $x \\in [a,b]$. \n",
    "\n",
    "In the case of $\\text{(i)}$ everything works as desired.  \n",
    "\n",
    "But $\\text{(2)}$ isn't true so we can proceed by contradiction:  \n",
    "Consider the left difference quotients at $g(x^*)$ we know they converge to $g'(x^*) = c \\gt 0$  \n",
    "From here select select $\\epsilon := \\frac{c}{2}$ and we find a contradiction if $g(x^*)$ is a minimum because for any (sufficientlty small) $\\delta \\gt 0$  \n",
    "\n",
    "$\\frac{g(x^* - \\delta) - g(x^*)}{(x^* - \\delta) - x^*)}= \\frac{ g(x^*)- g(x^* - \\delta) }{ \\delta)} \\leq  \\frac{ 0 }{ \\delta}=0  $  \n",
    "\n",
    "i.e. it can never be in the required neighborhood of \n",
    "\n",
    "$\\big(\\frac{1}{2}c, \\frac{3}{2}c\\big)$  \n",
    "\n",
    "and hence \n",
    "\n",
    "$g'(x^*) = \\lim_{x\\to (x^*)^{-}} \\frac{g(x)- g(x^*)}{x-x^*}$  \n",
    "does not exist.  But if the left limit does not exist, then the function is not differentiable at that point, which contradicts the fact that $g$ is differentiable in $(\\alpha, \\beta)$  \n",
    "\n",
    "\n",
    "\n",
    "and hence the sequence doesn't converge and the derivative does not exist -- a contradiction.  \n",
    "\n",
    "For avoidance of doubt:  the fact the $g$ is differentiable implies it is continuous, which combined with compactness guarantees a minimum value of $g$ in this $[a,b]$, thus we must have $\\text{(2)}$ \n",
    "\n",
    "note that the choices of $a,b$ were arbitrary so we may infer that for any $x_1, x_2 \\in (\\alpha, \\beta)$, if $x_1 \\lt x_2$ then $g(x_1) \\lt g(x_2)$ -- i.e. $g$ is strictly increasing whenever its derivative is positive.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof that non-negative first deriviative implies non-decreasing function**  \n",
    "\n",
    "To accomodate the weaker case of $g'(t)\\geq 0$ over $(\\alpha, \\beta)$ again consider $\\alpha \\lt a \\lt b \\lt \\beta$  \n",
    "\n",
    "$g: \\big[a,b\\big] \\longrightarrow [\\text{minimum}, \\text{maximum}]$  \n",
    "\n",
    "our interest is in the minimum value taken on by the function over this closed interval.  \n",
    "\n",
    "This time consider \n",
    "\n",
    "$\\text{(1)} g(a) \\leq g(x)$ for $x \\in (a,b]$  i.e. the left boundary is mapped to the the not necessarily unique minimum in this closed set   \n",
    "$\\text{(2)}$ there is (at least one) $x^* \\in(a,b]$ where $g(x^*) \\lt g(a)$ for all $x \\in [a,b]$, or equivalently  \n",
    "$\\gamma = \\text{minimum} \\lt g(a)$  \n",
    "\n",
    "as before there is no problem with $\\text{(1)}$ but $\\text{(2)}$ is wrong so proceed by contradiction.\n",
    "define  \n",
    "$x^* := \\text{inf x:   } g(x) = \\gamma = \\text{min x :  } g(x) = \\gamma$.  \n",
    "\n",
    "Note that the infimum must exist in $(a,b]$ but by continuity the preimage of points mapped to $\\gamma$ must have a minimum (i.e. left closed interval that is a lower bound -- assuming othereise results in a contradiction).  (There is a deeper topological point that $\\{\\gamma\\}$ is a closed set, and since our function is continuous, the pre-image is closed though we don't need this level of generality.)   \n",
    "\n",
    "However if we compute the left derivative at $x^*$ we have \n",
    "\n",
    "for any sufficiently small $\\delta \\gt 0$  \n",
    "$\\frac{g(x^*-\\delta) -g(x^*)}{-\\delta}  = \\frac{g(x^*) - g(x^*-\\delta)}{\\delta}\\lt   \\frac{0}{\\delta} = 0$  \n",
    "which contradicts the fact that the limit exists and $g(x^*) \\geq 0$  \n",
    "\n",
    "Thus as before, since we know a minimum value exists (continuous functions map compact sets to compact sets afterall) so  $(1)$ must be true, and in particular $g(a) \\leq g(b)$, and as above we recall that the selection of $a\\lt b$ was arbitrary hence the function must be non-decreasing over $U$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice **corollary** given on page 20 of Koerner -- The Constant Value Theorem-- is that if $U:= (\\alpha, \\beta)$ and  \n",
    "$f: U \\to \\mathbb R$ is differentiable and $f'(t) = 0$ for all $t \\in U$ then $f$ is constant.  \n",
    "\n",
    "in particular we apply mean value inequality one way to see, with $K=0$, for $a, b \\in U$ and $b \\gt a$  \n",
    "\n",
    "$f(b) - f(a) \\leq K(b-a) = 0(b-a) = 0$  \n",
    "or  \n",
    "\n",
    "$f(b) \\leq f(a)$  \n",
    "\n",
    "and we also apply it to $-f$, which still has the constant derivative equal to zero, to get  \n",
    "\n",
    "$f(a)  -f(b)  -f(b) - -f(a) \\leq K(b-a) = 0(b-a) = 0$  \n",
    "or \n",
    "$f(a) \\leq f(b)$  \n",
    "\n",
    "hence $f(a) = f(b)$  \n",
    "\n",
    "but the selection of $a,b$ was arbitrary hence we see that any and all 2 points in $U$ must have the same value under the image of $f$.  \n",
    "- - - - -  \n",
    "Another **corollary** is that  \n",
    "if two differentiable functions $f$, $g$ have the same derivative at all points in $U$ then $f(x) = g(x) + c$.  The proof comes from considering the function $h(x) = f(x) - g(x)$ which has a derivative of zero everywhere in $U$.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the complex variables case of ex 11.67  (i.e. with a holomorphic function on a path connected set)  \n",
    "\n",
    "we select $\\theta \\in [-\\pi, \\pi)$ such that \n",
    "$e^{i\\theta} \\big(f(z_2) - f(z_1)\\big) \\geq 0$  \n",
    "\n",
    "if it is zero there is nothing to do, select $\\theta := 0$ otherwise write $\\big(f(z_2) - f(z_1)\\big)$ in polar form, divide out the magnitude and conjugate -- the result is $e^{ia}$ and set $\\theta = a$ (and take principal value for convenience/ standardization),  \n",
    "\n",
    "now the *very* cleverly chosen function is \n",
    "\n",
    "$F(t) = \\mathbb R\\Big(e^{i\\theta}\\big\\{f\\big((1-t)z_1 + t z_2\\big) - f\\big(z_2\\big)\\big\\}\\Big)$  \n",
    "for $t \\in [0,1]$ -- though it may be convenient to make this an open set and extend this to $t \\in (-\\epsilon, 1+\\epsilon)$\n",
    "\n",
    "note that (where we evaluate the right difference quotient for convenience)   \n",
    "$F'(t) $  \n",
    "$= \\frac{d}{dt} \\mathbb R\\Big(e^{i\\theta}\\big\\{f\\big((1-t)z_1 + t z_2\\big) - f\\big(z_2\\big)\\big\\}\\Big)$  \n",
    "$=\\lim_{\\delta \\to 0^+} \\mathbb R\\Big(\\frac{e^{i\\theta}\\big\\{f\\big((1-t-\\delta)z_1 + (t+\\delta z_2\\big) - f\\big(z_2\\big)\\big\\} - e^{i\\theta}\\big\\{f\\big((1-t)z_1 + t z_2\\big) - f\\big(z_2\\big)\\big\\} }{\\delta}\\Big)$  \n",
    "$= \\lim_{\\delta \\to 0^+} \\mathbb R\\Big(\\frac{e^{i\\theta}\\big\\{f\\big((1-t-\\delta)z_1 + (t+\\delta z_2\\big) -f\\big((1-t)z_1 + t z_2\\big) \\big\\} }{\\delta}\\Big)$  \n",
    "$= \\mathbb R\\Big(e^{i\\theta} f'\\big((1-t)z_1 + t z_2\\big)\\cdot \\big(z_2 - z_1\\big)\\Big)$   \n",
    "by application of the chain rule  \n",
    "\n",
    "so we have  \n",
    "$\\mathbb R\\Big(e^{i\\theta} f'\\big((1-t)z_1 + t z_2\\big)\\cdot \\big(z_2 - z_1\\big)\\Big) $  \n",
    "$= F'(t) $  \n",
    "$\\leq \\big \\vert \\Big(e^{i\\theta} f'\\big((1-t)z_1 + t z_2\\big)\\cdot \\big(z_2 - z_1\\big)\\Big)\\big \\vert $  \n",
    "$= \\big \\vert f'\\big((1-t)z_1 + t z_2\\big)\\big \\vert \\cdot \\big \\vert z_2 - z_1 \\big \\vert $  \n",
    "$\\leq K \\cdot \\big \\vert z_2 - z_1 \\big \\vert $  \n",
    "\n",
    "where use the fact that for any complex number given as $(a+bi)$ we have  \n",
    "$a = \\mathbb R\\big(a + bi\\big) \\leq \\big \\vert a\\big \\vert  =  \\big(\\big \\vert a\\big \\vert^2\\big)^\\frac{1}{2} \\leq \\big(\\big \\vert a\\big \\vert^2 + \\big \\vert b\\big \\vert^2 \\big)^\\frac{1}{2} = \\big \\vert a + bi\\big \\vert $  \n",
    "\n",
    "because  \n",
    "$\\big \\vert a\\big \\vert^2 \\leq \\big \\vert a\\big \\vert^2 + \\big \\vert b\\big \\vert^2 $   \n",
    "(and taking square roots gives the result)  \n",
    "\n",
    "hence the $F$ is real valued and its derivative is bounded above by a real non-negative constant  \n",
    "- - - - -   \n",
    "But now we apply mean value inequality to $F(t)$  \n",
    "\n",
    "$= \\Big \\vert f\\big(z_2\\big) - f\\big(z_1 \\big)\\Big \\vert$    \n",
    "$= e^{i\\theta}\\Big(f\\big(z_2\\big) - f\\big(z_1 \\big)\\Big)$    \n",
    "$= 0 - \\mathbb R\\Big(e^{i\\theta}f\\big(z_1 \\big) - f\\big(z_2\\big)\\Big)$  \n",
    "$= \\mathbb R\\Big(e^{i\\theta}f\\big((1-1)z_1 + 1 z_2\\big) - f\\big(z_2\\big)\\Big) - \\mathbb R\\Big(e^{i\\theta}f\\big((1-0)z_1 + 0 z_2\\big) - f\\big(z_2\\big)\\Big)$  \n",
    "$=F(1) -F(0) $   \n",
    "$\\leq \\big(1-0\\big) K \\cdot \\big \\vert z_2 - z_1 \\big \\vert$  \n",
    "$ = K \\cdot \\big \\vert z_2 - z_1 \\big \\vert$  \n",
    "\n",
    "as desired  \n",
    "\n",
    "This is useful, e.g. in proving that if a path connected open set in $\\mathbb C$ has zero derivative, then all points are constant in value.  The point is immediate for an open ball or other nice structure.  The one ugly bit is that this approach can only directly be called upon for points that are connected by a polygonal path (and the bounds then apply to each line segment on the path).  As noted on page 16 of Sasane and Sasane, this polygon path restriction may be relaxed to mere path connectivity as this 'more general' set coincides with the polygonal path definition -- though the fine points showing this equivalence are rather subtle and sophisticated, not known to your author.  \n",
    "\n",
    "\n",
    "*remark:*  There are some subtle techniques at play here.  It seems that the main inequality is an example of using polarization.  The actual technique used is reminiscent of page 66 of CS Masterclass.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
