{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "note: the bottom of my notes on H&ouml;lder's Inequality (Chp 9) revisits and re-proves the power mean inequality. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Power Means in Disguise:\n",
    "\n",
    "where $x_1, x_2, x_3 \\gt 0$  \n",
    "\n",
    "**part a) claim: **  \n",
    "$\\frac{9}{2(x_1 + x_2 + x_3)}\\leq \\frac{1}{x_1+x_2} + \\frac{1}{x_2+x_3} + \\frac{1}{x_3+x_1}  $\n",
    "\n",
    "**proof: **  \n",
    "the above is equivalent to \n",
    "\n",
    "$\\frac{1}{\\frac{1}{3}2(x_1 + x_2 + x_3)}\\leq \\frac{1}{3}\\Big(\\frac{1}{x_1+x_2} + \\frac{1}{x_2+x_3} + \\frac{1}{x_3+x_1}\\Big)  $\n",
    "\n",
    "change of variables:   \n",
    "$a = x_1 + x_2$  \n",
    "$b = x_2 + x_3$  \n",
    "$c = x_3 + x_1$   \n",
    "\n",
    "$\\frac{1}{\\frac{1}{3}(a + b + c)}\\leq \\frac{1}{3}\\Big(\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c}\\Big)  $\n",
    "\n",
    "which is the harmonic mean bound given under 8.16 (p. 126)\n",
    "\n",
    "\n",
    "**part b) claim: ** \n",
    "\n",
    "$\\frac{1}{2}3^{2-r}(x_1 + x_2 + x_3)^{r-1} \\leq \\Big( \\frac{x_1^r}{x_2 + x_3} + \\frac{x_2^r}{x_3 + x_1} + \\frac{x_3^r}{x_1 + x_2}\\Big)$  \n",
    "\n",
    "for real $r \\geq 1$  \n",
    "\n",
    "**comment 1: ** The book points out that if $r =1 $ we have Nesbitt's Inequality, which was exercise 5.6.  \n",
    "\n",
    "\n",
    "**comment 2: ** The official solution may perhaps be better practice for this chapter but was subtle and not intuitive to your author.  It proceeds by first applying Chebyshev's Inequality (Consequences of Order), and then applies a power mean inequality to lowerbound the LHS from Chebyshev, and part a) to lower bound the other part of the LHS.  The approach shown below makes use of convexity.  \n",
    "\n",
    "\n",
    "**proof:** \n",
    "- - - -\n",
    "To proceed, assume $x_1 + x_2 + x_3 = 1$.  note if this was not true, we could have $\\frac{1}{x_1 + x_2 + x_3} = \\gamma\\gt 0$, and multiply each side by $\\gamma^{r-1}$ giving us the below expression:  \n",
    "\n",
    "$\\frac{1}{2}3^{2-r}(x_1 + x_2 + x_3)^{r-1}\\gamma^{r-1} = \\frac{1}{2}3^{2-r}(\\gamma x_1 + \\gamma x_2 + \\gamma x_3)^{r-1} \\leq \\gamma^{r-1}\\Big( \\frac{x_1^r}{x_2 + x_3} + \\frac{x_2^r}{x_3 + x_1} + \\frac{x_3^r}{x_1 + x_2}\\Big) = \\Big( \\frac{(\\gamma x_1)^r}{\\gamma(x_2 + x_3)} + \\frac{(\\gamma x_2)^r}{\\gamma(x_3 + x_1)} + \\frac{(\\gamma x_3)^r}{\\gamma(x_1 + x_2)}\\Big)$  \n",
    "\n",
    "so instead of $x_1, x_2, x_3$ we'd work with $(\\gamma x_1), (\\gamma x_2), (\\gamma x_3)$, which do sum to one, and have no change to the inequality.  Thus for the rest of the proof, we proceed assuming $x_1 + x_2 + x_3 = 1$, without loss of generality.  Consequently it is enough to prove:\n",
    "\n",
    "$\\frac{1}{2}3^{2-r} = \\frac{1}{2}3^{2-r}(1)^{r-1} = \\frac{1}{2}3^{2-r}(x_1 + x_2 + x_3)^{r-1} \\leq \\Big( \\frac{x_1^r}{x_2 + x_3} + \\frac{x_2^r}{x_3 + x_1} + \\frac{x_3^r}{x_1 + x_2}\\Big)$  \n",
    "- - - \n",
    "of course, since $x_i + x_j + x_k = 1$ we also know that\n",
    "\n",
    "$1 - x_i = x_j + x_k$, and so on.  \n",
    "\n",
    "now consider the mapping \n",
    "\n",
    "$f(u) = \\frac{u^s}{1-u}$  \n",
    "\n",
    "This mapping is convex over our domain, as confirmed by second derivative test that follows:  \n",
    "\n",
    "a direct, easy approach in this instance is:  \n",
    "since we are dealing with $u \\in (0,1)$, we are in the radius of convergence for the geometric series.  \n",
    "\n",
    "Thus we have \n",
    "$f(u) = u^s\\big( 1 + u + u^2 + u^3 + ...) = u^s + u^{s+1} + u^{s+2} + u^{s+3}+ ...$\n",
    "\n",
    "where each term is strictly positive.  After term by term differentiation, by application of the power rule, each term is *still* real non-negative.  And after differentiation once more, we *still* have strictly real non-negative terms in a series that converges -- and generally speaking these terms are are strictly postive, though there could be one zero in the infinte series if $s=1$.  This means that the second derivative is strictly positive over our domain.    \n",
    "\n",
    "\n",
    "From here we apply Jensen's Inequality (via the uniform / J convex case), i.e. \n",
    "\n",
    "$ \\frac{1}{2}3^{1-r} =  \\frac{1}{2}\\big(\\frac{1}{3}\\big)^{r-1}= \\frac{1}{2}\\Big(\\frac{\\big(\\frac{1}{3}\\big)^r}{\\frac{1}{3}}\\Big) = \\Big(\\frac{\\big(\\frac{1}{3}\\big)^r}{\\frac{2}{3}}\\Big)= f\\big(\\frac{1}{3}\\big) =f\\big(\\frac{1}{3}(x_1 + x_2 + x_3)\\big) = f\\big(E[X]\\big) \\leq E\\big[f(X)\\big] = \\frac{1}{3}\\Big( \\frac{x_1^r}{1-x_1} + \\frac{x_2^r}{1 - x_2} + \\frac{x_3^r}{1 - x_3 }\\Big)$\n",
    "\n",
    "we may multiply each side by 3 and get, \n",
    "\n",
    "$ \\frac{1}{2}3^{2-r}= \\frac{x_1^r}{1-x_1} + \\frac{x_2^r}{1 - x_2} + \\frac{x_3^r}{1 - x_3 } = \\frac{x_1^r}{x_2 + x_3} + \\frac{x_2^r}{x_3 + x_1} + \\frac{x_3^r}{x_1 + x_2}$\n",
    "\n",
    "which completes the proof.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 8.2 Harmonic Means and Recognizable Sums\n",
    "\n",
    "\n",
    "where $x_1, x_2, ..., x_n \\gt 0$ and $s = \\sum_{i=1}^n x_i$ \n",
    "\n",
    "**claim: **\n",
    "$\\frac{n^2}{2n-1} \\leq \\frac{s}{s - x_1} + \\frac{s}{s - x_2} + ... + \\frac{s}{s - x_n}$  \n",
    "\n",
    "**proof 1: via Jensen's Inequality**   \n",
    "*note this was a direct an intuitive approach to me*  \n",
    "\n",
    "first multiply each side by $\\frac{1}{n}$\n",
    "\n",
    "$\\frac{n}{2n-1} \\leq s \\Big(\\frac{1}{n} \\big(\\frac{1}{s - x_1} + \\frac{1}{s - x_2} + ... + \\frac{1}{s - x_n}\\big)\\Big)$  \n",
    "\n",
    "now consider  \n",
    "$f(u) = \\frac{1}{2c - u}$, for any $u\\gt 0$, for any $c \\gt u$  \n",
    "$f''(u) = \\frac{8}{(2c - u)^3} \\gt 0$  \n",
    "\n",
    "(in particular we are interesting in $c :=s$... we could perhaps more carefully approach this using $\\frac{1}{1 - v}$ where $v:=\\frac{x_i }{s}$, though the result is the same)\n",
    "\n",
    "This second derivative test confirms that $f$ is convex over our domain\n",
    "\n",
    "Application of Jensen's Inequality gives us: \n",
    "\n",
    "$\\frac{1}{s}\\frac{n}{2n-1} = \\frac{1}{s}\\frac{1}{ \\frac{2n-1}{n} } = \\frac{1}{s}\\frac{1}{2 - \\frac{1}{n}} = \\frac{1}{2s - \\frac{s}{n}} =  f\\big(E[X]\\big) \\leq E\\big[f(X)\\big] = \\frac{1}{n} \\big(\\frac{1}{s - x_1} + \\frac{1}{s - x_2} + ... + \\frac{1}{s - x_n}\\big)$\n",
    "\n",
    "now multiply the LHS and RHS by $sn$, and we get \n",
    "\n",
    "$\\frac{n^2}{2n-1} = sn\\big( \\frac{1}{s}\\frac{n}{2n-1}\\big)  \\leq sn\\Big(\\frac{1}{n} \\big(\\frac{1}{s - x_1} + \\frac{1}{s - x_2} + ... + \\frac{1}{s - x_n}\\big)\\Big) = \\frac{s}{s - x_1} + \\frac{s}{s - x_2} + ... + \\frac{s}{s - x_n} $ \n",
    "\n",
    "which completes the proof.  \n",
    "\n",
    "\n",
    "**proof 2: via Harmonic Means**  \n",
    "*this is the official solution approach, with some additional details added.  I don't 'see' these arrangements yet and perhaps need more practice.  It feels like I've closed to master using convexity which was hit hard in chapter 6 and again pretty hard in chapter 7 -- perhaps I am overusing it, and not using the tools like power means that can be built off of it?*  \n",
    "\n",
    "consider the uniform case where $a_1, a_2, ..., a_n \\gt 0$ $p_k = \\frac{1}{n}$ \n",
    "\n",
    "applying the harmonic mean inequality of 8.16 gives: \n",
    "\n",
    "$\\frac{1}{\\frac{1}{n}\\sum_{i=1}^n a_i} \\leq \\frac{1}{n}\\big(\\sum_{i=1}^n \\frac{1}{a_i}\\big)$\n",
    "\n",
    "multiply each side by $n$ and get:  \n",
    "\n",
    "$\\frac{n^2}{\\sum_{i=1}^n a_i} \\leq \\sum_{i=1}^n \\frac{1}{a_i}$\n",
    "\n",
    "assign $a_k := 2s - x_k$.  \n",
    "\n",
    "This gives $\\sum_{i=1}^n a_i = 2ns - s = s(2n-1)$  \n",
    "make the substitutions, and multiply each side by $s$  \n",
    "$\\frac{n^2}{(2n-1)} = s\\frac{n^2}{\\sum_{i=1}^n a_i} \\leq s\\sum_{i=1}^n \\frac{1}{a_i} = \\sum_{i=1}^n \\frac{s}{2s - x_i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.3 Integral Analogs\n",
    "\n",
    "where each $a_k \\geq 0$, show that \n",
    "\n",
    "\n",
    "(a) $\\Big(\\sum_{k=1}^n a_k^{\\frac{1}{2}}\\Big)^2 \\leq \\Big(\\sum_{k=1}^n a_k^{\\frac{1}{3}}\\Big)^3$ \n",
    "\n",
    "\n",
    "**proof:**\n",
    "\n",
    "if all $a_k =0$ then both sides equal zero and there is nothing to prove.  Now assume at least one $a_k \\gt 0$ \n",
    "\n",
    "let $\\gamma^{\\frac{1}{2}} := \\frac{1}{\\Big(\\sum_{k=1}^n a_k^{\\frac{1}{2}}\\Big)} \\gt 0$  \n",
    "\n",
    "thus \n",
    "\n",
    "$\\gamma \\Big(\\sum_{k=1}^n a_k^{\\frac{1}{2}}\\Big)^2 = \\Big(\\sum_{k=1}^n a_k^{\\frac{1}{2}}\\Big)^2 \\big(\\gamma^{\\frac{1}{2}}\\big)^2 = \\Big(\\sum_{k=1}^n a_k^{\\frac{1}{2}}\\Big)^2 \\big(\\gamma^{\\frac{1}{2}}\\big)^2 = \\Big(\\sum_{k=1}^n (\\gamma a_k)^{\\frac{1}{2}}\\Big)^2 = 1 \\leq \\Big(\\sum_{k=1}^n (\\gamma a_k)^{\\frac{1}{3}}\\Big)^3  =\\gamma \\Big(\\sum_{k=1}^n a_k^{\\frac{1}{3}}\\Big)^3$ \n",
    "\n",
    "\n",
    "the rescaling has no impact on the inequality, hence we may assume without loss of generality, assume that  \n",
    "\n",
    "$\\Big(\\sum_{k=1}^n a_k^{\\frac{1}{2}}\\Big) = 1$\n",
    "\n",
    "\n",
    "from here we need to just show that the RHS is justified\n",
    "\n",
    "$\\Big(\\sum_{k=1}^n a_k^{\\frac{1}{2}}\\Big) = 1  = \\Big(\\sum_{k=1}^n a_k^{\\frac{1}{2}}\\Big)^{\\frac{2}{3}} \\leq \\Big(\\sum_{k=1}^n a_k^{\\frac{1}{3}}\\Big)$ \n",
    "\n",
    "\n",
    "since each $\\sum_{k=1}^n a_k^{\\frac{1}{2}} =1$ and each terms is real non-negative, we know that each \n",
    "\n",
    "$0\\leq a_k^{\\frac{1}{2}}\\leq 1$\n",
    "\n",
    "taking advantage of non-negativity, we know \n",
    "$0\\leq a_k\\leq 1$\n",
    "\n",
    "note that a square root operation increases such $a_k$ and a cube root increases such $a_k$ by at least as much, hence every term on the RHS series is at least as big as each term on the LHS, and given non-negativity, we know the $LHS \\leq RHS $.  \n",
    "\n",
    "**more provactively, for (b), we are told to examine the case of non-negative** $f$\n",
    "\n",
    "$\\Big(\\int_{0}^1 \\big(f(x)\\big)^{\\frac{1}{2}}\\Big)^2 \\leq \\Big(\\int_{0}^1 \\big(f(x)\\big)^{\\frac{1}{3}}\\Big)^3$  \n",
    "which in general is **false**.  \n",
    "\n",
    "**example:**\n",
    "the above *false* inequality can be re-written as \n",
    "\n",
    "$\\Big(\\int_{0}^1 \\big(f(x)\\big)^{\\frac{1}{2}}\\Big)^\\frac{2}{3} \\leq \\int_{0}^1 \\big(f(x)\\big)^{\\frac{1}{3}}$  \n",
    "\n",
    "for example, consider the mapping $f(x) = 4x^2$\n",
    "\n",
    "in this case we have \n",
    "\n",
    "$1 = \\Big(2 \\big(\\frac{1}{2}\\big)\\Big) = \\Big(2 \\int_{0}^1 x^2\\Big)= \\Big(\\int_{0}^1 2 x^2\\Big)  =  \\Big(\\int_{0}^1 \\big(4x^2\\big)^{\\frac{1}{2}}\\Big) = \\Big(\\int_{0}^1 \\big(f(x)\\big)^{\\frac{1}{2}}\\Big) = \\Big(\\int_{0}^1 \\big(f(x)\\big)^{\\frac{1}{2}}\\Big)^\\frac{2}{3} \\nleq  \\int_{0}^1 \\big(f(x)\\big)^{\\frac{1}{3}} \\approx 0.95 $\n",
    "\n",
    "\n",
    "\n",
    "- - - - \n",
    "**remarks:**\n",
    "\n",
    "The suggested heuristic from Polya, Hardy and Littlwood indicates that the above expressions have LHS homogenous in order $2$ while the RHS are in general homogenous in order $3$, which pours cold water on our hope to generalize the summation to the integral.  This is worth spending more time thinking on. My own view at present is still developing.  For one, the integral form *is* an expected value while the summation is *not*... and if we tried to convert the summation form into say a uniform expected value, we'd have \n",
    "\n",
    "a mismatch of $\\frac{1}{n^{\\frac{1}{2}}}$ on the LHS but $\\frac{1}{n^{\\frac{1}{3}}}$ on the RHS, which is not equivalent for $n\\geq 2$.  This probaly *is* a reverse way of getting to what Polya, Hardy and Littlwood were saying.  \n",
    "\n",
    "Another way to think about this, is, in the summation form, we can be certain that *every* 'payoff' of the LHS and RHS is $0\\leq payoff \\leq 1$.  However with integrals it is possible, even if the LHS integrates to one, to have payoffs in there much greater than one -- they just have a certain density weighting.  But once it is possible that not all payoff are in the between 0 and 1, we can no longer know for sure the impact of switching from square roots to cube roots (i.e. it may make some payoff bigger and some smaller), and hence the bound doesn't in general hold.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 8.4 Polya's Minimax Characterization \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Ex 8.5 Geometric Mean Quasi-Linearization \n",
    "\n",
    "note: all numbers are real non-negative... and to cut to the chase, assume each number is positive\n",
    "\n",
    "The idea is we can reframe the geometric mean as an optimization problem, quasi-linearize it, and get some very nice results in a very intuitive way, like super-addivity  \n",
    "\n",
    "define, for some n dimensional vector $\\mathbf a$  \n",
    "\n",
    "$\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\text{GM}\\big(\\mathbf a\\big) = \\mathrm{\\big(\\prod_{k=1}^n a_k\\big)^\\frac{1}{n}} := {\\text{minimize}}\n",
    "& & \\mathrm{\\frac{1}{n}\\sum_{k=1}^n a_k x_k  = \\frac{1}{n}\\mathbf a^T \\mathbf x } \\\\\n",
    " & \\text{subject to:}\n",
    "& &\\prod_{k=1}^n x_k = 1 \\text{, }a_k \\gt 0 \\text{, }x_k \\gt 0\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}$\n",
    "\n",
    "to check this definition, consider that   \n",
    "\n",
    "$\\big(\\prod_{k=1}^n a_k\\big)^\\frac{1}{n} = \\big(\\prod_{k=1}^n a_k\\big)^\\frac{1}{n} \\big(1\\big)^\\frac{1}{n} = \\big(\\prod_{k=1}^n a_k\\big)^\\frac{1}{n}\\big(\\prod_{k=1}^n x_k\\big)^\\frac{1}{n} = \\big(\\prod_{k=1}^n (a_k x_k) \\big)^\\frac{1}{n} \\leq \\frac{1}{n}\\sum_{k=1}^n a_k x_k$  \n",
    "\n",
    "by $\\text{GM} \\leq \\text{AM}$ \n",
    "\n",
    "with equality **iff** $(a_1 x_1) = (a_2 x_2) = ... = (a_n x_n)$    \n",
    "\n",
    "so we select our minimizing vector called $\\mathbf x_a^*$  by setting \n",
    "\n",
    "$\\gamma := \\big(\\prod_{k=1}^n a_k\\big)^\\frac{1}{n}$ \n",
    "\n",
    "$x_k^{(a)} := \\frac{\\gamma}{a_k} \\gt 0$  \n",
    "\n",
    "which means that $a_k x_k^{(a)} = \\gamma$ \n",
    "\n",
    "-- i.e. each term has a constant value so the equality case of the inequality is met.  \n",
    "\n",
    "We also confirm that the product constraint is met:  \n",
    "\n",
    "$\\prod_{k=1}^n x_k^{(a)}  = \\prod_{k=1}^n \\frac{\\gamma}{a_k}  = \\prod_{k=1}^n \\gamma a_k^{-1} = \\big(\\gamma\\big)^n \\big(\\prod_{k=1}^n  a_k^{-1}\\big) = \\big(\\prod_{k=1}^n  a_k\\big)\\big(\\prod_{k=1}^n  a_k\\big)^{-1} = 1$  \n",
    "\n",
    "\n",
    "We thus have \n",
    "\n",
    "$\\text{GM}\\big(\\mathbf a\\big) = \\frac{1}{n}\\mathbf a^T \\mathbf x_a^*$   \n",
    "\n",
    "now consider the case of addivity  \n",
    "\n",
    "suppose we have $\\mathbf c := \\mathbf a + \\mathbf b $  \n",
    "\n",
    "then \n",
    "\n",
    "$\\text{GM}\\big(\\mathbf c\\big) = \\frac{1}{n}\\mathbf c^T \\mathbf x_{c}^* = \\frac{1}{n}\\big(\\mathbf a^T +\\mathbf b^T\\big)\\mathbf x_{c}^* =  \\frac{1}{n}\\mathbf a^T \\mathbf x_{c}^* + \\frac{1}{n}\\mathbf b^T \\mathbf x_{c}^* \\geq \\frac{1}{n}\\mathbf a^T \\mathbf x_{a}^* + \\frac{1}{n}\\mathbf b^T \\mathbf x_{c}^* \\geq \\frac{1}{n}\\mathbf a^T \\mathbf x_{a}^* + \\frac{1}{n}\\mathbf b^T \\mathbf x_{b}^* = \\text{GM}\\big(\\mathbf a\\big) + \\text{GM}\\big(\\mathbf b\\big)$  \n",
    "\n",
    "for the simple reason that in a 1 player game, having two choices to optimize (for our minimization problem) is better than merely one choice.    \n",
    "\n",
    "Equivalently, the Right Hand Side is at least as small as the Left Hand Side because when selecting our minimizing $\\mathbf x_a^*$ and $\\mathbf x_b^*$ we always have the option of setting them equal to $\\mathbf x_c^*$ -- i.e. we only choose $\\mathbf x_a^* \\neq \\mathbf x_c^*$ because it gives an even better result -- and the same applies for $\\mathbf x_b^*$ .  \n",
    "\n",
    "- - - - \n",
    "note: a primary upside to this approach is that it makes it painfully obvious as to why the inequality must be true.  A downside to it is that it makes the equality conditions / sharpness of the inequality not immediately obvious.  \n",
    "\n",
    "It certainly seems to suggest that the equality case requires \n",
    "\n",
    "$\\mathbf a \\propto \\mathbf c$  \n",
    "and  \n",
    "$\\mathbf b \\propto \\mathbf c$  \n",
    "\n",
    "and if we realize, that by dividing both sides by the $b_i$'s, we can assume WLOG that $\\mathbf b = \\mathbf 1$, this then implies that $\\mathbf a$ and $\\mathbf b$ must be homogenous as well (at which point, 'merely' figuring out the scaline is all that is needed). These point on sharpness still don't seem entirely satisfactory however.  The convexity argument however, while a bit more in the weeds in terms of symbol manipulation, makes the equality case just jump out at you.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** now link the abve into a general optimization scheme... i.e. use it to get the results of ex 2.8**\n",
    "\n",
    "the minimization problem is immediate --- just rescale each x by same positive amount so that the result is the k not 1.... (see my writeup in 2.8)... I think... I may need to clean this argument up a little bit  \n",
    "\n",
    "**extension:**  \n",
    "\n",
    "suppose we are given a constrained optimization problem of the form\n",
    "$\\text{min:} (b_1y_1 + b_2y_2 + ... + b_ny_n)$  subject to $ (y_1y_2...y_n) = k\\gt0$  \n",
    "\n",
    "where $b_i \\gt 0$ and $y_i \\gt 0$  \n",
    "\n",
    "A quick change of variables makes the solution clear   \n",
    "\n",
    "$a_i:= b_i \\big(k\\big)^\\frac{-1}{n}$  \n",
    "\n",
    "$x_i:= y_i \\big(k\\big)^\\frac{1}{n}$  \n",
    "or equivalently  \n",
    "$y_i  = \\big(k\\big)^\\frac{-1}{n}x_i$  \n",
    "\n",
    "\n",
    "hence \n",
    "\n",
    "$a_i x_i = b_i \\big(k\\big)^\\frac{-1}{n} y_i \\big(k\\big)^\\frac{1}{n} = b_i y_i$   \n",
    "\n",
    "and our constraint is: \n",
    "\n",
    "$ \\Big(y_1y_2...y_n\\Big) = \\Big(\\big(k^\\frac{-1}{n}x_1\\big) \\big(k^\\frac{-1}{n}x_2\\big)...\\big(k^\\frac{-1}{n}x_n\\big)\\Big)=\\big(k\\big)^\\frac{-n}{n}\\Big(x_1x_2...x_n\\Big) = k\\Big(x_1x_2...x_n\\Big) = k$ \n",
    "\n",
    "i.e. our equality constraint is \n",
    "\n",
    "$\\Big(x_1x_2...x_n\\Big) = 1$  \n",
    "\n",
    "and the answer is immediate from the above quasi-linearization  \n",
    "\n",
    "and as before we keep an eye on the equality condition-- the inequality is strict except when \n",
    "\n",
    "$a_1x_1 = a_2 x_2 = ... = a_n x_n$ \n",
    "\n",
    "And this is equivalent ot saying \n",
    "\n",
    "$b_1y_1 = b_2 y_2 = ... = b_n y_n$   \n",
    "\n",
    "** potential extension: run this backward for the maximization problem **  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 8.7\n",
    "\n",
    "(a) My overkill aproach was to show this follows by having the linear portion of a taylor polynomial in the Lagrange form of the remainder.  The better approach (from the back of the book) is to set this up as a difference quotient and pass a limit to recover the derivative.  \n",
    "\n",
    "(b) suppose $x_k \\gt 0$ and each $p_k \\geq 0$ where $\\sum_{k=1}^n p_k = 1$, then \n",
    "\n",
    "$\\big(\\sum_{k=1}^n  p_k x_k\\big)\\log\\big(\\sum_{k=1}^n p_k x_k\\big)\\leq \\big(\\sum_{k=1}^n p_k x_k \\log(x_k)\\big)$  \n",
    "\n",
    "**proof:**  \n",
    "\n",
    "where $E[X] = \\mu = \\big(\\sum_{k=1}^n  p_k x_k\\big)$\n",
    "\n",
    "consider $f(v) = v\\log(v)$,   \n",
    "then $f''(v) = \\frac{1}{v} \\gt 0$ for all $v\\gt 0$, confirming the convexity of $f$ over our domain.  Thus we have \n",
    "\n",
    "\n",
    "$\\big(\\sum_{k=1}^n  p_k x_k\\big)\\log\\big(\\sum_{k=1}^n p_k x_k\\big) = \\mu \\log(\\mu) =  f\\big(E[X]\\big) \\leq E\\big[f(X)\\big] = \\sum_{k=1}^n \\big(p_k x_k \\log(x_k)\\big)$  \n",
    "\n",
    "\n",
    "by direct application of Jensen's Inequality, which completes the proof.  \n",
    "\n",
    "\n",
    "*commentary:* this problem was about the rare but powerful occassions where meaningful information about an inequality can be extracted by differentiating an inequality with special structure that allows special information to 'flow to' the inequality as well (typically this is not a meaningful operation).  Unfortunately your author didn't really get the spirit of that and just applied Jensen's Inequality for (b).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 8.8 \n",
    "\n",
    "**claim**  \n",
    "for some sequences of n-tuples, suppose where each term is real non-negative, and there is some constant $\\mu \\geq 0$, where \n",
    "\n",
    "$\\lim_{k \\to \\infty}\\sum_{j=1}^n a_{i,k} = n \\mu$\n",
    "\n",
    "and there exists some $1 \\lt r\\lt \\infty$ \n",
    "\n",
    "where \n",
    "\n",
    "$\\lim_{k \\to \\infty}\\sum_{j=1}^n a_{i,k}^r = n \\mu^r$\n",
    "\n",
    "then this proves that \n",
    "\n",
    "$\\lim_{k \\to \\infty}a_{j,k} = \\mu$ for all $1 \\leq j \\leq n$  \n",
    "\n",
    "\n",
    "**re-stated claim**  \n",
    "\n",
    "if \n",
    "\n",
    "$\\lim_{k \\to \\infty}\\frac{1}{n}\\sum_{j=1}^n a_{i,k} = \\mu$\n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "$\\lim_{k \\to \\infty}\\frac{1}{n}\\sum_{j=1}^n a_{i,k}^r = \\mu^r$\n",
    "\n",
    "then this proves that \n",
    "\n",
    "$\\lim_{k \\to \\infty}a_{j,k} = \\mu$ for all $1 \\leq j \\leq n$  \n",
    "\n",
    "**proof**\n",
    "\n",
    "In the event $\\mu$ is zero, we can verify by inspection that all terms are zero (by non-negativity).  Also if $\\mu \\gt 0$ then we know that not all terms can be equal to zero.  The rest of the proof assume $\\mu \\gt 0$.  \n",
    "\n",
    "consider the map $f(v) = v^r$ for all $r \\gt 1$, then \n",
    "\n",
    "$f''(v) = (r-1)rv^{r-2} \\gt 0$ \n",
    "\n",
    "for our domain where $v \\gt0$ and is equal to zero only in the case of $v=0$,  hence our function is strictly convex except in the special case of $v=0$ where it is 'merely' convex.  \n",
    "\n",
    "However, since at least one of our terms must be $\\gt 0$ because $\\mu \\gt 0$ we may treat the mapping as strictly convex.\n",
    "\n",
    "Application of Jensen's Inequality tells us \n",
    "\n",
    "$\\lim_{k \\to \\infty}\\frac{1}{n}\\sum_{j=1}^n a_{i,k}^r = \\mu^r = \\lim_{k \\to \\infty}f\\big(E[a_i]\\big) \\lt \\lim_{k \\to \\infty} E\\big[f(a_i)\\big]= \\lim_{k \\to \\infty}\\frac{1}{n}\\sum_{j=1}^n a_{i,k}^r = \\mu^r$\n",
    "\n",
    "**except** there is equality **iff** all terms are identical.  (Note the official solution invokes a power mean that depends on Jensen's Inequality and approaches the proof a touch more carefully / indirectly via a proof by contradiction.)  \n",
    "\n",
    "\n",
    "**commentary:**  \n",
    "The comment in the problem refers to a consistency principle underlying this:  \"if all coordinates of a vector and the sum of the corresponding rth powers have limits that are consistent with the possibility that all of the coordinates converge to a common constant, then that must indeed be the case.\"\n",
    "\n",
    "This statement is *awfully* close to a rank based argument in algebraic combinatorics.  Indeed when we first approached these power mean inequalities in exercise 2.14, your author's approach was to use an associated Hankel matrix that in essence is using a rank based argument.  Note: if we had $r=2$ this would also literally translate to zero variance, in which case Jensen's Inequality is not necessarily needed.  \n",
    "\n",
    "it is also somewhat similar to the proofs in the Vandermonde matrix writeup in that a large enough number of traces to a certain power totally characterize the $a_{j,k}'s$.  In this case because of the equality \n",
    "\n",
    "What is interesting, is at the end the book notes: \"The consistency principle has many variations and, like the optimality principle of Ex 2.8, page 33, it provides useful guidance even when it does not formally apply.\"  The idea of using this consistency principle, and whether there is hidden convexity or perhaps a rank based argument, is something that should be in one's mental checklist ot use quickly from time to time. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
