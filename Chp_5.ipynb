{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chebyshev's \"other\" Inequality re-visited  \n",
    "\n",
    "(as shown in chapter 2 -- page 73-- of Ross & Pekoz and elsewhere)  \n",
    "\n",
    "if $f$ and $g$ are real valued non-decreasing functions, \n",
    "\n",
    "consider $X_1$ and $X_2$ which are iid random variables  \n",
    "\n",
    "we have the relation \n",
    "\n",
    "$0 \\leq \\Big(f\\big(X_1\\big) -f\\big(X_2\\big)\\Big)\\Big(g\\big(X_1\\big) -g\\big(X_2\\big)\\Big)$   \n",
    "\n",
    "i.e. because our maps are both non-decreasing they preserve ordering in the same way, thus the above is a product of two non-negative terms or the product of two non-positive terms.  Taking expectations gives  \n",
    "\n",
    "$0 $  \n",
    "$\\leq E\\Big[\\Big(f\\big(X_1\\big) -f\\big(X_2\\big)\\Big)\\Big(g\\big(X_1\\big) -g\\big(X_2\\big)\\Big)\\Big]$   \n",
    "$= E\\Big[f\\big(X_1\\big)g\\big(X_1\\big) +f\\big(X_2\\big)g\\big(X_2\\big) - f\\big(X_1\\big)g\\big(X_2\\big)- f\\big(X_2\\big)g\\big(X_1\\big)\\Big]$   \n",
    "$= E\\Big[f\\big(X_1\\big)g\\big(X_1\\big)\\Big] + E\\Big[f\\big(X_2\\big)g\\big(X_2\\big)\\Big] - E\\Big[f\\big(X_1\\big)g\\big(X_2\\big)\\Big]- E\\Big[f\\big(X_2\\big)g\\big(X_1\\big)\\Big]$   \n",
    "\n",
    "which we can re-arrange to say  \n",
    "\n",
    "$E\\Big[f\\big(X_1\\big)g\\big(X_2\\big)\\Big]+ E\\Big[f\\big(X_2\\big)g\\big(X_1\\big)\\Big] $  \n",
    "$\\leq  E\\Big[f\\big(X_1\\big)g\\big(X_1\\big)\\Big] + E\\Big[f\\big(X_2\\big)g\\big(X_2\\big)\\Big]$   \n",
    "$=  2 \\cdot E\\Big[f\\big(X_1\\big)g\\big(X_1\\big)\\Big]  $   \n",
    "\n",
    "however, taking advantage of the Tower Property and independence, we may see \n",
    "\n",
    "$E\\Big[f\\big(X_1\\big)g\\big(X_2\\big)\\Big]$  \n",
    "$= E\\Big[E\\big[f\\big(X_1\\big)g\\big(X_2\\big)\\big \\vert X_2 \\big]\\Big] $   \n",
    "$= E\\Big[g\\big(X_2\\big) E\\big[f\\big(X_1\\big)\\big \\vert X_2 \\big]\\Big] $   \n",
    "$= E\\Big[g\\big(X_2\\big) E\\big[f\\big(X_1\\big)\\big]\\Big] $   \n",
    "$= E\\Big[f\\big(X_1\\big)\\Big] \\cdot E\\Big[g\\big(X_2\\big) \\Big] $ \n",
    "\n",
    "where we made use of the fact that \n",
    "\n",
    "$E\\Big[f\\big(X_1\\big)\\big \\vert X_2 \\Big] = E\\Big[f\\big(X_1\\big)\\Big]$  \n",
    "because by independece of $X_1$ and $X_2$ we know that for any sample value of $X_2(\\omega)$  \n",
    "$X_1$ has the same CDF (which implies same probability mass / density / measure) \n",
    "and from here we make use of LOTUS and apply the mapping $f$ while the expectation (LOTUS) to get the result  \n",
    "\n",
    "a virtually identical argument yields \n",
    "$E\\Big[f\\big(X_2\\big)g\\big(X_1\\big)\\Big]$  \n",
    "$= E\\Big[f\\big(X_2\\big)\\Big] \\cdot E\\Big[g\\big(X_1\\big) \\Big] $ \n",
    "\n",
    "and hence we can restate our inequality as \n",
    "\n",
    "$2 \\cdot E\\Big[f\\big(X_1\\big)\\Big] E\\Big[g\\big(X_1\\big)\\Big] $  \n",
    "$E\\Big[f\\big(X_1\\big)\\Big] E\\Big[g\\big(X_2\\big)\\Big]+ E\\Big[f\\big(X_2\\big)\\Big]E\\Big[g\\big(X_1\\big)\\Big] $  \n",
    "$E\\Big[f\\big(X_1\\big)g\\big(X_2\\big)\\Big]+ E\\Big[f\\big(X_2\\big)g\\big(X_1\\big)\\Big] $  \n",
    "$\\leq  E\\Big[f\\big(X_1\\big)g\\big(X_1\\big)\\Big] + E\\Big[f\\big(X_2\\big)g\\big(X_2\\big)\\Big]$   \n",
    "$=  2 \\cdot E\\Big[f\\big(X_1\\big)g\\big(X_1\\big)\\Big]  $   \n",
    "\n",
    "and dividing by 2 gives the desired result:  \n",
    "$ E\\Big[f\\big(X_1\\big)\\Big] E\\Big[g\\big(X_1\\big)\\Big] $  \n",
    "$\\leq  E\\Big[f\\big(X_1\\big)g\\big(X_1\\big)\\Big]  $   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the key findings in this chapter, is,\n",
    "\n",
    "for real positive $a_k$, $b_k$\n",
    "\n",
    "$\\big(\\sum_{k=1}a_k^2\\big)^\\frac{1}{2}\\big(\\sum_{k=1}b_k^2\\big)^\\frac{1}{2} \\leq \\rho \\big(\\sum_{k=1}a_k b_k\\big)$\n",
    "\n",
    "and to figure out a useful $\\rho$ on the RHS.\n",
    "\n",
    "The derivation proceeds by finding lower bound (m) and upper bounds (M), where \n",
    "\n",
    "$0\\lt m \\leq \\frac{a_k}{b_k} \\leq M \\lt \\infty$\n",
    "\n",
    "from here we easily note that\n",
    "\n",
    "$\\big(M - \\frac{a_k}{b_k}\\big) \\geq 0$  \n",
    "(i.e. LHS is positive or at worst zero)\n",
    "\n",
    "and  \n",
    "\n",
    "$\\big(m - \\frac{a_k}{b_k}\\big) \\leq 0$   (i.e. LHS is negative or at best zero)\n",
    "multiply by -1 \n",
    "\n",
    "$\\big( \\frac{a_k}{b_k}-m \\big) = \\big(-m + \\frac{a_k}{b_k}\\big) = -1 \\big(m - \\frac{a_k}{b_k}\\big)   \\geq 0$  \n",
    "\n",
    "hence the multiplication of two terms that are each $\\geq 0$ gives a product that is $\\geq 0$\n",
    "\n",
    "$\\big(M - \\frac{a_k}{b_k}\\big)\\big( \\frac{a_k}{b_k}-m \\big) \\geq 0 $\n",
    "\n",
    "expanding this, we get \n",
    "\n",
    "$M \\frac{a_k}{b_k} + m \\frac{a_k}{b_k} - \\big( \\frac{a_k}{b_k} \\big)^2 - m*M \\geq 0 $\n",
    "\n",
    "$M \\frac{a_k}{b_k} + m \\frac{a_k}{b_k} - \\big( \\frac{a_k}{b_k} \\big)^2  \\geq m*M $\n",
    "\n",
    "recalling that each $b_k \\gt 0$, we multiply each side by $b_k^2$\n",
    "\n",
    "$M a_kb_k + m a_kb_k - a_k^2  \\geq mM b_k^2 $\n",
    "\n",
    "$\\big(M + m \\big) a_kb_k   \\geq a_k^2 + mM b_k^2 $\n",
    "\n",
    "and indeed this inequality holds over any summation: \n",
    "- - - -   \n",
    "**key line**  \n",
    "$\\sum_{k=1}^{n}\\big(M + m \\big) a_kb_k   \\geq \\sum_{k=1}^{n} \\big(a_k^2 + mM b_k^2\\big) $\n",
    "\n",
    "*note: we combe back to the above equation shortly-- it is quite useful in Kantorovich's Inequality*  \n",
    "- - - -    \n",
    "now multiply each side by $\\frac{1}{2}$ and use $AM \\geq GM$\n",
    "\n",
    "\n",
    "$\\frac{1}{2}\\sum_{k=1}^{n}\\big(M + m \\big) a_kb_k   \\geq \\frac{1}{2} \\sum_{k=1}^{n} \\big(a_k^2 + mM b_k^2\\big)  \\geq \\big(\\sum_{k=1}^{n} a_k^2\\big)^\\frac{1}{2}\\big(mM\\sum_{k=1}^{n} b_k^2\\big)^\\frac{1}{2} $\n",
    "\n",
    "\n",
    "hence we find\n",
    "\n",
    "$ \\rho \\sum_{k=1}^{n} a_kb_k = \\frac{1}{2}\\big(M + m \\big) \\frac{1}{\\sqrt{mM}}\\sum_{k=1}^{n} a_kb_k \\geq \\big(\\sum_{k=1}^{n} a_k^2\\big)^\\frac{1}{2}\\big(\\sum_{k=1}^{n} b_k^2\\big)^\\frac{1}{2}$\n",
    "\n",
    "i.e. $\\rho = \\frac{1}{2}\\big(M + m \\big) \\frac{1}{\\sqrt{mM}}$\n",
    "\n",
    "which is of interest since $\\rho$ is equal to the arithmetic mean of the upper and lower bound on the ratios, divided by the geometric mean of said ratios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.8: Kantorovich's Inequality\n",
    "\n",
    "where $0 \\lt m = x_1 \\leq x_2 \\leq ... \\leq x_n = M \\lt \\infty$ and probabilities $p_j$ where $0\\leq p_j \\leq 1$ where $\\sum_{j=1}^n p_j = 1$\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$1 \\leq s_1 s_{-1}= \\Big(\\sum_{j=1}^n p_j x_j\\Big)\\Big(\\sum_{j=1}^n p_j \\frac{1}{x_j}\\Big) \\leq \\frac{\\alpha^2}{\\gamma^2}$\n",
    "\n",
    "where $\\alpha = \\frac{m + M}{2}$ and $\\gamma = \\sqrt{mM}$  \n",
    "\n",
    "Note the LHS inequality of 1 was added via Hankel matrix formulation where we know  \n",
    "\n",
    "$1 =  s_0= s_0^2\\leq s_1 s_{-1} $   \n",
    "\n",
    "note that $s_0 = E[X^0] = 1$, $s_1 = E[X]$ and $s_{-1} = E[X^{-1}]$  \n",
    "(see near the bottom of \"Chp_2.ipynb\", or near bottom of \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb\" for more information.)  \n",
    "\n",
    "(or see problem 1.2 in chapter 1, for a variation of this exact problem) \n",
    "\n",
    "\n",
    "**proof:**\n",
    "\n",
    "since the LHS inequality comes from other notebooks, we model the RHS inequality.\n",
    "\n",
    "$\\Big(\\sum_{j=1}^n p_j x_j\\Big)\\Big(\\sum_{j=1}^n p_j \\frac{1}{x_j}\\Big) \\leq \\frac{\\alpha^2}{\\gamma^2}$  \n",
    "\n",
    "For the easiest proof, **we assume, without a loss of generality, that** $\\gamma = 1$.  Since $\\gamma$ is strictly positive, (in a manner similar to other proofs in this book) we can always normalize by multiplying each side by $\\gamma^2$ so that it is in effect equal to one, do the proof, and then divide each side by $\\gamma^2$ to see that the inequality is intact. \n",
    "\n",
    "\n",
    "note: since $\\gamma = 1 = \\sqrt{mM}$, this means \n",
    "\n",
    "$\\gamma^2 = 1 = mM$\n",
    "\n",
    "i.e. $M = m^{-1}$\n",
    "\n",
    "define \n",
    "\n",
    "$b_k = x_j^{\\frac{1}{2}}$  \n",
    "$a_k = b_k^{-1} =  \\frac{1}{x_j^{\\frac{1}{2}}}$\n",
    "\n",
    "using our key line in the earlier cell:\n",
    "\n",
    "$ \\sum_{k=1}^{n} \\big(a_k^2 + mM b_k^2\\big) \\leq \\sum_{k=1}^{n}\\big(M + m \\big) a_kb_k $\n",
    "\n",
    "we update it for the problem at hand:  \n",
    "\n",
    "$\\sum_{k=1}^{n} \\big(a_k^2 + mm^{-1}b_k^2\\big) \\leq \\sum_{k=1}^{n}\\big(m^{-1} + m \\big) a_kb_k  = \\sum_{k=1}^{n}\\big(m^{-1} + m \\big) 1  $\n",
    "\n",
    "multiply each side by $\\frac{1}{2}$  \n",
    "$\\frac{1}{2}\\Big(\\big(\\sum_{k=1}^{n} a_k^2 \\big)+ \\big(\\sum_{k=1}^{n} b_k^2\\big)\\Big) = \\frac{1}{2}\\sum_{k=1}^{n} \\big(a_k^2 + b_k^2\\big) \\leq \\frac{1}{2}\\big(m^{-1} + m \\big) \\sum_{k=1}^{n} 1  $\n",
    "\n",
    "somewhat sloppily, we'd say: multiply each side by $p_k$, and simplify each side  \n",
    "\n",
    " $\\frac{1}{2}\\Big(E[X] + E[X^{-1}]\\Big) =   \\frac{1}{2}\\Big(\\big(\\sum_{k=1}^{n} p_k a_k^2 \\big) + \\big(\\sum_{k=1}^{n}  p_kb_k^2\\big)\\Big) \\leq \\frac{1}{2}\\big(m^{-1} + m \\big) \\sum_{k=1}^{n}p_k  =\\frac{1}{2}\\big(m^{-1} + m \\big) \\big( 1\\big) = \\alpha $\n",
    " \n",
    "- - - -\n",
    "*begin interlude *    \n",
    "\n",
    "more carefully, we could look at:\n",
    "\n",
    "$\\frac{1}{2}\\Big( a_k^2 + b_k^2 \\Big) \\leq \\frac{1}{2} \\big(m^{-1} + m \\big) $  \n",
    "\n",
    "\n",
    "for each and every $k$.  Hence we can multiply each side by $p_k$ without changing the inequality, and *then* sum over $k$ to recover the series above.\n",
    "\n",
    "As yet one more alternative, we could also note that for each and every value of discrete random variable $X$ that has positive probability, we see the relationship:  \n",
    "\n",
    "$\\frac{1}{2}\\Big( X + X^{-1}\\Big) \\leq \\frac{1}{2} \\big(m^{-1} + m \\big) $\n",
    "\n",
    "and then take the expectation of each side,\n",
    "\n",
    "$E\\big[\\frac{1}{2}\\Big( X + X^{-1}\\Big)\\big] = \\frac{1}{2}\\Big( E\\big[X\\big] + E\\big[X^{-1}\\big] \\Big) \\leq \\frac{1}{2} E\\big[\\big(m^{-1} + m \\big)\\big]= \\frac{1}{2}\\big(m^{-1} + m \\big) = \\alpha $\n",
    "\n",
    "*return to main proof*    \n",
    "- - - -\n",
    "\n",
    "finally  on LHS use $GM \\leq AM$\n",
    "\n",
    "$E[X]^{\\frac{1}{2}} E[X^{-1}]^{\\frac{1}{2}} \\leq  \\frac{1}{2}\\Big(E[X] + E[X^{-1}]\\Big) \\leq \\alpha $\n",
    "\n",
    "square both sides and we see:\n",
    "\n",
    "$E[X] E[X^{-1}] = \\Big(\\sum_{j=1}^n p_j x_j\\Big)\\Big(\\sum_{j=1}^n p_j \\frac{1}{x_j}\\Big) \\leq \\alpha^2 $\n",
    "\n",
    "- - - -\n",
    "noting that the LHS is 1 and RHS is $\\frac{\\alpha}{\\gamma^2}$, which is the AM of upper, lower bound, divided by the GM of upper, lower bound, we can see that the $1 \\leq RHS$ unless $m = x_1 = x_2  = ... = x_n = M$, i.e. a single variable case.  This creates a squeeze and we see $LHS = RHS$.  We know from Vandermonde matrix modelling that if there are 2 or more unique values, then the associated Hankel Matrix (which is always positive semi-definite when the underlying moments have are from positive numbers) is non-singular.\n",
    "\n",
    "$\\mathbf H =  \\begin{bmatrix}\n",
    "s_{-1} & s_0  & \\\\ \n",
    "s_0 &  s_1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "i.e. $0 \\lt det\\big(\\mathbf H\\big) =  s_{-1}s_{1} - s_0^2 = s_{-1}s_{1} - 1$\n",
    "\n",
    "i.e. \n",
    "\n",
    "i.e. $1 \\lt s_{-1}s_{1} $\n",
    "\n",
    "hence the inequality becomes an equality and can only exist in the case of there being one unique value amongst the $x's$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** a much later note**  \n",
    "after a significant amount of time has passed, a much cleaner and insightful comes to your author's mind.  \n",
    "\n",
    "as before we have $0 \\lt m = x_1 \\leq x_2 \\leq ... \\leq x_n = M \\lt \\infty$  \n",
    "\n",
    "so we have a point-wise lower bound of \n",
    "$m \\leq x_k $  \n",
    "and \n",
    "$M \\leq \\frac{1}{x_k}$  \n",
    "rescaling by nonnegative $p_k$ (probabilities really), we get  \n",
    "\n",
    "$p_k m \\leq p_k x_k $  \n",
    "and \n",
    "$p_k M \\leq p_k \\frac{1}{x_k}$  \n",
    "\n",
    "summing overt the bounds give  \n",
    "$m = m \\sum_{k=1}^n p_k =\\sum_{k=1}^n p_k m \\leq \\sum_{k=1}^n p_k x_k$  \n",
    "$M = M \\sum_{k=1}^n p_k =\\sum_{k=1}^n p_k M \\leq \\sum_{k=1}^n p_k \\frac{1}{x_k}$  \n",
    "\n",
    "since all terms are positive, we can multiply these inequalities and see \n",
    "\n",
    "$\\gamma^2 = mM \\leq \\big(\\sum_{k=1}^n p_k \\frac{1}{x_k}\\big)\\big(\\sum_{k=1}^n p_k x_k\\big)$  \n",
    "\n",
    "this is half of Kantorovich's inequality.  I.e. we can restate Kantorich's Inequality in full as  \n",
    "\n",
    "$\\gamma^2 = mM \\leq \\big(\\sum_{k=1}^n p_k x_k\\big)\\big(\\sum_{k=1}^n p_k \\frac{1}{x_k}\\big)\\leq \\frac{m+M}{2} = \\mu^2 $   \n",
    "\n",
    "to prove the upper bound, take square roots and seek to prove the equivalent \n",
    "$\\big(\\sum_{k=1}^n p_k x_k\\big)^\\frac{1}{2}\\big(\\sum_{k=1}^n p_k \\frac{1}{x_k}\\big)^\\frac{1}{2}\\leq \\mu$  of the middle terms and see \n",
    "\n",
    "$\\big(\\sum_{k=1}^n p_k x_k\\big)^\\frac{1}{2}\\big(\\sum_{k=1}^n p_k \\frac{1}{x_k}\\big)^\\frac{1}{2}$  \n",
    "$\\leq \\frac{1}{2} \\Big(\\big(\\sum_{k=1}^n p_k x_k\\big) + \\Big(\\big(\\sum_{k=1}^n p_k \\frac{1}{x_k}\\big)\\Big)$  \n",
    "$= \\frac{1}{2}\\Big(\\sum_{k=1}^n p_k (x_k +\\frac{1}{x_k}) \\Big)$  \n",
    "$\\leq \\frac{1}{2}\\Big(\\sum_{k=1}^n p_k (M + m ) \\Big)$  \n",
    "$= \\frac{M + m }{2}\\Big(\\sum_{k=1}^n p_k  \\Big)$  \n",
    "$= \\frac{M + m }{2}$  \n",
    "$= \\mu$  \n",
    "\n",
    "where the first inequality follows from $\\text{GM} \\leq \\text{AM}$ and the second inequality follows from $x_k \\leq M$ and $\\frac{1}{x_k} \\leq m$, adding them to get \n",
    "$x + \\frac{1}{x_k} \\leq M + m$  \n",
    "then rescaling by nonnegative number $\\frac{1}{2}p_k$, and summing over the bound.  \n",
    "\n",
    "\n",
    "In standard form, then, we can state \n",
    "\n",
    "$1 \\leq \\big(\\sum_{k=1}^n p_k x_k\\big)\\big(\\sum_{k=1}^n p_k \\frac{1}{x_k}\\big)\\leq \\frac{m+M}{2} = \\frac{\\mu^2}{\\gamma^2} $ \n",
    "\n",
    "due to positivity and homogeneity of the exponents -- so as mentioned in the original solution, we may assume WLOG that $\\gamma = 1$  (and we note that the lower bound was proven in exercise 1.2)  \n",
    "\n",
    "put differently, with respect to what we've directly proven,  consider rescaling the sequence by $\\gamma$ so we have \n",
    "\n",
    "$0 \\lt \\frac{m}{\\gamma} = \\frac{x_1}{\\gamma} \\leq \\frac{x_2}{\\gamma} \\leq ... \\leq \\frac{x_n}{\\gamma} = \\frac{M}{\\gamma} \\lt \\infty$  \n",
    "\n",
    "or using a change of variables  \n",
    "$0 \\lt \\frac{m}{\\gamma} = y_1 \\leq y_2 \\leq ... \\leq y_n = \\frac{M}{\\gamma} \\lt \\infty$   \n",
    "\n",
    "which has a geometric mean of upper and lower bound equal to \n",
    "\n",
    "$\\big(\\frac{m}{\\gamma}\\frac{M}{\\gamma}\\big)^\\frac{1}{2} = \\frac{\\sqrt{mM}}{\\gamma} = \\frac{\\gamma}{\\gamma} = 1$  \n",
    "\n",
    "and re-running our original upper bound argument gives \n",
    "\n",
    "$\\big(\\sum_{k=1}^n p_k y_k\\big)\\big(\\sum_{k=1}^n p_k \\frac{1}{y_k}\\big)\\leq \\big(\\frac{\\frac{m}{\\gamma}+ \\frac{M}{\\gamma}}{2}\\big)^2 =  \\big(\\frac{\\frac{m + M}{2}}{\\gamma}\\big)^2 = \\frac{\\mu^2}{\\gamma^2}$   \n",
    "\n",
    "and if we actually wanted the geometric mean of the upper and lower bound to be the original value of $c\\gt 0$, we'd simply rescale each value by $c$ which is equivalent to multiplying each the side by $1= \\frac{c}{c} = \\frac{c^2}{c^2}$   getting \n",
    "\n",
    "$ \\big(\\sum_{k=1}^n p_k x_k\\big)\\big(\\sum_{k=1}^n p_k \\frac{1}{x_k}\\big)= \\big(\\sum_{k=1}^n p_k (cy_k)\\big)\\big(\\sum_{k=1}^n p_k \\frac{1}{cy_k}\\big)=  \\frac{c}{c}\\big(\\sum_{k=1}^n p_k y_k\\big)\\big(\\sum_{k=1}^n p_k \\frac{1}{y_k}\\big)\\leq \\frac{(c\\mu)^2}{(c\\gamma)^2} = \\frac{\\mu^2}{\\gamma^2}\\frac{c^2}{c^2} = \\frac{\\mu^2}{\\gamma^2}$   \n",
    "\n",
    "which is the Kantorovich inequality  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 5.5\n",
    "\n",
    "suppose \n",
    "\n",
    "$0 \\lt a \\leq a_k \\leq A$ and $ 0 \\lt b \\leq b_k \\leq B$ for k = 1, 2, ..., n\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\frac{\\big \\Vert \\mathbf a \\big \\Vert_2^2 \\big \\Vert \\mathbf b \\big \\Vert_2^2}{\\big(\\mathbf a^T \\mathbf b\\big)^2} =  \\frac{\\big(\\sum_{k=1}^n a_k^2 \\big)\\big(\\sum_{k=1}^n b_k^2 \\big)}{\\big(\\sum_{k=1}^n a_k b_k\\big)^2} \\leq \\frac{1}{4}\\big(\\frac{\\sqrt{AB}}{\\sqrt{ab}} + \\frac{\\sqrt{ab}}{\\sqrt{AB}} \\big)^2$\n",
    "\n",
    "**proof**\n",
    "note: we can re-write the above bounds as\n",
    "\n",
    "$0 \\lt m= \\frac{a}{B} \\leq \\frac{a_k}{b_k} \\leq \\frac{A}{b} = M$  \n",
    "\n",
    "\n",
    "then clear the denominator on the LHS of our claim, and square root both sides\n",
    "\n",
    "$\\big(\\sum_{k=1}^n a_k^2 \\big)^\\frac{1}{2}\\big(\\sum_{k=1}^n b_k^2 \\big)^\\frac{1}{2} = \\frac{1}{2}\\big(\\frac{\\sqrt{AB}}{\\sqrt{ab}} + \\frac{\\sqrt{ab}}{\\sqrt{AB}} \\big)\\Big({\\sum_{k=1}^n a_k b_k}\\Big)  $\n",
    "\n",
    "now all we have to do is prove that \n",
    "\n",
    "$\\rho = \\frac{1}{2}\\big(M + m \\big) \\frac{1}{\\sqrt{mM}} = \\frac{1}{2}\\big(\\frac{\\sqrt{AB}}{\\sqrt{ab}} + \\frac{\\sqrt{ab}}{\\sqrt{AB}} \\big)$ \n",
    "\n",
    "or equivalently\n",
    "\n",
    "$2\\rho = \\big(M + m \\big) \\frac{1}{\\sqrt{mM}} = \\big(\\frac{\\sqrt{AB}}{\\sqrt{ab}} + \\frac{\\sqrt{ab}}{\\sqrt{AB}} \\big)$ \n",
    "\n",
    "starting with the left hand term, and plugging in, we see:  \n",
    "$\\big(M + m \\big) \\frac{1}{\\sqrt{mM}} =\\big(\\frac{A}{b} + \\frac{a}{B}  \\big) \\frac{1}{\\sqrt{\\frac{a}{B} \\frac{A}{b}}} = \\big(\\frac{A}{b} + \\frac{a}{B}  \\big) \\frac{\\sqrt{Bb}}{\\sqrt{aA}} = \\big(\\frac{AB}{Bb} + \\frac{ab}{Bb}  \\big) \\frac{\\sqrt{Bb}}{\\sqrt{aA}} = \\big(AB+ ab\\big) \\frac{1}{\\sqrt{BbaA}} = \\Big(\\frac{\\sqrt{AB}}{\\sqrt{ba}} +\\frac{\\sqrt{ba}}{\\sqrt{AB}} \\Big)$\n",
    "\n",
    "which completes the proof.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Rearrangement Inequality\n",
    "\n",
    "this problem states that for the following real valued sequences\n",
    "\n",
    "$-\\infty \\lt a_1 \\leq a_2 \\leq ... \\leq a_n \\lt \\infty$  and   \n",
    "\n",
    "$-\\infty \\lt b_1 \\leq b_2 \\leq ... \\leq b_n \\lt \\infty$\n",
    "\n",
    "\n",
    "where $\\sigma $ indicates a permutation of that set (or list or arangement)\n",
    "\n",
    "# claim:\n",
    "\n",
    "$\\mathbf a^T \\mathbf {Jb} = \\sum_{k=1}^n a_k b_{n-k+1} \\leq  \\mathbf a^T \\mathbf {Pb} = \\sum_{k=1}^n a_k b_{\\sigma(k)}  \\leq \\sum_{k=1}^n a_k b_k = \\mathbf a^T \\mathbf {I b} = \\mathbf a^T \\mathbf {b}$ \n",
    "\n",
    "Where $\\mathbf I$ is the $n$ x $n$ identity matrix, $\\mathbf P$ is any $n$ x $n$ permutation matrix, and $\\mathbf J$ is the $n$ x $n$ reflection matrix given below: \n",
    "\n",
    "$\\mathbf J = \\begin{bmatrix}\n",
    "0 & 0 &0 & \\cdots& 0 &1 \\\\ \n",
    "0 & 0 &0 &\\cdots & 1 &0 \\\\ \n",
    "0 & 0 & 0&\\cdots & 0 & 0\\\\ \n",
    "\\vdots & \\vdots& \\vdots & \\ddots & \\vdots &\\vdots \\\\ \n",
    "0 & 1 &0 & \\cdots & 0 & 0\\\\\n",
    "1 & 0 &0& \\cdots & 0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "(note $\\mathbf J$ is the reflection matrix, here, which is a special kind of permutation matrix.  The same one we may multiply a toeplitz matrix $\\mathbf T$ on the right by, in order to get a Hankel matrix $\\mathbf H$.  I.e. $\\mathbf H = \\mathbf{TJ}$.  It is also worth remarking that there is an involution, i.e. $\\mathbf J^2 = \\mathbf I$. )  \n",
    "\n",
    "This reads as the maximal sum is when you calculate $\\mathbf a^T \\mathbf b$ \n",
    "\n",
    "Any permutation thereof cannot get a higher sum.  Finally, the absolute lowest value occurs when you 'read in' the b values backwards (or equivalently, the b's are read 'forwards' but the a's are read in 'backwards').  \n",
    "\n",
    "# proof:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 0:  \n",
    "In a nutshell, the re-arrangement in equality with 2 lists of n terms comes down to uncovering the simplest non-trivial case, i.e. with $n=2$.  \n",
    "\n",
    "So consider $a_1 \\gt a_2$ and $b_1 \\gt b_2$  \n",
    "\n",
    "The *claim* is that \n",
    "$a_1 b_1 + a_2 b_2 \\gt a_1 b_2 + a_2 b_1$  \n",
    "\n",
    "*proof:*  \n",
    "this is equivalent to  \n",
    "$a_1 b_1 - a_1 b_2 + a_2 b_2 -  a_2 b_1  \\gt 0 $   \n",
    "$a_1( b_1 - b_2) + a_2 (b_2 - b_1)  \\gt 0 $   \n",
    "letting $\\gamma = ( b_1 - b_2) \\gt 0$  \n",
    "$a_1 (\\gamma)  + a_2 (-\\gamma)  \\gt 0 $   \n",
    "$a_1 (\\gamma)  - a_2 (\\gamma)  \\gt 0 $   \n",
    "$\\gamma(a_1  - a_2) \\gt 0 $    \n",
    "and dividing out our positive number $\\gamma$ this reduces to  \n",
    "\n",
    "$a_1  - a_2 \\gt 0 $    \n",
    "which holds because $a_1 \\gt a_2$  \n",
    "\n",
    "\n",
    "For the more general case of $n$ terms, consider that there are only finitely many different lists arrangements between all terms and hence (in essence a compactness argument) there is (at least one) arrangement with maximal dot product.  If we ever have even one term out of decreasing order, then that configuration cannot be a maximum, because we may label those two terms $b_1$ and $b_2$ and get an even better score by effecting an exchange.  There is a bit more book-keeping to be done if terms are out of order in each list, though this is the core of it.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part 1: key exchange for increasing payoffs**   \n",
    "\n",
    "where $k > r$\n",
    "\n",
    "and \n",
    "\n",
    "where $k > i$\n",
    "\n",
    "$a_k \\geq a_r$  \n",
    "subtract $a_r$ from both sides and see:  \n",
    "\n",
    "$(a_k - a_r) \\geq 0$\n",
    "\n",
    "same with $b$\n",
    "\n",
    "$(b_k - b_i) \\geq 0$\n",
    "\n",
    "hence  \n",
    "$(a_k - a_r) (b_k - b_i) \\geq 0$\n",
    "\n",
    "expand\n",
    "\n",
    "$a_k b_k + a_r b_i -a_r b_k - a_k b_i \\geq 0$\n",
    "\n",
    "re-arrange:  \n",
    "\n",
    "$a_k b_k + a_r b_i \\geq a_r b_k + a_k b_i $\n",
    "\n",
    "note that this argument works the same, whether we have\n",
    "\n",
    "$ k > i \\geq r$\n",
    "\n",
    "or \n",
    "\n",
    "$ k > r \\geq i$\n",
    "\n",
    "\n",
    "\n",
    "- - - -\n",
    "*commentary: *\n",
    "\n",
    "What this tells us is: if we start looking through the sorted indices of values in $\\mathbf a$ in reverse order from $k = \\{n, n-1, n-2, ... 2, 1\\}$.  If we ever find a case where the $k$th entry in $a$ is not being multiplied by the $kth$ entry in $b$, we can improve our sum by making a swap such that $a_k$ is multiplied by $b_k$, and then continuing down our reverse sorted list. \n",
    "\n",
    "\n",
    "It is important to remark that if all entries in $\\mathbf a$ and all entries in $\\mathbf b$ are unique, then the inequality becomes strict\n",
    "\n",
    "$a_k b_k + a_r b_i \\gt a_r b_k + a_k b_i $  \n",
    "\n",
    "- - - -\n",
    "\n",
    "** Part 2: key exchange for decreasing payoffs** \n",
    "\n",
    "Consider the case of $\\big(\\mathbf {Jb}\\big)$ (i.e. where everything is swapped in $\\mathbf b$), and *with an abuse of notation,  we use conjugation sign to denote that the value is in its \"opposite\" place*.  With this change in mind, reconsider\n",
    "\n",
    "\n",
    "again, where $k > r$, and $k > i$\n",
    "\n",
    "we still see:\n",
    "\n",
    "$(a_k - a_r) \\geq 0$\n",
    "\n",
    "\n",
    "- - - -\n",
    "we take a bit more care with $b$.  here we consider:\n",
    "\n",
    "if $b_i$ is  greater than or equal to the 'midpoint'\n",
    "\n",
    "$\\bar{b_k} \\leq \\bar{b_i} \\leq b_i \\leq b_k $\n",
    "\n",
    "if $b_i$ is less than the 'midpoint':   \n",
    "\n",
    "we see it must be the case that, either :  \n",
    "\n",
    "$\\bar{b_k} \\leq  b_i \\leq \\bar{b_i} \\leq b_k $ \n",
    "\n",
    "or this is the case:\n",
    "\n",
    "$\\bar{b_k} \\leq  b_i \\leq b_k \\leq \\bar{b_i}$\n",
    "\n",
    "However, the key result in all of these cases is that \n",
    "\n",
    "$\\bar{b_k} \\leq \\bar {b_i}$\n",
    "- - - -\n",
    "our expression becomes  \n",
    "$(\\bar{b_k} - \\bar{b_i}) \\leq 0$\n",
    "\n",
    "hence  \n",
    "$(a_k - a_r) (\\bar{b_k} - \\bar{b_i}) \\leq 0$\n",
    "\n",
    "expand\n",
    "\n",
    "$a_k \\bar{b_k} + a_r \\bar{b_i} -a_r \\bar{b_k} - a_k \\bar{b_i} \\leq 0$\n",
    "\n",
    "re-arrange:  \n",
    "\n",
    "$a_k \\bar{b_k} + a_r \\bar{b_i} \\leq a_r \\bar{b_k}+ a_k \\bar{b_i} $\n",
    "\n",
    "again, note that this argument works the same, whether we have\n",
    "\n",
    "$ k > i \\geq r$\n",
    "\n",
    "or \n",
    "\n",
    "$ k > r \\geq i$\n",
    "\n",
    "- - - -\n",
    "*commentary: *\n",
    "\n",
    "What this second exchange tells us is: if we start looking through the sorted indices in reverse order from $k = n$, then $n-1$, then $n-2$, then... then $2$, then $1$.  If we ever find a case where the $k$th entry in $a$ is not being multiplied by it's \"opposite\" (i.e. we don't have $a_k \\bar{b_k} = a_k b_{n-k+1}$ but instead have some $a_k\\bar{b_i}$), we can decrease (or at worst not increase) our sum by making a swap such that $a_k$ is multiplied by $\\bar{b_k}$, and then continue on down.  \n",
    "\n",
    "It is important to remark that if all entries in $\\mathbf a$ and all entries in $\\mathbf b$ are unique, then the inequality becomes strict\n",
    "\n",
    "$a_k \\bar{b_k} + a_r \\bar{b_i} \\lt a_r \\bar{b_k}+ a_k \\bar{b_i} $\n",
    "- - - -\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In a manner similar to verifying optimality for greedy algorithms and minimum spanning trees, this leads to a nice exchange argument.  \n",
    "\n",
    "\n",
    "i.e. to start, consider the case where all values are unique.  \n",
    "\n",
    "\n",
    "We may simply guess that we get the true highest sum occurs when we greedily sort $\\mathbf a$ and $\\mathbf b$ and then dot them.  \n",
    "\n",
    "However, suppose we find a highest payoff solution $S^* =  \\mathbf a^T \\mathbf {Pb} $, where $\\mathbf P \\neq \\mathbf I$.  This means at least one (in fact at least two-- because a list cannot have only one entry out of order) entry $k$ is out of order, and we can get an even better payoff by making the exchange (recalling the the inequality is strict if there are no duplicate values).  Hence $S^*$ was not an optimal solution afterall.  \n",
    "\n",
    "The argument is a bit muddier if ties can happen, but the same underlying idea still occrus.  If we find any optimal solution $S^* =  \\mathbf a^T \\mathbf {Pb} $, where $\\mathbf P \\neq \\mathbf I$, we can always make these pairwise swaps to improve or at least not decrease our score.  If we make at most $\\binom{n}{2}$ of these swaps, each time without improving our score, then we can still agree that $S^*$ is optimal but its payoff is weakly dominant and no better than $ \\mathbf a^T \\mathbf {Ib}$ which always results after at most $\\binom{n}{2}$  of these swaps.  (Unfortunately the argument can get a bit tedious in proving that the identity permutation given by $\\mathbf I$ always results after some finite number of swaps... the case where all values are unique is much more clean cut and satisfying.)  \n",
    "\n",
    "- - - -\n",
    "In the case of finding a strictly lowest solution, again consider the case first where all values are unique.  Suppose we find some strictly lowest solution $S^* =  \\mathbf a^T \\mathbf {Pb} $, where $\\mathbf P \\neq \\mathbf J$.  There must be at least one case (and in fact, again at least 2 cases) where $\\mathbf a_k $ isn't being multiplied by it's \"opposite\" $b$  (i.e. we don't have $a_k \\bar{b_k} = a_k b_{n-k+1}$) \n",
    "\n",
    "Then we can find an *even better solution* if we make the 'Part 2 exchange' -- this contradicts the fact that $S^*$ is said to be strictly optimal.  \n",
    "\n",
    "The argument then generalizes, as above, to accomodate ties.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.7\n",
    "\n",
    "suppose we have \n",
    "\n",
    "$\\mathbf c$ with each value real and positive.  \n",
    "\n",
    "With a slight abuse of notation consider $\\mathbf c^{-1}$ a vector where every value is the component-wise inverse of that in $\\mathbf c$\n",
    "\n",
    "The ordering in $\\mathbf c$ does not ultimately matter, but suppose we use the optimal permutation matrix $\\mathbf U$, such that $\\big(\\mathbf {Uc}\\big)$ has items sorted from smallest to largest.  \n",
    "\n",
    "of course by orthgonality, $\\big(\\mathbf {Uc}\\big)^T\\big(\\mathbf {Uc}\\big) = \\mathbf c^T \\mathbf c$\n",
    "\n",
    "note that after constructing $\\mathbf c^{-1}$, we consider $\\big(\\mathbf {Uc}^{-1}\\big)$\n",
    "\n",
    "since items were sorted from smallest to largest (and all items are positive) in $\\big(\\mathbf {Uc}\\big)$,  then $\\big(\\mathbf {Uc}^{-1}\\big)$ must have all items sorted from largest to smallest  -- i.e. in reverse.  \n",
    "\n",
    "Thus, per the above Rearrangement inequality, we find the lowest possible result we can get is: \n",
    "\n",
    "$\\big(\\mathbf {Uc}\\big)^T \\big(\\mathbf {Uc}^{-1}\\big) = \\mathbf {c}^T\\mathbf U^T \\mathbf {Uc}^{-1}=  \\mathbf c^T \\mathbf c^{-1} = \\frac{c_1}{c_1} + \\frac{c_2}{c_2} + \\frac{c_3}{c_3} + ... + \\frac{c_n}{c_n} = n$\n",
    "\n",
    "I.e. $n$ is our lower bound.  \n",
    "\n",
    "now consider applying some permutation matrix $\\mathbf P$ to $\\big(\\mathbf {Uc}^{-1}\\big) $.  The resulting sum must be $\\geq $ our lower bound.  Specifically consider,\n",
    "\n",
    "$n = \\mathbf c^T \\mathbf c^{-1} \\leq  \\big(\\mathbf {Uc}\\big)^T \\Big(\\mathbf P\\big(\\mathbf {Uc}^{-1}\\big)\\Big) = \\frac{c_1}{c_n} + \\frac{c_2}{c_1} + \\frac{c_3}{c_2} + ... + \\frac{c_n}{c_{n-1}} $\n",
    "\n",
    "\n",
    "- - - -\n",
    "from here the problem nudges us to consider some very important special cases.  Specifically, from here carefully consider the case where \n",
    "\n",
    "$c_1 := \\rho x_1$  \n",
    "$c_2 := \\rho^2 x_1 x_2$  \n",
    "and in general\n",
    "\n",
    "$c_k := \\rho^k \\prod_{j = 1}^k x_j$    \n",
    "\n",
    "\n",
    "where each $x_j \\gt 0$ and $\\rho \\gt 0$ \n",
    "\n",
    "note that this means  $c_n = \\rho^n \\big(\\prod_{j = 1}^n x_j\\big) = \\rho^n\\Big( \\big(\\prod_{j = 1}^n x_j\\big)^\\frac{1}{n}\\Big)^n = \\rho^n\\Big( GM\\Big)^n$  \n",
    "\n",
    "where GM denotes geometric mean (which of course is positive)    \n",
    "\n",
    "this gives us\n",
    "\n",
    "$n \\leq \\frac{c_1}{c_n} + \\frac{c_2}{c_1} + \\frac{c_3}{c_2} + ... + \\frac{c_n}{c_{n-1}} = \\frac{\\rho x_1}{\\rho^n GM^n} + \\frac{\\rho^2 x_1 x_2}{\\rho x_1} + \\frac{\\rho^3 x_1 x_2 x_3}{\\rho^2 x_1 x_2} + ... + \\frac{\\rho^n \\prod_{j = 1}^n x_j }{\\rho^{n-1} \\prod_{j = 1}^{n-1} x_j} = \\frac{\\rho x_1}{\\big(\\rho \\text{GM}\\big)^n} + \\rho x_2 + \\rho x_3 + ... + \\rho x_n $\n",
    "\n",
    "if we cleverly set $\\rho := \\frac{1}{\\text{GM}}$ to extinguish the denominator in that first term, we get:  \n",
    "\n",
    "$n \\leq  \\frac{\\rho x_1}{\\big(\\frac{1}{\\text{GM}} GM\\big)^n} + \\rho x_2 + \\rho x_3 + ... + \\rho x_n  = \\rho x_1 + \\rho x_2 + \\rho x_3 + ... + \\rho x_n = \\rho \\big(x_1 + x_2 + x_3 + ... + x_n\\big) =  \\frac{1}{\\text{GM}} \\big(x_1 + x_2 + x_3 + ... + x_n\\big) $\n",
    "\n",
    "if we multiply each side by $\\frac{\\text{GM}}{n}$, then we get:   \n",
    "\n",
    "$\\text{GM} \\leq \\frac{1}{n} \\big(x_1 + x_2 + x_3 + ... + x_n\\big) = \\text{AM}$ \n",
    "\n",
    "recovering the $\\text{GM} \\leq \\text{AM}$ inequality.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
