{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Minkowski's Light Cone\n",
    "\n",
    "(see page 63 of *The Cauchy-Schwarz Master Class*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Preamble* \n",
    "\n",
    "\n",
    "all scalars are in $\\mathbb R$  \n",
    "\n",
    "where   \n",
    "\n",
    "$[\\mathbf x, \\mathbf y] = tu - (x_1y_1 + x_2 y_2 + ... + x_d y_d) = tu - \\mathbf x^T \\mathbf y =   tu - \\mathbf y^T \\mathbf x$\n",
    "\n",
    "Note that this means, for example:   \n",
    "\n",
    "$[\\mathbf x, \\mathbf x] = t^2 - (x_1^2 + x_2^2 + ... + x_d^2) = t^2 - \\mathbf x^T \\mathbf x$\n",
    "\n",
    "note: for this kind of product, there is a crucial **constraint** whereby:  \n",
    "\n",
    "for $\\mathbf x$ \n",
    "\n",
    "$t \\geq (x_1^2 + x_2^2 + ... + x_d^2)^\\frac{1}{2} \\geq 0$ \n",
    "\n",
    "equivalently\n",
    "\n",
    "$t \\geq  \\big \\Vert \\mathbf x \\big \\Vert_2$ \n",
    "\n",
    "since the lower bound on the $\\text{RHS}$ is $\\geq 0$, the $\\text{LHS}$ must be as well, thus we may square both sides and see:\n",
    "\n",
    "$t^2 \\geq  \\big \\Vert \\mathbf x \\big \\Vert_2^2$ \n",
    "\n",
    "which means \n",
    "\n",
    "$t^2 -  \\big \\Vert \\mathbf x \\big \\Vert_2^2\\geq 0$ \n",
    "\n",
    "i.e. we conclude:  $[\\mathbf x, \\mathbf x] \\geq 0 $ \n",
    "\n",
    "similarly for $\\mathbf y$  there is the constraint that:  \n",
    "\n",
    "$u \\geq \\big \\Vert \\mathbf y \\big \\Vert_2$\n",
    "\n",
    "and as before, we recognize that $[\\mathbf y, \\mathbf y] \\geq 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assuming OP fixes the domain to allow $x_i \\geq 1$, \n",
    "\n",
    "the nicest proof I can think of is a bit  sophisticated: the answer is to consider log reciprocal x elements, i.e.   $y_i := \\log\\big(\\frac{1}{x_i}\\big)$ \n",
    "\n",
    "in vector form, ordered from largest to smallest for convenience:  \n",
    "$\\mathbf y = \\begin{bmatrix}\n",
    "\\log\\big(\\frac{1}{x_1}\\big)\\\\ \n",
    "\\log\\big(\\frac{1}{x_2}\\big)\\\\ \n",
    "\\vdots  \\\\\n",
    "\\log\\big(\\frac{1}{x_{n-1}}\\big)\\\\ \n",
    "\\log\\big(\\frac{1}{x_{n}}\\big)\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "the optimal vector is  \n",
    "\n",
    "$\\mathbf y' = \\begin{bmatrix}\n",
    "\\log\\big(\\frac{1}{1}\\big)\\\\ \n",
    "\\log\\big(\\frac{1}{1}\\big)\\\\ \n",
    "\\vdots  \\\\\n",
    "\\log\\big(\\frac{1}{1}\\big)\\\\ \n",
    "\\log\\big(\\frac{1}{2^n}\\big)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0\\\\ \n",
    "0\\\\ \n",
    "\\vdots  \\\\\n",
    "0\\\\ \n",
    "\\log\\big(\\frac{1}{2^n}\\big)\n",
    "\\end{bmatrix}$  \n",
    "because $\\mathbf y' \\succeq \\mathbf y$ for any feasible $\\mathbf y$ where $\\succeq$ denotes (strong) majorization. I.e.  If you consider the sum of the first k items in $\\mathbf y'$ for $k\\in\\{1,2,...,n-1\\}$ it is zero which is at least as big as any partial sum for any other configuration. \n",
    "\n",
    "And  \n",
    "$\\mathbf 1^T \\mathbf y' = \\mathbf 1^T\\mathbf y= y_1 + y_2 + ... + y_n = \\log\\big(\\frac{1}{2^n}\\big)$  \n",
    "in all cases.  \n",
    "\n",
    "The mapping $u \\mapsto e^u$ is (strictly) convex, and let $g$ be given by $g\\big(\\mathbf y\\big) = e^{y_1} + e^{y_2} + ... + e^{y_n}$.   \n",
    "\n",
    "$g$ is thus schur convex and  \n",
    "$\\big(\\sum_{k=1}^{n-1} 1\\big)+\\frac{1}{2^n} = n-1 + \\frac{1}{2^n} = g\\big(\\mathbf y'\\big) \\geq g\\big(\\mathbf y\\big)$   \n",
    "for any allowable $\\mathbf y$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assuming OP fixes the domain to allow $x_i \\geq 1$, unfortunately the only clean proof I can think of is rather sophisticated: the answer is to consider reciprocal x elements $y_i = \\frac{1}{x_i}$  and optimal vector  \n",
    "\n",
    "$\\mathbf y' = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "\\vdots  \\\\\n",
    "1\\\\ \n",
    "\\frac{1}{2^n}\n",
    "\\end{bmatrix}$  \n",
    "because $\\mathbf y' \\succeq_w \\mathbf y$ for any feasible $\\mathbf y$  where $\\succeq_w$ denotes weak majorization (or log-majorization).  If you consider the sum of the first k items in $\\mathbf y'$ for $k\\in\\{1,2,...,n-1\\}$ it is necessarily bigger than any other $\\mathbf y$ since the maximal value of each component is 1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Claim:  \n",
    "\n",
    "backwards Cauchy Schwarz:  \n",
    "$[\\mathbf x, \\mathbf x]^\\frac{1}{2}[\\mathbf y, \\mathbf y]^\\frac{1}{2}\\leq [\\mathbf x, \\mathbf y] $\n",
    "\n",
    "since the Lower bound is real nonnegative, we may square both sides and get the following: \n",
    "\n",
    "$[\\mathbf x, \\mathbf x][\\mathbf y, \\mathbf y]\\leq [\\mathbf x, \\mathbf y]^2 $\n",
    "\n",
    "# Proof:  \n",
    "\n",
    "note that if $\\mathbf x = \\mathbf 0$ and $\\mathbf y = \\mathbf 0$, then this statement reduces to $t^2 u^2 = (tu)^2$ which is true by commutativity of scalar multiplication.  If just one of those vectors is equal to the zero vector (say it is $\\mathbf y = \\mathbf 0$), what we have is $\\big(t^2 - \\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big)u^2 = (tu)^2 - u^2 \\big \\Vert \\mathbf x \\big \\Vert_2^2 \\leq (tu)^2$ which is equivalent to saying $ -1* u^2 \\big \\Vert \\mathbf x \\big \\Vert_2^2 \\leq 0$ which we verify is true by inspection because negative one times two nonzero numbers gets a result that is at most zero.  \n",
    "\n",
    "the rest of the positing assumes that $\\mathbf x \\neq \\mathbf 0$ and $\\mathbf y \\neq \\mathbf 0$\n",
    "\n",
    "- - - - \n",
    "- - - - \n",
    "*intial setup* \n",
    "\n",
    "$\\mathbf Z := \\bigg[\\begin{array}{c|c}\n",
    "\\mathbf x & \\mathbf y\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "Now $\\mathbf Z$ is either a rank 1 or rank 2 matrix.  \n",
    "\n",
    "Hence $\\mathbf Z^T \\mathbf Z$ is real symmetric positive semi-definite if $\\mathbf Z$ is rank one, or it is real symmetric positive definite if $\\mathbf Z$ is rank 2.  \n",
    "\n",
    "Now consider the  rank one, real symmetric positive semi definite matrix given by \n",
    "\n",
    "$\\mathbf T =  \\begin{bmatrix}\n",
    "t\\\\ \n",
    "u\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "t\\\\ \n",
    "u\n",
    "\\end{bmatrix}^T= \\begin{bmatrix}\n",
    "t^2 & tu \\\\ \n",
    "tu & u^2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "note that because $\\mathbf T$ is $2$ x $2$ but has rank $= 1$, this means there is some $\\mathbf v \\neq \\mathbf 0$ where $\\mathbf {Tv} = \\mathbf 0$.  \n",
    "\n",
    "Combining these two matrices, we define the real symmetric matrix given by $\\mathbf B$, below.  \n",
    "\n",
    "\n",
    "$\\mathbf B := \\mathbf T - \\mathbf Z^T \\mathbf Z = \\begin{bmatrix}\n",
    "t^2 - \\mathbf x^T \\mathbf x & tu - \\mathbf x^T \\mathbf y \\\\ \n",
    "tu - \\mathbf y^T \\mathbf x  & u^2- \\mathbf y^T \\mathbf y\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "[\\mathbf x, \\mathbf x] &  [\\mathbf x, \\mathbf y]\\\\ \n",
    "[\\mathbf x, \\mathbf y]  & [\\mathbf y, \\mathbf y]\n",
    "\\end{bmatrix}  $ \n",
    "\n",
    "- - - - \n",
    "- - - - \n",
    "$\\mathbf B$*'s first eigenvalue clue*   \n",
    "\n",
    "Being real symmetric, $\\mathbf B$'s eigenvectors can (and will) be chosen to be mutually orthonmal.  All eigenvalues are strictly real.  \n",
    "\n",
    "We know that that $\\text{trace}\\big(\\mathbf B\\big) \\geq 0$.  Why?\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) = b_{1,1} + b_{2,2} = [\\mathbf x, \\mathbf x] + [\\mathbf y, \\mathbf y]\\geq 0$\n",
    "\n",
    "where we stated at the top that:  \n",
    "$[\\mathbf x, \\mathbf x] \\geq 0$   \n",
    "and     \n",
    "$[\\mathbf y, \\mathbf y] \\geq 0$ \n",
    "\n",
    "Since $\\mathbf B$ is real symmetric and the sum of its eigenvalues, given by its trace, is $\\geq 0$ we know that $\\mathbf B$ has at least one non-negative eigenvalue.  \n",
    "\n",
    "- - - - \n",
    "- - - - \n",
    "$\\mathbf B$*'s second eigenvalue clue*    \n",
    "\n",
    "now consider what happens when we look at $\\mathbf v^T \\mathbf B \\mathbf v$, where $\\mathbf v$ is that non-zero vector from $\\mathbf T$'s nullspace.  \n",
    "\n",
    "$\\mathbf v^T \\mathbf B \\mathbf v = \\mathbf v^T \\mathbf T \\mathbf v - \\mathbf v^T \\big(\\mathbf Z^T \\mathbf Z\\big) \\mathbf v = \\mathbf v^T \\big(\\mathbf T \\mathbf v\\big) - \\mathbf v^T \\big(\\mathbf Z^T \\mathbf Z\\big) \\mathbf v = 0 - \\mathbf v^T \\big(\\mathbf Z^T \\mathbf Z\\big) \\mathbf v$\n",
    "\n",
    "recall that $\\mathbf Z^T \\mathbf Z$ is positive semi-definite when $\\mathbf Z$ is rank one and positive definite when $\\mathbf Z$ is rank two.  \n",
    "\n",
    "This means that $\\mathbf v^T \\big(\\mathbf Z^T \\mathbf Z\\big) \\mathbf v \\geq 0$  in the rank one case and $\\mathbf v^T \\big(\\mathbf Z^T \\mathbf Z\\big) \\mathbf v \\gt 0$ in the rank two case.  \n",
    "\n",
    "Hence we see:\n",
    "\n",
    "if $\\mathbf Z$ is rank one, this means that $\\mathbf B$ must have at least one non-positive eigenvalue  \n",
    "$\\mathbf v^T \\mathbf B \\mathbf v = - \\mathbf v^T \\big(\\mathbf Z^T \\mathbf Z\\big) \\mathbf v \\leq 0$ \n",
    "\n",
    "\n",
    "if $\\mathbf Z$ is rank two, this means that $\\mathbf B$ must have at least one negative eigenvalue  \n",
    "$\\mathbf v^T \\mathbf B \\mathbf v = - \\mathbf v^T \\big(\\mathbf Z^T \\mathbf Z\\big) \\mathbf v \\lt 0$\n",
    "\n",
    "- - - -\n",
    "- - - - \n",
    "*Putting it all together*\n",
    "\n",
    "if $\\mathbf Z$ is rank one, $\\mathbf B$ has at least one non-negative eigenvalue and at least one non-positive eigevalue.  This means $\\mathbf B$ either is singular, or it has one positive eigenvalue and one negative eigenvalue. Recalling that the determinant of $\\mathbf B$ is the product of $\\mathbf B$'s eigenvalues, we see that this means $\\det\\big(\\mathbf B\\big) \\leq 0$.\n",
    "\n",
    "At first glance, if $\\mathbf Z$ is rank two, $\\mathbf B$ has at least one non-negative eigenvalue and at least one negative eigenvalue.  In fact we can strengthen this: we know there is at least one negative eigenvalue but $\\text{trace}\\big(\\mathbf B\\big) \\geq 0$.  This means $\\mathbf B$ *must* have one positive eigenvalue and one negative eigenvalue.  Hence $\\det\\big(\\mathbf B\\big) \\lt 0$.\n",
    "\n",
    "using this, we know in the first (rank one) case:\n",
    "\n",
    "$\\det\\big(\\mathbf B\\big) = b_{1,1}b_{2,2} -b_{1,2}b_{2,1} = [\\mathbf x, \\mathbf x][\\mathbf y, \\mathbf y]- [\\mathbf x, \\mathbf y]^2 \\leq 0 $\n",
    "\n",
    "re-arranging terms we have:  \n",
    "$[\\mathbf x, \\mathbf x][\\mathbf y, \\mathbf y] \\leq [\\mathbf x, \\mathbf y]^2  $\n",
    "\n",
    "\n",
    "and in the second (rank two) case we have a strict inequality: \n",
    "\n",
    "$\\det\\big(\\mathbf B\\big) = b_{1,1}b_{2,2} -b_{1,2}b_{2,1} = [\\mathbf x, \\mathbf x][\\mathbf y, \\mathbf y]- [\\mathbf x, \\mathbf y]^2 \\lt 0 $\n",
    "\n",
    "giving us:  \n",
    "$[\\mathbf x, \\mathbf x][\\mathbf y, \\mathbf y]\\lt  [\\mathbf x, \\mathbf y]^2  $\n",
    "\n",
    "if $\\mathbf x$ and $\\mathbf y$ are linearly independent.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An alternative (and better) proof:  \n",
    "\n",
    "\n",
    "# Claim:  \n",
    "\n",
    "backwards Cauchy Schwarz:  \n",
    "$[\\mathbf x, \\mathbf x]^\\frac{1}{2}[\\mathbf y, \\mathbf y]^\\frac{1}{2}\\leq [\\mathbf x, \\mathbf y] $\n",
    "\n",
    "since the lower bound is real nonnegative, we may square both sides and get the following: \n",
    "\n",
    "$[\\mathbf x, \\mathbf x][\\mathbf y, \\mathbf y]\\leq [\\mathbf x, \\mathbf y]^2$\n",
    "\n",
    "# Proof:  \n",
    "\n",
    "we import the \"Preamble\" from the above and proceed as follows:\n",
    "\n",
    "because each of the two terms on the \\text{LHS} are real non-negative, we upperbound the \\text{LHS} with $GM \\leq AM$. But first, without a loss of generality we want to assume $t = 1$ and $u =1$.  As indicated in the Preamble either $t$ and $u$ are each positive, or one or both is zero.  If both are zero the above claim reduces to an equality, i.e. $0 = 0$.  If one is zero, we still have $0=0$.  For anything else, that means $t \\gt 0$ and $u \\gt 0$.  From here, we note the following:   if $t \\neq 1$ and/or $u \\neq 1$  but both are positive, then we may multiply both sides by $\\frac{1}{t}$ and $\\frac{1}{u}$ without altering the inequality, i.e. that \n",
    "\n",
    "$\\frac{1}{t}\\frac{1}{u}[\\mathbf x, \\mathbf x]^\\frac{1}{2}[\\mathbf y, \\mathbf y]^\\frac{1}{2}\\leq \\frac{1}{t}\\frac{1}{u}[\\mathbf x, \\mathbf y]$\n",
    "\n",
    "and distributing these scalars, we write this out as:\n",
    "\n",
    "$\\frac{1}{t}\\frac{1}{u} \\text{LHS} =  \\frac{1}{t}\\big(t^2 - \\mathbf x^T \\mathbf x\\big)^{\\frac{1}{2}}\\frac{1}{u}\\big(u^2 - \\mathbf y^T \\mathbf y\\big)^{\\frac{1}{2}}= \\big(1 - \\frac{1}{t^2}\\mathbf x^T \\mathbf x\\big)^{\\frac{1}{2}}\\big(1 - \\frac{1}{u^2}\\mathbf y^T \\mathbf y\\big)^{\\frac{1}{2}}\\leq \\big(1 - (\\frac{1}{t}\\mathbf x)^T (\\frac{1}{u}\\mathbf y) \\big)= \\frac{1}{t}\\frac{1}{u}\\big(tu - \\mathbf x^T \\mathbf y \\big)  =\\frac{1}{t}\\frac{1}{u}\\text{RHS} $\n",
    "\n",
    "\n",
    "and in *this* setup, we can proceed we can run the argument using $(\\frac{1}{t}\\mathbf x)$ and $(\\frac{1}{u}\\mathbf y)$, with the implied t values for our 'new' $x$ and $v$ each being $1$.   Consequently, we proceed where, **without a loss of generality we assume** $t= 1$ **and **$ u =1$.      \n",
    "\n",
    "revisiting our claim, we can upper bound the $\\text{LHS}$ with $\\text{GM} \\leq \\text{AM}$ \n",
    "\n",
    "$\\text{LHS} $  \n",
    "$= \\big([\\mathbf x, \\mathbf x][\\mathbf y, \\mathbf y]\\big)^\\frac{1}{2}$  \n",
    "$\\leq \\text{AM} $  \n",
    "$= \\frac{1}{2} \\big([\\mathbf x, \\mathbf x] + [\\mathbf y, \\mathbf y]\\big) $  \n",
    "$= \\frac{1}{2}\\big(1 - \\big \\Vert \\mathbf x \\big \\Vert_2^2 + 1 - \\big \\Vert \\mathbf y \\big \\Vert_2^2\\big)$  \n",
    "$= 1 - \\frac{1}{2}\\big \\Vert \\mathbf x \\big \\Vert_2^2 - \\frac{1}{2}\\big \\Vert \\mathbf y \\big \\Vert_2^2 $    \n",
    "$\\leq 1 -\\mathbf x^T\\mathbf y $  \n",
    "$= [\\mathbf x, \\mathbf y] $  \n",
    "$= \\text{RHS} $\n",
    "\n",
    "where the first inequality is true by $\\text{GM} \\leq \\text{AM}$.  Thus if we can prove:\n",
    "\n",
    " $1 - \\frac{1}{2}\\big \\Vert \\mathbf x \\big \\Vert_2^2 - \\frac{1}{2}\\big \\Vert \\mathbf y \\big \\Vert_2^2 \\leq 1 -\\mathbf x^T\\mathbf y $\n",
    "\n",
    "then we are done.  \n",
    "\n",
    "\n",
    "After subtracting $1$ from each side, we realize that the above statement is equivalent to: \n",
    "\n",
    " $ - \\frac{1}{2}\\big \\Vert \\mathbf x \\big \\Vert_2^2 - \\frac{1}{2}\\big \\Vert \\mathbf y \\big \\Vert_2^2 \\leq  -\\mathbf x^T\\mathbf y $\n",
    " \n",
    "and equivalently, after multiplying each side by $-2$: \n",
    "\n",
    "$2\\mathbf x^T\\mathbf y \\leq   \\big \\Vert \\mathbf x \\big \\Vert_2^2 + \\big \\Vert \\mathbf y \\big \\Vert_2^2  $\n",
    " \n",
    "which we know is true over Reals, by Cauchy Schwarz (as proven in chapter 1).  \n",
    "- - - -\n",
    "*Alternative close:*  \n",
    "\n",
    "while noting the symmetry of the real inner product, we can verify the above final statement is equivalent to the squared length (2 norm) of the vector given by $\\big(\\mathbf x- \\mathbf y\\big)$.  I.e., by construction:  \n",
    "\n",
    "$\\big \\Vert \\big(\\mathbf x- \\mathbf y\\big)\\big \\Vert_2 \\geq 0$\n",
    "\n",
    "square both sides\n",
    "\n",
    "$\\big \\Vert \\big(\\mathbf x- \\mathbf y\\big)\\big \\Vert_2^2 \\geq 0$\n",
    "\n",
    "expand the $\\text{LHS}$:    \n",
    "\n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 + \\big \\Vert \\mathbf y \\big \\Vert_2^2 - 2\\mathbf x^T \\mathbf y  = \\big \\Vert \\big(\\mathbf x- \\mathbf y\\big)\\big \\Vert_2^2 \\geq 0$\n",
    "\n",
    "Thus \n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 + \\big \\Vert \\mathbf y \\big \\Vert_2^2 \\geq 2\\mathbf x^T \\mathbf y$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "On the whole this was an enjoyable chapter.  However, most of the exercises where heavy on Gramm Schmidt and Inner Products.  Given my familiarity with these topics I was able to comlete the majority of the exercises with a few lines of scribbling in the margins of the book, sometimes just noting that the result is the same as one I know from elsewhere.  (E.g. exercise 4.8 is just an abstract way of stating QR factorization.  And ex 4.10 gives a relation that is the same as that on page 135 of \"Thirty Three Miniatures\" though I liked the setup in this book a bit better.)  \n",
    "\n",
    "Thus I did not generally find it worthwhile to type up the results of these exercises in a Jupyter notebook as I was already quite familiar with the bulk of the results.\n",
    "\n",
    "I did quite enjoy reading the book's approach to proving the Minkowski light cone, and then doing the two approaches above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 \n",
    "\n",
    "First, we may expand the (squared) 2 norm  \n",
    "$\\langle \\mathbf x, \\mathbf y \\rangle = \\frac{1}{n} \\sum_{k=0}^{n-1} \\big \\Vert \\mathbf x + \\omega^k \\mathbf y \\big \\Vert_2^2 \\omega^k = \\frac{1}{n} \\sum_{k=0}^{n-1} \\Big(\\big \\Vert \\mathbf x \\big \\Vert_2^2 \\omega^k +  \\big \\Vert  \\mathbf y \\big \\Vert_2^2 \\omega^k + \\langle \\mathbf x, \\omega^k \\mathbf y \\rangle \\omega^k + \\langle \\omega^k \\mathbf y, \\mathbf x \\rangle \\omega^k \\Big)$  \n",
    "\n",
    "Next we split the summation, and make use of conjugate linearity by removing the $\\omega^k$ from the $\\mathbf y$ terms of the inner product\n",
    "\n",
    "$\\langle \\mathbf x, \\mathbf y \\rangle = \\frac{1}{n} \\sum_{k=0}^{n-1} \\Big(\\big(\\big \\Vert \\mathbf x \\big \\Vert_2^2 +  \\big \\Vert \\mathbf y \\big \\Vert_2^2\\big)\\omega^k \\Big) + \\frac{1}{n} \\sum_{k=0}^{n-1} \\Big(\\langle \\mathbf x, \\mathbf y \\rangle \\bar{\\omega}^k\\omega^k\\Big) + \\frac{1}{n} \\sum_{k=0}^{n-1}  \\Big(\\langle  \\mathbf y, \\mathbf x \\rangle \\omega^{k} \\omega^k\\Big)$\n",
    "\n",
    "finally: \n",
    "\n",
    "$\\langle \\mathbf x, \\mathbf y \\rangle = \\Big(\\big( \\big \\Vert \\mathbf x \\big \\Vert_2^2  + \\big \\Vert \\mathbf y \\big \\Vert_2^2 \\big)  \\frac{1}{n} \\big(\\sum_{k=0}^{n-1} \\omega^k\\big) \\Big) +  \\langle \\mathbf x, \\mathbf y \\rangle \\Big(\\frac{1}{n} \\sum_{k=0}^{n-1} 1 \\big)+ \\langle \\mathbf y, \\mathbf x \\rangle\\Big(\\frac{1}{n} \\sum_{k=0}^{n-1} \\omega^{2k} \\Big) = \\big(0\\big) + \\langle \\mathbf x, \\mathbf y \\rangle \\big(1\\big)+ \\big(0\\big) = \\langle \\mathbf x, \\mathbf y \\rangle   $\n",
    "\n",
    "\n",
    "where we recall that $\\sum_{k=0}^{n-1} \\omega^k = 0$, and again with the restriction that natural number $n \\geq 3$  we recall  $\\sum_{k=0}^{n-1} \\omega^{2k} = 0$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.10\n",
    "\n",
    "using slightly different notation: this is the same as problem 3.3 in Matousek and Gaertner's Semidefinite Programming book\n",
    "\n",
    "we are in an n dimensional real or complex vector space and have $k$ mututally orthonormal vectors \n",
    "\n",
    "$\\{\\mathbf u_1, \\mathbf u_2, ..., \\mathbf u_k\\}$   \n",
    "\n",
    "where $1 \\leq k \\leq n $ \n",
    "\n",
    "**claim**  \n",
    "$\\sum_{i=1}^k \\big \\vert  \\mathbf x^* \\mathbf u_i\\big \\vert^2 \\leq \\big \\Vert \\mathbf x \\big \\Vert_2^2= \\mathbf x^* \\mathbf x$  \n",
    "\n",
    "(note: due to homogeneity of the exponent, we can assume WLOG that  $\\big \\Vert \\mathbf x \\big \\Vert_2 = 1$ though it isn't technically needed)  \n",
    "\n",
    "**proof**  \n",
    "Bessel's Inequality comes from first extending this as needed so that we have an orthonormal basis \n",
    "\n",
    "$\\{\\mathbf u_1, \\mathbf u_2, ..., \\mathbf u_k, \\mathbf u_{k+1}, ..., \\mathbf u_n\\}$   \n",
    "\n",
    "since this is a basis we can write $\\mathbf x$ as a linear combination of these vectors.  \n",
    "\n",
    "$\\mathbf x = \\sum_{i=1}^n \\alpha_i \\mathbf u_i$  \n",
    "where for avoidance of doubt, we can left multipy by $\\mathbf u_i^* $ to see \n",
    "\n",
    "$\\mathbf u_i^* \\mathbf x = \\alpha_i \\mathbf u_i^*\\mathbf u_i = \\alpha_i$  \n",
    "- - - \n",
    "so we have \n",
    "$\\mathbf x = \\sum_{i=1}^n \\alpha_i \\mathbf u_i$   \n",
    "\n",
    "now left multiply by $\\mathbf x^*$ and see \n",
    "\n",
    "$\\mathbf x^* \\mathbf x = \\big \\Vert \\mathbf x \\big \\Vert_2^2 = \\sum_{i=1}^n \\alpha_i \\mathbf x^*\\mathbf u_i =  \\sum_{i=1}^n \\alpha_i \\big(\\mathbf u_i^*\\mathbf x\\big)^* = \\sum_{i=1}^n \\alpha_i \\bar{\\alpha_i} = \\sum_{i=1}^n \\big \\vert \\alpha_i\\big \\vert^2 \\geq \\sum_{i=1}^k \\big \\vert \\alpha_i\\big \\vert^2 = \\sum_{i=1}^k \\big \\vert  \\mathbf x^* \\mathbf u_i\\big \\vert^2$    \n",
    "\n",
    "This *is* Bessel's Inequality.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a striking resemblance between the above Bessel's Inequality, and the **Kolmogorov Inequality** as expressed for **independent zero mean** random variables $X_i$ (which form a vector space) with finite variance (which we can interpret as their length).  \n",
    "\n",
    "*remark:*  \n",
    "This inequality is most commonly shown in martingale form, however there is a way of showing it without the machinery of martingales that makes it, while less general, more accessible.  This form may also give us different insights. This is an adaptation of page 156 Feller vol 2 (2nd edition).  \n",
    "\n",
    "where  \n",
    "$S_n := X_1 + X_2 + ... +X_n$  \n",
    "\n",
    "*claim:*  \n",
    "the Kolmogorov inequality states, for any $x \\gt 0$   \n",
    "\n",
    "$Pr\\Big(\\max\\big\\{\\big \\vert S_1\\big \\vert,\\big \\vert S_2\\big \\vert, ...,\\big \\vert S_n\\big \\vert \\big\\} \\gt x \\Big) \\leq E\\big[S_n^2\\big]\\cdot \\frac{1}{x^2}$  \n",
    "\n",
    "*starting items*  \n",
    "First, note  \n",
    "$E\\big[S_k^2\\big] = \\sum_{k=1}^n  E\\big[X_k^2\\big] + \\sum_{k=1}^n \\sum_{j\\neq k} \\cdot E\\big[X_k X_j\\big]= \\sum_{k=1}^n  E\\big[X_k^2\\big] = \\sum_{k=1}^n\\sigma_k^2$  \n",
    "because these are independent zero mean random variables so  \n",
    "$E\\big[X_k X_j\\big] = E\\big[X_k \\big] E\\big[X_j\\big] = 0$  for $j\\neq k$  and  \n",
    "$E\\big[X_k^2 \\big] = \\sigma_k^2$  \n",
    "\n",
    "we begin by partitioning into mutually exclusive events.  \n",
    "If $S_j \\geq x^2 = t$ then there must have been a time of first crossing (i.e. when the previous partial sum was $S_{k-1}\\leq t$ but $S_{k}\\gt t$. Denote these first crossing as $\\{A_1, A_2, ..., A_n\\}$ to indicate that the time of first crossing occurred at times $1, 2, ...., n$ and let $B$ be the event that no crossing occurred.   \n",
    "\n",
    "Then $\\{B\\}\\cup \\{A_1, A_2, ..., A_n\\}$ forms a partition of the probability space or equivalently,  \n",
    "$1 =  \\mathbb I_{B}+\\mathbb I_{A_1}+ \\mathbb I_{A_2}+...+ \\mathbb I_{A_n}$  \n",
    "\n",
    "*lemma*  \n",
    "while   \n",
    "$E\\Big[S_n^2\\big \\vert \\mathbb I_{A_n}\\Big] = E\\Big[S_n^2\\big \\vert \\mathbb I_{A_n}\\Big] $  \n",
    "\n",
    "for $1\\leq j \\leq n-1$  \n",
    "$E\\Big[S_j^2\\big \\vert \\mathbb I_{A_j}\\Big] \\leq  E\\Big[S_n^2\\big \\vert \\mathbb I_{A_j}\\Big] $   \n",
    "because   \n",
    "$E\\Big[S_n^2\\big \\vert \\mathbb I_{A_j}\\Big] = E\\Big[S_j^2\\big \\vert \\mathbb I_{A_j}\\Big] + 2\\sum_{i\\leq j}\\sum_{k\\gt j} E\\Big[X_iX_k \\big \\vert \\mathbb I_{A_j}\\Big] + E\\Big[(S_n - S_j)^2\\big \\vert \\mathbb I_{A_j}\\Big] = E\\Big[S_j^2\\big \\vert \\mathbb I_{A_j}\\Big] + E\\Big[(S_n - S_j)^2\\big \\vert \\mathbb I_{A_j}\\Big]  $     \n",
    "\n",
    "where the cross terms $ E\\Big[X_iX_k \\big \\vert \\mathbb I_{A_j}\\Big]$ evaluate to zero since  \n",
    "$E\\Big[X_iX_k \\big \\vert \\mathbb I_{A_j}\\Big] = E\\Big[E\\big[X_iX_k \\big \\vert X_i \\big ] \\big \\vert \\mathbb I_{A_j}\\Big] =E\\Big[X_i \\cdot E\\big[X_k \\big \\vert X_i \\big ] \\big \\vert \\mathbb I_{A_j}\\Big] = E\\Big[X_i \\cdot 0 \\big \\vert \\mathbb I_{A_j}\\Big] = 0 \\cdot E\\Big[X_i  \\big \\vert \\mathbb I_{A_j}\\Big]= 0 $  \n",
    "\n",
    "where we note $E\\big[X_k \\big \\vert X_i \\big ] = E\\big[X_k\\big ] = 0$   (due to independence and zero mean)  \n",
    "\n",
    "and hence $E\\Big[S_j^2\\big \\vert \\mathbb I_{A_j}\\Big] \\leq  E\\Big[S_n^2\\big \\vert \\mathbb I_{A_j}\\Big] $  is equivalent to observing that \n",
    "$0 \\leq E\\Big[(S_n - S_j)^2\\big \\vert \\mathbb I_{A_j}\\Big] $   \n",
    "which holds because it is a sum of squares and in fact by independence \n",
    "\n",
    "further note that  \n",
    "$E\\Big[(S_n - S_j)^2\\big \\vert \\mathbb  I_{A_j}\\Big] = E\\Big[(S_n - S_j)^2\\Big]$  \n",
    "i.e. it is scalar valued-- though we don't use this fact  \n",
    "\n",
    "\n",
    "*main proof*  \n",
    "inline with a typical markov inequality proof we observe a pointwise bound\n",
    "$t \\cdot \\mathbb I_{A_j} \\leq S_j^2 \\mathbb I_{A_j}$   \n",
    "now we sum take expectations to have the pointwise bound  \n",
    "$t \\cdot P(A_j) = t \\cdot E\\big[\\mathbb I_{A_j}\\big] = E\\big[t \\cdot \\mathbb I_{A_j}\\big] \\leq  E\\big[S_j^2 \\mathbb I_{A_j}\\big]$   \n",
    "and summing over the bound  \n",
    "\n",
    "$t\\cdot Pr\\Big(\\max\\big\\{\\big \\vert S_1\\big \\vert,\\big \\vert S_2\\big \\vert, ...,\\big \\vert S_n\\big \\vert \\big\\}\\gt x \\Big)$  \n",
    "$=t \\cdot P\\big( A_1 \\cup A_2 \\cup ... \\cup A_n\\big)$  \n",
    "$=t \\sum_{j=1}^n  p(A_j) $  \n",
    "$= \\sum_{j=1}^n  t\\cdot p(A_j) $  \n",
    "$\\leq  \\sum_{j=1}^n E\\big[S_j^2 \\mathbb I_{A_j}\\big]$   \n",
    "$=  \\sum_{j=1}^n E\\Big[E\\big[S_j^2 \\mathbb I_{A_j}\\big \\vert \\mathbb I_{A_j}\\big]\\Big]$   \n",
    "$=  \\sum_{j=1}^n E\\Big[\\mathbb I_{A_j} \\cdot E\\big[S_j^2 \\big \\vert \\mathbb I_{A_j} \\big]\\Big]$   \n",
    "$\\leq  \\sum_{j=1}^n E\\Big[\\mathbb I_{A_j} \\cdot E\\big[S_n^2 \\big \\vert \\mathbb I_{A_j} \\big]\\Big]$   \n",
    "$= \\sum_{j=1}^n E\\Big[ E\\big[\\mathbb I_{A_j} S_n^2 \\big \\vert \\mathbb I_{A_j} \\big]\\Big]$   \n",
    "$=  \\sum_{j=1}^n E\\Big[\\mathbb I_{A_j}S_n^2 \\Big]$   \n",
    "$\\leq E\\Big[\\mathbb I_{B}S_n^2 \\Big] +   \\sum_{j=1}^n E\\Big[\\mathbb I_{A_j}S_n^2 \\Big]$   \n",
    "$= E\\Big[\\mathbb I_{B}S_n^2 + \\sum_{j=1}^n \\mathbb I_{A_j}S_n^2 \\Big]$   \n",
    "$= E\\Big[\\big(\\mathbb I_{B}+ \\sum_{j=1}^n \\mathbb I_{A_j}\\big)S_n^2 \\Big]$   \n",
    "$=  E\\Big[\\big(1\\big) \\cdot S_n^2 \\Big]$   \n",
    "$= E\\big[S_n^2 \\big]$   \n",
    "\n",
    "dividing each side by $t$ and subtituting $t = x^2$ then gives the result  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extension**  \n",
    "\n",
    "As an extension consider a similar problem, though we instead no longer require finite variance, but we have $X_i$ as *symmetric* random variables, then  \n",
    "\n",
    "for some $t\\gt 0$  \n",
    "$Pr\\Big(\\max\\big\\{ S_1,S_2, ..., S_n \\big\\} \\gt t \\Big) \\leq 2 \\cdot P\\big(S_n \\geq t\\big)$  \n",
    "\n",
    "remark:  \n",
    "this is exercise 1.8 in Lawler's \"Random Walk and the Heat Equation\"  and is also addressed in a different setup on page 149 of Feller vol 2.  \n",
    "\n",
    "inheriting notation and events from the preceding inequality, except noting that for this problem \n",
    "$P\\big(S_n \\geq t\\big \\vert B\\big) =0$  \n",
    "\n",
    "we consider partitioning the RHS by condititioning on $A_j$'s and observe  \n",
    "\n",
    "$\\frac{1}{2} \\leq  P\\big(S_n \\geq t\\big \\vert A_j\\big)$  \n",
    "the inequality obviously holds when $j=n$ but by symmetry it also holds for lower order $j$  (i.e. the probability of $S_j \\to S_n$ decreasing is exactly one half if there's zero probability of ties and in general at most one half, and further the event $\\Big(S_n \\geq t\\big \\vert A_j \\Big)^C = \\Big(S_n \\lt t\\big \\vert A_j \\Big) $ is a subset of $S_j \\to S_n$ decreasing given $A_j$.  \n",
    "\n",
    "so  \n",
    "$\\frac{1}{2}P\\big(A_j\\big) \\leq P\\big(A_j\\big) P\\big(S_n \\geq t\\big \\vert A_j\\big) =  P\\big(S_n \\geq t, A_j\\big) = P\\big(S_n \\geq t\\big) \\cdot P\\big(A_j\\big \\vert S_n \\geq t\\big)$  \n",
    "and summing over the bound  \n",
    "$\\frac{1}{2}Pr\\Big(\\max\\big\\{ S_1,S_2, ..., S_n \\big\\} \\gt t \\Big) = \\frac{1}{2}\\sum_{j=1}^n P\\big(A_j\\big) \\leq \\sum_{j=1}^n  P\\big(S_n \\geq t, A_j\\big) = P\\big(S_n \\geq t\\big)$  \n",
    "\n",
    "*technical nit*  \n",
    "Conditioning on zero probability events can cause technical problems in some cases.  To address this:  \n",
    "In the event all $P\\big( A_j\\big) = 0$ for all $j$ the inequality is trivially true as $0\\leq 0$.  In the event that $P\\big( A_j\\big) = 0$ holds for a proper subset of all $j$, then we may simply remove then from our set of events to condition on and re-run the argument verbatim (except notationally we'd say the summation is over $j \\in S$).  Since these are all essentially the same with the same results, we assume WLOG that $P\\big( A_j\\big) \\gt 0$ for all $j$ and hence the conditioning remains valid.  A substantially similar argument holds with respect to conditioning in the inequality that preceded this one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
