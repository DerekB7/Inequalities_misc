{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1\n",
    "\n",
    "Mengoli's Inequality\n",
    "\n",
    "$\\frac{1}{x-1} + \\frac{1}{x} + \\frac{1}{x+1} \\gt \\frac{3}{x}$ for all $x \\gt 1$   \n",
    "\n",
    "The suggested approach is to use basic algebra and then verify via Jensen's Inequality.  Finally use the above inequality to prove that Harmonic series doesn't converge to a finite number.   (This last part was subtle)\n",
    "\n",
    "\n",
    "**proof:** \n",
    "\n",
    "$x \\big(\\frac{1}{x-1} + \\frac{1}{x} + \\frac{1}{x+1}\\big) = \\frac{x}{x-1} + 1 + \\frac{x}{x+1} \\gt 3$ for all $x \\gt 1$   \n",
    "\n",
    "subtract 1 from each side and we see:  \n",
    "\n",
    "$\\frac{x}{x-1} + \\frac{x}{x+1} \\gt 2$ for all $x \\gt 1$   \n",
    "\n",
    "$\\frac{x(x+1)}{(x-1)(x+1)} + \\frac{x(x-1)}{(x-1)(x+1)} \\gt 2$ for all $x \\gt 1$   \n",
    "\n",
    "$\\frac{x^2 + x + x^2-x}{(x^2 - 1)} = 2\\frac{x^2}{(x^2 - 1)} \\gt 2$ for all $x \\gt 1$   \n",
    "\n",
    "divide both side by 2 and we see:  \n",
    "\n",
    "$\\frac{x^2}{(x^2 - 1)} \\gt 1$ for all $x \\gt 1$   \n",
    "\n",
    "which we can verify as \n",
    "\n",
    "$x^2 \\gt x^2 - 1$ or equivalently $0 \\gt -1$ \n",
    "\n",
    "This proves the inequality.  The subtle part is applying it to the harmonic series.  \n",
    "\n",
    "- - - -\n",
    "Here is the proof of Mengoli's inequality using convexity  \n",
    "\n",
    "here our function $f$ is \n",
    "\n",
    "$f(X) = \\frac{1}{X}$\n",
    "\n",
    "notice that \n",
    "$f''(x) = \\frac{2}{x^3} \\gt 0$ for all $x \\gt 0$  \n",
    "\n",
    "hence our function $f$ is convex (with respect to our domain of all $x \\gt 1$) \n",
    "\n",
    "By applying Jensen's Inequality, we know \n",
    "\n",
    "$ E[f(X]) \\gt f(E[X]) $  \n",
    "\n",
    "Now for the expected value of $X$, consider the set of possible values that occur with equiprobability $X = \\{x-1, x, x+1\\}$\n",
    "\n",
    "$E[X]= \\frac{1}{3}\\big((x-1) + (x)+ (x+1)\\big) = \\frac{1}{3}\\big(3x\\big) = x$\n",
    "\n",
    "\n",
    "$\\frac{1}{3}\\big(\\frac{1}{x-1} + \\frac{1}{x} + \\frac{1}{x+1}\\big) = E[f(X])  \\gt  f(E[X]) =  \\frac{1}{x} $  \n",
    "\n",
    "multiply each side by 3 and we get \n",
    "\n",
    "$\\frac{1}{x-1} + \\frac{1}{x} + \\frac{1}{x+1} \\gt \\frac{3}{x}$  \n",
    "\n",
    "\n",
    "- - - -\n",
    "\n",
    "The Harmonic series is given by:\n",
    "\n",
    "$H_{\\infty}= \\sum_{k=1}^\\infty \\frac{1}{k} = 1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5} + \\frac{1}{6 } + \\frac{1}{7} + \\frac{1}{8} + \\frac{1}{9} + \\frac{1}{10} +... $\n",
    "\n",
    "for a contradiction, assume this series is finite, i.e. $H_{\\infty} \\lt \\infty$  \n",
    "\n",
    "$H_{\\infty} = \\sum_{k=1}^\\infty \\frac{1}{k}= 1 + \\Big(\\big(\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4}\\big) + \\big(\\frac{1}{5} + \\frac{1}{6 } + \\frac{1}{7}\\big) + \\big(\\frac{1}{8} + \\frac{1}{9} + \\frac{1}{10}\\big) +... \\Big)$\n",
    "\n",
    "To simplify, subtract one from each side:\n",
    "\n",
    "$H_{\\infty} - 1 = \\sum_{k=2}^\\infty \\frac{1}{k}= \\Big(\\big(\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4}\\big) + \\big(\\frac{1}{5} + \\frac{1}{6 } + \\frac{1}{7}\\big) + \\big(\\frac{1}{8} + \\frac{1}{9} + \\frac{1}{10}\\big) +... \\Big)$\n",
    "\n",
    "\n",
    "now apply Mengoli's Inequality  \n",
    "\n",
    "$ H_{\\infty} - 1 = \\Big(\\big(\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4}\\big) + \\big(\\frac{1}{5} + \\frac{1}{6 } + \\frac{1}{7}\\big) + \\big(\\frac{1}{8} + \\frac{1}{9} + \\frac{1}{10}\\big) +... \\Big) \\gt \\Big(\\big(\\frac{3}{3}\\big) + \\big(\\frac{3}{6}\\big) + \\big(\\frac{3}{9}\\big) + ... \\Big)= \\Big(1 + \\frac{1}{2} + \\frac{1}{3} + ... \\Big)= H_{\\infty}$\n",
    "\n",
    "\n",
    "Thus we have   \n",
    "$H_{\\infty} - 1 \\gt H_{\\infty} $  \n",
    "or  \n",
    "$-1 \\gt 0$   \n",
    "\n",
    "which is a contradiction.  Hence we know $H_{\\infty}$ does not converge to a finite number.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2\n",
    "\n",
    "**claim: **\n",
    "\n",
    "where $x_1, x_2, x_3 \\gt 0$ and $ x_1 + x_2 + x_3 = 1$\n",
    "\n",
    "$64 \\leq \\big(1 + \\frac{1}{x_1}\\big)\\big(1 + \\frac{1}{x_2}\\big)\\big(1 + \\frac{1}{x_3}\\big)$\n",
    "\n",
    "*commentary: *\n",
    "\n",
    "This problem was somehow simple yet the modelling approach was opaque to me.  I could easily see the (uniform) equality case, but it was a bit too subtle how to build the solution inequality off of that.  I had to look up the solution and expand / adapt it and chew on it a bit.  My hope is that I can port of a lot of the modelling / thinking from here into information theory problems.  \n",
    "\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "find the convex function $f(t)$ and build off of it.  \n",
    "\n",
    "in this case we would guess that a logarithm is involved -- i.e. taking the log of each side will allow us to convert from a product to a sum.  Specifically consider:\n",
    "\n",
    "$f(t) = \\log\\big(1 + \\frac{1}{t}\\big)$  \n",
    "\n",
    "the logarithm itsel is negative convex.  However if we put a bit more work in and check the second derivative of this function, we see \n",
    "\n",
    "$f''(t) = -\\frac{1}{(1+t)^2} + \\frac{1}{t^2} \\gt 0$ for $t \\gt 0$\n",
    "\n",
    "i.e. $\\frac{1}{t^2} \\gt \\frac{1}{(1+t)^2} $ for $t \\gt 0$\n",
    "\n",
    "hence our function $f$ is convex.  \n",
    "\n",
    "Thus by Jensen's Inequality, we have $f(E[t]) \\leq E[f(t)]$  \n",
    "\n",
    "- - -\n",
    "note: for convenience, we actually use $\\log_2(x)$ not $\\ln(x)$ but recall that one is just a positive scalar multiple of the other and hence does not change any inequality relationships.\n",
    "- - - -\n",
    "\n",
    "returning to our claim, we take $\\log_2$ of each side and then multiply each side by $\\frac{1}{3}$  \n",
    "\n",
    "$LHS = \\frac{1}{3}\\log_2(64) =  \\frac{1}{3}6 = 2 \\leq \\frac{1}{3}\\Big(\\log_2\\big(1 + \\frac{1}{x_1}\\big)+ \\log_2\\big(1 + \\frac{1}{x_2}\\big)+ \\log_2\\big(1 + \\frac{1}{x_3}\\big)\\Big) = E[f(x)] = RHS $\n",
    "\n",
    "Applying Jensen's Inequality, we note that the RHS is bounded below by $f(E[x])$.   \n",
    "\n",
    "$E[x] = \\frac{1}{3}(x_1 + x_2 + x_3) =\\frac{1}{3}(1)= \\frac{1}{3}$ \n",
    "\n",
    "i.e. the RHS is bounded below by $f(\\frac{1}{3})$\n",
    "\n",
    "$LHS = 2 = \\log_2(4) = \\log_2(1 + 3)  =  \\log_2(1 + \\frac{1}{\\frac{1}{3}}) =   f(\\frac{1}{3})   =  f(E[x])  \\leq E[f(x)]  = RHS$\n",
    "\n",
    "thus the LHS equals the lower bound set by Jensen's Inequality.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Investment Inequalities \n",
    "(a refinement of $\\text{GM} \\leq \\text{AM}$)\n",
    "\n",
    "where $0 \\lt r_k \\lt \\infty$, if an events of one dollar in year k grows to $1 + r_k$  dollars at the end of the year, $r_k$ is the investment return in year k.  \n",
    "\n",
    "where $V = (1+r_1)(1+r_2)...(1+r_n)$\n",
    "\n",
    "$ r_G = (r_1r_2...r_n)^\\frac{1}{n}$ (i.e. geomean of investment returns)  \n",
    "$ r_A = \\frac{1}{n}(r_1 + r_2 + ... + r_n)$   (i.e. arithmetic mean of investment returns)   \n",
    "$V = \\prod_{k=1}^n (1+r_k)$   \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$(1+r_G)^n \\leq \\prod_{k=1}^n (1+r_k) \\leq (1+r_A)^n$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "take $\\frac{1}{n}$th root of all terms, recalling that we're dealing in reals, and in fact every term is positive: \n",
    "\n",
    "$1 + r_G \\leq \\big(\\prod_{k=1}^n (1+r_k)\\big)^{\\frac{1}{n}} = V^{\\frac{1}{n}} \\leq 1+r_A$\n",
    "- - - -  \n",
    "*begin commentary:  *  \n",
    "we could subtract 1 from each of these terms and get:\n",
    "$r_G \\leq V^{\\frac{1}{n}} - 1 \\leq r_A$\n",
    "\n",
    "and hence from this perspective, we are are trying to prove a refinement of the $\\text{GM} \\leq \\text{AM}$ inequality.  \n",
    "*end commentary:  *  \n",
    "\n",
    "*subsequent note from Chapters 12 and 13:  *\n",
    "The Left Hand side inequality $(1+r_G)^n \\leq \\prod_{k=1}^n \\big(1+r_k\\big)$ can be interpretted / proven directly with Muirhead's inequality, after expanding the product and matching the sums of terms with same exponential degree.  (Note: as is common with Muirhead, we could unpack this expression somewhat more, and prove with $\\text{GM} \\leq \\text{AM}$.)  \n",
    "\n",
    "\n",
    "The Right Hand side inequality $\\prod_{k=1}^n (1+r_k) \\leq (1+r_A)^n$ can be interpretted/ proven by expanding the product and again comparing terms with same exponential degrees.  From here, referencing ex 13.4, we'd recognize the negative Schur Convexity (i.e. Schur concavity) of elementary sums, and the RHS with the arithmetic mean is majorized by the center, and hence greater than or equal to it, for an given collection of terms with the same degree.  \n",
    "\n",
    "- - - -  \n",
    "\n",
    "\n",
    "First, proving the lower bound:  $1 + r_G \\leq \\big(\\prod_{k=1}^n (1+r_k)\\big)^{\\frac{1}{n}}$\n",
    "\n",
    "$e^{y_k} := r_k$\n",
    "or equivalently  (where we use the natural log for convenience):\n",
    "\n",
    "$y_k = \\log(r_k)$\n",
    "\n",
    "\n",
    "$1 + r_G =  1 + (r_1r_2...r_n)^\\frac{1}{n} = 1 + r_1^\\frac{1}{n} r_2^\\frac{1}{n}...r_n^\\frac{1}{n}= 1+ \\big(e^{y_1}\\big)^\\frac{1}{n}\\big(e^{y_2}\\big)^\\frac{1}{n} ...\\big(e^{y_n}\\big)^\\frac{1}{n}=1+ \\big(e^{\\frac{y_1}{n}}\\big)\\big(e^{\\frac{y_2}{n}}\\big) ...\\big(e^{\\frac{y_n}{n}}\\big) = 1 + e^{\\frac{1}{n}\\sum_{k=1}^n y_k} $\n",
    "\n",
    "$1 + r_G = 1 + e^{E[Y]}$\n",
    "\n",
    "at this point it is quite helpful to 'flip' to using sums, via logarithms, recalling that logarithm is well defined and monotonically increasing over our domain (and we get invertibility for free) and hence preserves ordering \n",
    "\n",
    "\n",
    "$f\\Big(E\\big[Y\\big]\\Big) = \\log(1+ r_G) = \\log(1 + e^{E[Y]}) \\leq \\frac{1}{n}\\log(V) = \\frac{1}{n}\\sum_{k=1}^n  \\log(1+ e^{y_k}) = E[\\log(1+ e^{Y})] = E\\Big[f\\big(Y\\big)\\Big]$   \n",
    "\n",
    "to be certain: when we consider the real valued function  \n",
    "\n",
    "$f(x) = \\log(1 + e^x)$  \n",
    "\n",
    "$f'(x) = \\frac{e^x}{1 + e^x}$   \n",
    "$f''(x) = \\frac{e^x}{(1 + e^x)^2} \\gt 0$ \n",
    "\n",
    "and hence the function is strictly convex by virtue of the fact that the second derivative is strictly positive.  \n",
    "\n",
    "by application of Jensen's Inequality, we know\n",
    "\n",
    "$f\\Big(E\\big[Y\\big]\\Big)  \\leq E\\Big[f\\big(Y\\big)\\Big]$ \n",
    "\n",
    "which proves that \n",
    "\n",
    "$\\log(1 + e^{E[Y]}) \\leq E[\\log(1+ e^{Y})]= \\frac{1}{n}\\log(V)$ \n",
    "\n",
    "where, given the strict convexity, there is equality **iff ** $r_1 = r_2 = ... = r_n$ or equivalently: $Y$ takes on only one value \n",
    "\n",
    "*now for the right hand side of the inequality*  \n",
    "we want to prove  \n",
    "\n",
    "$V^{\\frac{1}{n}} \\leq 1+r_A = 1+ E[R]$ \n",
    "\n",
    "or equivalently in logarithm form,  \n",
    "\n",
    "$\\frac{1}{n}\\log(V) = \\frac{1}{n}\\sum_{k=1}^n \\log(1+r_k)  \\leq \\log(1+r_A)$ \n",
    "\n",
    "now we consider the function \n",
    "\n",
    "$g(x) = 1 + \\log(x)$  \n",
    "\n",
    "and we verify it is negative convex by the second derivative test.  I.e. we observe:  \n",
    "\n",
    "$g'(x) = \\frac{1}{1+x}$  \n",
    "$g''(x) = \\frac{-1}{(1+x)^2}\\lt 0$  \n",
    "\n",
    "hence by application of Jensen's Inequality, we know $E[g(X)] \\leq g(E[X])$ \n",
    "\n",
    "which proves that \n",
    "\n",
    "$\\frac{1}{n}\\sum_{k=1}^n \\log(1+r_k) = E\\Big[\\log\\big(1+ R\\big)\\Big] \\leq \\log\\big(1+ E\\big[R\\big]\\Big)=  \\log\\Big(1+r_A\\Big)$  \n",
    "\n",
    "Giving us the second leg of the inequality.  Thus we've proved:  \n",
    "\n",
    "$\\log(1+r_G) \\leq \\frac{1}{n}\\log\\big(\\prod_{k=1}^n (1+r_k)\\big) = \\log\\Big(\\big(\\prod_{k=1}^n (1+r_k)\\big)^\\frac{1}{n}\\Big)  \\leq \\log(1+r_A)$\n",
    "\n",
    "from here, it is simply a matter of exponentiating all terms, and then raising them to the nth power, hence proving our original claim that:  \n",
    "\n",
    "$(1+r_G)^n \\leq \\prod_{k=1}^n (1+r_k) \\leq (1+r_A)^n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Superadditivity of Geometric Mean\n",
    "\n",
    "This was originally proven in ex 2.11.  The problem seeks to use Jensen's inequality to do the proof...\n",
    "\n",
    "*remarks: It seems worth noting that the problem introduces a very simple but powerful technique at step one --> convert the problem from* $2n$ to $n$ *variables, by dividing out the a's right away.  This solution however does not directly use Jensen's Inequality, and instead reduces the problem to that of 6.4 which is solved above.  *\n",
    "\n",
    "**claim:**\n",
    "\n",
    "for real non-negative $a_j$ and $b_j$ for $j = \\{1, 2, ..., n\\}$ we have the superadditivity of the geometric mean as:\n",
    "\n",
    "$(a_1a_2...a_n)^\\frac{1}{n} + (b_1b_2...b_n)^\\frac{1}{n} \\leq \\Big((a_1 + b_1)(a_2 + b_2)...(a_n + b_n)\\Big)^{\\frac{1}{n}}$\n",
    "\n",
    "**proof:** \n",
    "\n",
    "note again the equality conditions, by inspection if $n=1$.  Also if all values are zero, then there is an equality.  If there is a single $a_j = 0$ or $b_j=0$ the $LHS = 0$ and the RHS involves a product of real non-negative numbers, hence it the inequality holds.  Now for the rest of the problem assume each $a_j$ and $b_j$ are strictly positive. \n",
    "\n",
    "let $\\gamma = (a_1a_2...a_n)^\\frac{1}{n}$\n",
    "\n",
    "\n",
    "$\\frac{1}{\\gamma} \\Big((a_1a_2...a_n)^\\frac{1}{n} + (b_1b_2...b_n)^\\frac{1}{n}\\Big) \\leq \\frac{1}{\\gamma} \\Big((a_1 + b_1)(a_2 + b_2)...(a_n + b_n)\\Big)^{\\frac{1}{n}}$\n",
    "\n",
    "equivalently, let \n",
    "\n",
    "$r_j = \\frac{b_j}{a_j}$  \n",
    "\n",
    "*This is the key technique outlined in the solutions section of the book, as we now have reduced the scope to n strictly positive variables called*  $\\{r_1, r_2, ... , r_n\\}$\n",
    "\n",
    "our inequality becomes \n",
    "\n",
    "$1 + (r_1r_2...r_n)^\\frac{1}{n}  = \\big(\\prod_{i=1}^n 1\\big)^\\frac{1}{n} + \\big(\\prod_{i=1}^n r_i \\big)^\\frac{1}{n} \\leq \\big(\\prod_{i=1}^n (1 + r_i) \\big)^\\frac{1}{n} =  \\Big(( 1 + r_1)(1 + r_2)...(1 + r_n)\\Big)^{\\frac{1}{n}} $\n",
    "\n",
    "From here define\n",
    "\n",
    "$r_G := (r_1r_2...r_n)^\\frac{1}{n}$.  \n",
    "\n",
    "We then raise each side to the nth power, giving us: \n",
    "\n",
    "$\\big(1 + r_G\\big)^n \\leq ( 1 + r_1)(1 + r_2)...(1 + r_n)$\n",
    "\n",
    "which is the left hand side of the inequality proven above, in 6.4.  \n",
    "\n",
    "**subsequent note:** \n",
    "\n",
    "it maybe perhaps be best to prove the super-additivity of the Geometric Mean via quasi-linearisation (see chp 8), then run the above argument backward, to prove that \n",
    "\n",
    "$\\big(1 + r_G\\big)^n \\leq ( 1 + r_1)(1 + r_2)...(1 + r_n)$  \n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\big(1 + r_G\\big) \\leq \\big(( 1 + r_1)(1 + r_2)...(1 + r_n)\\big)^\\frac{1}{n}$  \n",
    "\n",
    "i.e. super aditivity of the geometric mean gives us \n",
    "\n",
    "$1 + r_G = \\big(\\prod_{i=1}^n 1\\big)^\\frac{1}{n} + \\big(\\prod_{i=1}^n r_i\\big)^\\frac{1}{n} = \\text{GM}\\big(\\mathbf 1\\big) + \\text{GM}\\big(\\mathbf r\\big) \\leq \\text{GM}\\big(\\mathbf 1 + \\mathbf r\\big) =\\big(\\prod_{i=1}^n (1+r_i)\\big)^\\frac{1}{n}= \\big(( 1 + r_1)(1 + r_2)...(1 + r_n)\\big)^\\frac{1}{n}$  \n",
    "\n",
    "\n",
    "As for the right hand side of the inequality, this can be proven by observing\n",
    "\n",
    "$\\big(\\prod_{k=1}^n (1+r_k)\\big)^\\frac{1}{n} \\leq  \\frac{1}{n}\\Big( \\sum_{k=1}^n (1+r_k)\\Big) = \\frac{1}{n}\\big( \\sum_{k=1}^n 1\\big) + \\frac{1}{n}\\big(\\sum_{k=1}^n r_k\\big) = \\big(1\\big) + \\big(r_A\\big) = \\big(1+r_A\\big)$    \n",
    "\n",
    "by $\\text{GM} \\leq \\text{AM}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**subsequent note: using Chapter 12 materials  ** \n",
    "\n",
    "$\\big(1 + r_G\\big)^n \\leq ( 1 + r_1)(1 + r_2)...(1 + r_n)$ \n",
    "\n",
    "we could expand this to say \n",
    "\n",
    "$\\big(1 + r_G\\big)^n = 1 + \\binom{n}{1}r_G^1 + \\binom{n}{2}r_G^2 + ... + \\binom{n}{n-1}r_G^{n-1} + r_G^n \\leq 1 + e_1(\\mathbf r) + e_2(\\mathbf r) + .... + e_{n-1}(\\mathbf r) + r_G^n = ( 1 + r_1)(1 + r_2)...(1 + r_n)$ \n",
    "\n",
    "where of course $e_{0}(\\mathbf r) = 1 $ and $e_{n}(\\mathbf r) = r_G^n$  \n",
    "\n",
    "now the LHS and RHS each have a 1.  Both have $r_G^n$ as well.  However, via use of Maclaurin's Inequalities, we have term by term (weak) dominance \n",
    "\n",
    "for $1 \\leq j \\leq n-1$\n",
    "\n",
    "$\\binom{n}{j}r_G^j \\leq e_j(\\mathbf r) $  \n",
    "i.e. divide each side by $\\binom{n}{j}$ to recover Maclaurin's Inequalities as \n",
    "\n",
    "$\\Big(E_n\\big(\\mathbf r\\big)^\\frac{1}{n}\\Big)^j = \\Big(r_G\\Big)^j \\leq \\frac{e_j(\\mathbf r)}{\\binom{n}{j}} = E_j\\big(\\mathbf r\\big) $  \n",
    "\n",
    "or in standard Maclaurin inequality form:    \n",
    "$E_n\\big(\\mathbf r\\big)^\\frac{1}{n} \\leq E_j\\big(\\mathbf r\\big)^\\frac{1}{j} $  \n",
    "\n",
    "- - - - -\n",
    "Also note that we could use a similar approach to prove the RHS of 6.4:  i.e. that \n",
    "\n",
    "$\\prod_{k=1}^n (1+r_k) =  1 + e_1(\\mathbf r) + e_2(\\mathbf r) + .... + e_{n-1}(\\mathbf r) + e_{n}(\\mathbf r)  \\leq  1 + \\binom{n}{1}r_A^1 + \\binom{n}{2}r_A^2 + ... + \\binom{n}{n-1}r_A^{n-1} + r_A^{n} = (1+r_A)^n$  \n",
    "\n",
    "here the value of 1 matches on each side, but then for $1 \\lt j \\leq n$ we have term by term (weak) dominance, specifically that:  \n",
    "\n",
    "\n",
    "$e_j(\\mathbf r) \\leq \\binom{n}{j}r_A^j$   \n",
    "or in more common form    \n",
    "$E_j(\\mathbf r) = \\frac{e_j(\\mathbf r)}{\\binom{n}{j}} \\leq r_A^j = E_1(\\mathbf r)^j $   \n",
    "\n",
    "and in standard Maclaurin inequality form:  \n",
    "$E_j(\\mathbf r)^\\frac{1}{j} \\leq E_1(\\mathbf r) $    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.6 and 6.7 -- maybe write these up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.8 has some interesting ideas on the geometry of convexity but the errata sheet notes that the problem is fatally flawed.  The author indicated he'd fix this in 2007 (a decade ago) but did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6.9 Pending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Verification of Convexity\n",
    "\n",
    "a point made in the chapter but not heavily reinforced is that in many cases we can verify convexity directly from applying the definition and seeing how convex combinations relate.  \n",
    "\n",
    "**example verification**  \n",
    "\n",
    "\n",
    "constraining our selve to $x \\gt 0$, \n",
    "\n",
    "we know the function $f$ given by $f(x) = \\frac{1}{x}$  is strictly convex.  Typically we verify this via the usage of a second derivative test.  But there is another very simple approach.  We just need   \n",
    "\n",
    "**claim to prove **  \n",
    "$w_1f(x_1) + w_2f(x_2) \\geq f(w_1 x_1 +w_2 x_2)$  \n",
    "\n",
    "\n",
    "and in particular since we are claiming strict convexity, we want to show that the equality holds **iff** $x_1 = x_2$  \n",
    "\n",
    "note as always, we have $w_1 + w_2 =1$ and $ w_1, w_2 \\gt 0$  (the resulting inequality is trivially equal if either $w_1 = 0$ or $w_2 = 0$ hence these cases are excluded to avoid muddying up our analysis of the sharpness of the inequality)  \n",
    "\n",
    "**proof **  \n",
    "$w_1f(x_1) + w_2f(x_2) = \\frac{w_1}{x_1} + \\frac{w_2}{x_2} \\geq \\frac{1}{w_1 x_1 + w_2 x_2} =  f(w_1 x_1 +w_2 x_2)$  \n",
    "\n",
    "multiplying each side by $(w_1 x_1 + w_2 x_2)$ (which of course is positive), we 'merely' need to prove \n",
    "\n",
    "$\\frac{w_1(w_1 x_1 + w_2 x_2)}{x_1} + \\frac{w_2(w_1 x_1 + w_2 x_2)}{x_2} \\geq 1$   \n",
    "\n",
    "this unpacks into \n",
    "\n",
    "$w_1^2 + \\frac{w_1 w_2 x_2}{x_1} + \\frac{w_1 w_2 x_1}{x_2} + w_2^2 = w_1^2 + w_2^2 + w_1 w_2\\big(\\frac{x_2}{x_1} + \\frac{x_1}{x_2}\\big) \\geq w_1^2 + w_2^2 + w_1 w_2\\big(\\frac{x_1}{x_1} + \\frac{x_2}{x_2}\\big)= w_1^2 + w_2^2 + w_1 w_2(2) = \\big(w_1 + w_2\\big)^2 = 1$   \n",
    "\n",
    "where we apply the re-arrangement inequality (see e.g. Exercise 5.7 in Chp_5.ipynb) to justify \n",
    "\n",
    "$\\big(\\frac{x_2}{x_1} + \\frac{x_1}{x_2}\\big)\\geq \\big(\\frac{x_1}{x_1} + \\frac{x_2}{x_2}\\big)$   \n",
    "and note that the inequality is strict when $x_1 \\neq x_2$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.10 Proof that the Second Derivative criterion confirms Convexity\n",
    "\n",
    "*a guided proof problem, with additional information added by your author*  \n",
    "\n",
    "definition of convexity:   \n",
    "$ (p) f\\big(x_1\\big) + (1-p)f\\big(x_2\\big) \\geq f\\big((p)x_1 + (1-p)x_2\\big) $  \n",
    "\n",
    "where $0\\leq p \\leq 1$\n",
    "- - - -\n",
    "**some special cases as preliminaries**  \n",
    "\n",
    "note that if p =0 we see: $ 0 f(x_1) + (1)f(x_2) \\geq (1)f(x_2)$ which is to say $f(x_2) \\geq f(x_2)$  which is trivially true.  Similarly if p = 1 we get $f(x_1) \\geq f(x_1)$ also always true.  \n",
    "\n",
    "And of course if $x_1 = x_2$, we have \n",
    "\n",
    "$ p f(x_1) + (1-p)f(x_2)  = p f(x_1) + (1-p)f(x_1) = f(x_1) \\geq  f(x_1) =  f(p x_1 + (1-p)x_1) = f(px_1 + (1-p)x_2) $  \n",
    "which is also trivially true.  All of these trivial cases are true irrespective of whether the function is actually convex, because they simply state that $f(x) = f(x)$ or that $f(x) \\leq f(x)$ .  \n",
    "\n",
    "It is worth mentioning that if $f(x) = c$, i.e. f is a constant mapping, when can also identify that the relationship is trivially true. \n",
    "\n",
    "And if  \n",
    "$f(x) = ax $   \n",
    "i.e. a linear mapping, then \n",
    "\n",
    "$p f(x_1) + (1-p)f(x_2) \\geq p\\big(f(x_1)\\big) + (1-p)\\big(f(x_2)\\big) =  f\\big(px_1\\big) + f\\big((1-p)x_2\\big) = f\\big(px_1 + (1-p)x_2\\big) $ \n",
    "\n",
    "which is true via linearity.  And if we have $f(x) = ax + c$, i.e. an affine mapping, then we verify it is true because it is a linear combination of the preceding two inequalities (the most direct approach is to simply subtract $c$ from each 'chunk' of the above inequality.  Subtracting a constant from each does not change the inequality, but it does center things and we recover the linear mapping.  The only remaining thread is to remember that $p c + (1-p)c = (p + 1-p)c = c$.)  \n",
    "\n",
    "All other functions where $f''(x) \\geq 0$ are considered in this post -- the functions that can be modelled as affine have already been dealt with and hence are excluded from the analysis that follows.  (Otherwise they would induce a few too many special cases that your author thought distracted from the writeup.)  \n",
    "\n",
    "**Thus with trivial cases out of the way, we restrict ourselves to the case where:  $0 \\lt p \\lt 1 $ and $x_1 \\neq x_2$.  Without a loss of generality, assume $x_1 \\lt x_2$.**\n",
    "\n",
    "- - - - -\n",
    "\n",
    "**claim: **   \n",
    "\n",
    "if $f$ is at least twice differentiable and $f''(x) \\geq 0$ for $x \\in [a,b]$ then $f$ must be convex over $[a,b]$. \n",
    "\n",
    "**proof:  **  \n",
    "\n",
    "$\\mu := p x_1 + (1-p)x_2$ \n",
    "\n",
    "Now evaluate $f(x_1), f(x_2), f(\\mu)$.  \n",
    "\n",
    "Note that for any legal $x_1$ and $x_2$, $\\mu$ must also be in our domain of $[a,b]$ and because of our restriction on $p$, we have $x_1 \\lt \\mu \\lt x_2$.  \n",
    "\n",
    "Hence we have 3 unique data points to apply our function on.  From here we can use a Vandermonde matrix $\\mathbf W$, and find the unique quadratic polynomial $Q(x)$ to interpolate those data points.  \n",
    "\n",
    "$Q(x) = t_0 + t_1 x + t_2 x^2$  \n",
    "\n",
    "where $\\mathbf W \\mathbf t= \\mathbf y$ \n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & \\mu  & \\mu^2 \\\\ \n",
    "1 & x_1 & x_1^2 \\\\ \n",
    "1 & x_2 & x_2^2 \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "t_0 \\\\ \n",
    "t_1\\\\ \n",
    "t_2\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "y_0 \\\\ \n",
    "y_1\\\\ \n",
    "y_2\n",
    "\\end{bmatrix}  = \\begin{bmatrix}\n",
    "f(\\mu) \\\\ \n",
    "f(x_1)\\\\ \n",
    "f(x_2)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "(See \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb\" in the Linear Algebra folder, for a **lot** more information on Vandermonde Matrices.  One key result is that $\\mathbf W^{-1}$ must exist because $\\mu$, $x_1$, $x_2$ are all unique.)  \n",
    "\n",
    "Thus we know that $\\mathbf t = \\mathbf W^{-1} \\mathbf y$ \n",
    "\n",
    "By construction, then, $Q(x_1) = f(x_1)$, $Q(x_2) = f(x_2)$ and $Q(\\mu) = f(\\mu)$.\n",
    "\n",
    "Now consider the function $g$ with domain $[a,b]$, where $g(x) = f(x) - Q(x)$.  \n",
    "\n",
    "$g(x)$ has (at least) three zeros in our domain, because $0 = g(x_1) = g(x_2) = g(\\mu)$.  \n",
    "\n",
    "Note that $f$ could, for instance, itself be a quadratic polynomial, in which case *all* values of $g(x)$ would be identically zero.    \n",
    "\n",
    "It follows that  \n",
    "$g'(x) = f'(x) - Q'(x) = f'(x) - (t_1 + 2t_2 *x)$  \n",
    "$g''(x) = f''(x) - Q''(x) = f''(x) - 2t_2$  \n",
    "\n",
    "Our goal is to show that there must be (at least) one $x^* \\in [a,b]$ where $g''(x^*) = 0$.  (In the case where $f$ is quadratic any $x \\in [a,b]$ will do).  \n",
    "\n",
    "- - - - - \n",
    "*first: a more intuitve argument which requires a continuous second derivative* $f''(x)$   \n",
    "\n",
    "In general where $g(x) $ is not identically zero, there must be some $x^*$ where $g''(x^*) = 0$.  Why?  Because $g(x)$ has at least 3 zeros this means that our real valued, continuous, function crossed zero 3 times.  Put differently, there is at least one local minimum and one local maximum in there, and local maxima at say $x_{ma}$ have $g''(x_{ma}) \\leq 0$ and the local minima at $x_{mi}$ has $g''(x_{mi}) \\geq 0$.  Hence we know of a valid point where the second derivative is $\\geq 0$ and another where the second derivative is $\\leq 0$ -- and by continuity of our second derivative mapping, this means there must be a point where the second derivative is $= 0$.  (I.e. $ + \\to -$ must go through $0$ and $- \\to +$ must go through zero, and by continuity, go through zero means there must be at least one point $x^*$ where $g''(x^*) = 0$.  In the event $g''(x_{mi}) = 0$ we can just set $x^* := x_{mi}$ or if or $g''(x_{ma}) = 0$ then $x^* := x_{ma}$.) \n",
    "\n",
    "This argument works, however we sacrifice a *touch* of generality here because it requires $g''(x)$ (or equivalently, $f''(x)$) to be continuous.  \n",
    "\n",
    "- - - - - \n",
    "*second: a slightly more general argument which merely requires * $f''(x)$ *to exist*\n",
    "\n",
    "If we want the slightly more general claim, where $g''(x)$ exists but may not be continuous, then we apply Rolle's Theorem twice.  Specifically,  \n",
    "\n",
    "We know $g(x_1) = g(\\mu) = g(x_2) = 0$. By Rolle's theorem, there must be some $x_l \\in (x_1, \\mu)$  where $g'(x_l) =0$ and there must be some $x_u \\in (\\mu, x_2)$ where $g'(x_u) = 0$.  Now we apply Rolle's theorem once more, this time noting that because $g'(x_l) = g'(x_u) = 0$, then there must be some $x^* \\in (x_l, x_u)$ where $g''(x^*) = 0$.  This proves the existence of $x^*$ in a slightly more general setting.\n",
    "\n",
    "- - - - - \n",
    "\n",
    "**Finally:**\n",
    "\n",
    "we consider $g''(x) = f''(x) - Q''(x) = f''(x) - 2 t_2 $ and evaluate at $x^*$. \n",
    "\n",
    "$g''(x^*) = f''(x^*) - 2t_2 = 0$\n",
    "\n",
    "thus \n",
    "$f''(x^*) = 2t_2$\n",
    "\n",
    "But we know $f''(x) \\geq 0$ for any $x$ in our domain, thus $2t_2 = f''(x^*) \\geq 0$ \n",
    "\n",
    "Key finding:  we now know \n",
    "\n",
    "$t_2 \\geq 0$  \n",
    "\n",
    "From here we 'unpack' and examine $t_2$. Where $\\mathbf e_3$ is the 3rd standard basis vector (i.e. 3rd column slice of the 3 x 3 Identity Matrix)\n",
    "\n",
    "$t_2 = \\mathbf e_3^T \\mathbf t =  \\mathbf e_3^T\\big(\\mathbf W^{-1}\\mathbf y\\big) =  \\mathbf e_3^T\\left[\\begin{matrix} * & * & *\\\\\n",
    "* & * & *\\\\\n",
    "\\frac{1}{x_{1} x_{2} - x_{1} \\mu - x_{2} \\mu + \\mu^{2}} & \\frac{1}{x_{1}^{2} - x_{1} x_{2} - x_{1} \\mu + x_{2} \\mu} &  \\frac{1}{-x_{1} x_{2} + x_{1} \\mu + x_{2}^{2} - x_{2} \\mu}\\end{matrix}\\right]\\begin{bmatrix}\n",
    "f(\\mu) \\\\ \n",
    "f(x_1)\\\\ \n",
    "f(x_2)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$t_2 =  \\big( \\frac{f(\\mu)}{x_{1} x_{2} - x_{1} \\mu - x_{2} \\mu + \\mu^{2}}\\big) +\\big( \\frac{f(x_1)}{x_{1}^{2} - x_{1} x_{2} - x_{1} \\mu + x_{2} \\mu}\\big) +\\big( \\frac{f(x_2)}{-x_{1} x_{2} + x_{1} \\mu + x_{2}^{2} - x_{2} \\mu}\\big) = \\big( \\frac{f(\\mu)}{(\\mu-x_1)(\\mu-x_2)}\\big) +\\big( \\frac{f(x_1)}{(x_1 - x_2)(x_1 - \\mu)}\\big) +\\big( \\frac{f(x_2)}{(x_2 - x_1)(x_2 - \\mu)}\\big) \\geq 0 $  \n",
    "\n",
    "Hence we know \n",
    "\n",
    "$\\big( \\frac{f(x_1)}{(x_1 - x_2)(x_1 - \\mu)}\\big) +\\big( \\frac{f(x_2)}{(x_2 - x_1)(x_2 - \\mu)}\\big) \\geq - \\big( \\frac{f(\\mu)}{(\\mu-x_1)(\\mu-x_2)}\\big) $\n",
    "\n",
    "*Cleanup items:* notice that $(x_1 - \\mu) \\lt 0$, $(x_1 - x_2) \\lt 0$ and $(\\mu - x_2) \\lt 0$  \n",
    "We adjust each parenthetical term in our denominators to be strictly positive, factoring out $-1$ as needed.  After cleaning up, our inequality can be written as:  \n",
    "\n",
    "$ \\frac{1}{(x_2 - x_1)(\\mu - x_1 )}f(x_1) + \\frac{1}{(x_2 - x_1)(x_2 - \\mu)}f(x_2) \\geq  \\frac{f(\\mu)}{(\\mu-x_1)(x_2 - \\mu)} $\n",
    "\n",
    "we now clear the denominator of the RHS (which is positive), giving us  \n",
    "\n",
    "$ \\frac{(\\mu-x_1)(x_2 - \\mu)}{(x_2 - x_1)(\\mu - x_1 )}f(x_1) + \\frac{(\\mu-x_1)(x_2 - \\mu)}{(x_2 - x_1)(x_2 - \\mu)}f(x_2) \\geq  f(\\mu) $\n",
    "\n",
    "which simplifies to\n",
    "\n",
    "$\\big( \\frac{(x_2 - \\mu)}{(x_2 - x_1)}\\big)f(x_1) + \\big(\\frac{(\\mu-x_1)}{(x_2 - x_1)}\\big)f(x_2) \\geq  f(\\mu) $\n",
    "\n",
    "- - - -\n",
    "Three key findings: \n",
    "\n",
    "$0 \\lt \\big( \\frac{(x_2 - \\mu)}{(x_2 - x_1)}\\big) \\lt 1$\n",
    "\n",
    "$0 \\lt \\big( \\frac{(\\mu-x_1)}{(x_2 - x_1)}\\big) \\lt 1$   \n",
    "\n",
    "$\\big(\\frac{(x_2 - \\mu)}{(x_2 - x_1)}\\big) + \\big(\\frac{(\\mu-x_1)}{(x_2 - x_1)}\\big) = \\frac{x_2 - x_1}{(x_2 - x_1)} = 1 $ \n",
    "\n",
    "Thus the two terms satisfy the requirements for being $p$ and $1-p$.  \n",
    "- - - - \n",
    "\n",
    "Accordingly we set $p:= \\frac{(x_2 - \\mu)}{(x_2 - x_1)}$ and observe that this means:  \n",
    "\n",
    "$ (p)f\\big(x_1\\big) + (1-p)f\\big(x_2\\big) \\geq  f\\big(\\mu\\big) = f\\big((p)x_1 + (1-p)x_2\\big)  $  \n",
    "\n",
    "Which is the definition of convexity. \n",
    "\n",
    "We have thus proved that  $f''(x) \\geq 0$ for $x \\in [a,b]$ is sufficient for demonstrating that $f$ is convex in $[a,b]$.  \n",
    "\n",
    "- - - - - \n",
    "*extension: * \n",
    "\n",
    "An easy extension is when $f''(x) \\gt 0$, we immediately get strict convexity, because we have \n",
    "\n",
    "$\\big( \\frac{(x_2 - \\mu)}{(x_2 - x_1)}\\big)f(x_1) + \\big(\\frac{(\\mu-x_1)}{(x_2 - x_1)}\\big)f(x_2) \\gt  f(\\mu) $\n",
    "\n",
    "$ (p)f\\big(x_1\\big) + (1-p)f\\big(x_2\\big) \\gt  f\\big(\\mu\\big) $  \n",
    "\n",
    "which is the definition of strict convexity.  \n",
    "\n",
    "- - - - - \n",
    "\n",
    "*epliogue: *\n",
    "\n",
    "The extra cautious reader may want to inspect our assignment for $p$, and see:  \n",
    "\n",
    "$p:= \\frac{(x_2 - \\mu)}{(x_2 - x_1)} = \\frac{(x_2 - (p*x_1 + (1-p)x_2) )}{(x_2 - x_1)} = \\frac{(x_2 - (p*x_1 + x_2 - p*x_2))}{(x_2 - x_1)}  = \\frac{(x_2 - p*x_1 - x_2 + p*x_2)}{(x_2 - x_1)} = \\frac{( p*x_2 - p*x_1 )}{(x_2 - x_1)} = p \\frac{( x_2 - x_1 )}{(x_2 - x_1)} = p$\n",
    "\n",
    "which tells us that our definition of $p$ is not (over) determined, and hence we have full flexibility to choose any $p$ where  $0\\lt p \\lt 1$, and then get the corresponding $\\mu$.  Or equivalently, we may choose any $\\mu$ where $x_1 \\lt \\mu \\lt x_2$ and recover the implied $p$.  \n",
    "\n",
    "- - - - - \n",
    "**Commentary:**  \n",
    "I originally followed the first 2 of the 3 steps -- however I got stuck on the 3rd step as I just had  $\\mathbf e_3^T \\mathbf t =  \\mathbf e_3^T\\big(\\mathbf W^{-1}\\mathbf y\\big)$ but did not actually explicitly show / calculuate $\\mathbf W^{-1}$.  I could of course guess elements of the resulting denominator (via the Vandermonde Determinant), or calculated the relevant coordinate value via Cramer's Rule, but ultimately it seemed that showing, and calculating $\\mathbf W^{-1}$ explicitly (i.e. looking at the components in its bottom row) is very helpful to complete the proof.  Consulting the solution in the back convinced me to explicitly invert $\\mathbf W$.  On the whole this is an extremely good problem.   \n",
    "\n",
    "In some sense this is the best proof of convexity and its links with the second derivative test.  While I still think about the second derivative linkage via Taylor Polynomials with a remainder of $O(n^2)$, in Lagrange form, I don't know the full proof building up to said form.  On the other hand, this approach uses quadratic polynomial interpolation (and a Vandermonde matrix when I do it) which comes quite naturally.  \n",
    "\n",
    "When you peel back all of the various maneuvers, the above approach, at its core, is awfully straightforward.  \n",
    "\n",
    "In this approach, we have two unique input points plus a third unique one generated by the expected value of those two input points.  Hence if we want to do polynomial interpolation, we know from the Vandermonde Matrix writeup that the maximum degree polynomial that is uniquely specified with 3 unique 'data points' is a quadratic.    \n",
    "\n",
    "The approach here vs taylor polynomials is in essence the same:  we are interested in results of a function that can be inferred from special structure in its second derivative -- we approximate the function with an appropriate quadratic polynomial, and then try to prove convexity from there.  \n",
    "\n",
    "some of the symbols generated at the end of this problem come up again in 6.17, 6.18, and 6.19, where the Three Chord Lemma, Near Differentiability of Convex Functions, and Linear Lower Bound of a convex function, all come up.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extension:**  \n",
    "\n",
    "**claim:** \n",
    "When we have strict convexity, Jensen's Inequality becomes an inequality **iff** all values are identical.  \n",
    "\n",
    "i.e. if $f$ is strictly convex and \n",
    "\n",
    "$f\\Big(\\sum_{i=1}^n p_i x_i\\Big) = \\sum_{i=1}^n p_i f\\big(x_i)$ \n",
    "\n",
    "this occurs **iff** $x_1 = x_2 = x_3 = ... = x_n$  \n",
    "\n",
    "*commentary:*  \n",
    "This is a recut of the proof in problem 6.2 (i.e. a 'problem' but not an 'exercise') on page 89 of the book.  The proof as presented in the book was not quite my style, but close.  The underlying setup also seems, to a currious extent, to remind me of the proof underlying strictly diagonally dominant matrices being invertible (see 'Gerschgorin_discs' writeup in Linear Algebra folder).  \n",
    "\n",
    "\n",
    "**proof:  **  \n",
    "\n",
    "note that if all $X$ are identical then $f(X) = c$ and $E[X] = X = \\mu$, giving us $E[f(X)] = E[c] = c = f(X) = f(E[X])$  (i.e. in the constant case, we can interpret $X$ as a degenerate random variable that only is equal to one value and hence is equal to its mean).\n",
    "\n",
    "The trickier part is to prove the other leg of the **iff**, i.e. that equality in Jensen's Inequality with a strictly convex function must imply that all values of $X$ are identical.  \n",
    "\n",
    "now, the definition of strict convexity is \n",
    "$ (q)f\\big(z_1\\big) + (1-q)f\\big(z_2\\big) \\gt  f\\big((q)z_1 + (1-q)z_2\\big) $  \n",
    "\n",
    "where $0 \\lt q \\lt 1$ and $z_1 \\neq z_2$.  \n",
    "\n",
    "We proceed with proof by contradiction.  I.e. suppose not all values of $X = x_i$ are the same yet equality still holds.  \n",
    "\n",
    "Specifically we hypothesize $f\\Big(\\sum_{i=1}^n p_i x_i\\Big) = \\sum_{i=1}^n p_i f\\big(x_i\\big)$, with not all $x_i$ identical.  \n",
    "\n",
    "Since all values are in reals, there must be a maximal value.  (Note: were are dealing with discrete and really finite cases here, but for avoidance of doubt, we can state that $X$ has a finite first moment -- and from here we could instead just select some $x_k \\gt \\bar{X}$-- i.e. since not all $x_i$'s identically match, and the first moment is finite, there must be some value, with positive probability, that is greater than the mean -- and we can pick any of those values. Also, for avoidance of doubt, we can also 'throw out' or ignore any zero probability cases.)  \n",
    "\n",
    "Define the maximal value as $x_k := max\\big(X=x_i\\big)$ and $x_j :\\lt max\\big(X=x_i\\big)$. Hence we've partitioned the random variable into two parts.  \n",
    "\n",
    "\n",
    "Define \n",
    "\n",
    "$q := \\sum_{j} p_j$   \n",
    "$1-q = \\sum_k p_k = 1 - \\sum_j p_j$  \n",
    "\n",
    "and \n",
    "\n",
    "$z_1 = \\frac{1}{q}\\sum_jp_j x_j$\n",
    "\n",
    "$z_2 = \\frac{1}{1-q}\\sum_k p_k x_k = x_k\\frac{1}{1-q}\\sum_k p_k = x_k\\frac{1}{1-q}\\big(1-q\\big) = x_k $  \n",
    "- - - - - \n",
    "*Detour comment: this is awfully similar to Law of Total Expectation territory.  *  \n",
    "With perhaps some notational challenges, we could represent our partition as \n",
    "\n",
    "$X =  \\big(q\\big) \\big(X \\big\\vert X \\neq x_k\\big) + \\big(1-q\\big) \\big(X \\big\\vert X = x_k\\big)$ \n",
    "\n",
    "and by Law of Total Expectation, we have: \n",
    "\n",
    "$E\\big[X\\big]= \\big(q\\big) E\\big[X \\big\\vert X \\neq x_k\\big] + \\big(1-q\\big) E\\big[X \\big\\vert X = x_k\\big] = q z_1 + (1-q)z_2$  \n",
    "- - - - - \n",
    "\n",
    "so revisiting our original equation: \n",
    "\n",
    "$f\\Big(E\\big[X\\big]\\Big)  = f\\Big(\\sum_{i=1}^n p_i x_i\\Big) = f\\Big(q z_1 + (1-q)z_2\\Big) \\lt q f\\big(z_1\\big) + \\big(1-q\\big)f\\big(z_2\\big) $  \n",
    "\n",
    "by strict convexity.  \n",
    "\n",
    "Now we take the right hand side and apply the general case of Jensen's Inequality ( using a $\\leq $ as strict inequality  is not needed, and in fact is what we are trying to prove here!)\n",
    "\n",
    "$q \\Big(f\\big(z_1\\big)\\Big) + \\big(1-q\\big)f\\big(z_2\\big) \\leq q \\Big(\\frac{1}{q}\\sum_jp_j f\\big(x_j\\big)\\Big) + \\big(\\sum_k p_k\\big)f\\big(x_k\\big) = \\sum_jp_j f\\big(x_j\\big) + \\sum_k p_k f\\big(x_k\\big) = \\sum_{i=1}^n p_i f\\big(x_i\\big) $\n",
    "\n",
    "hence we have: \n",
    "\n",
    "$f\\Big(\\sum_{i=1}^n p_i x_i\\Big) \\lt  q f\\big(z_1\\big) + \\big(1-q\\big)f\\big(z_2\\big) \\leq \\sum_{i=1}^n p_i f\\big(x_i)\n",
    "$\n",
    "\n",
    "which contradicts our hypothesis that $f\\Big(\\sum_{i=1}^n p_i x_i\\Big) = \\sum_{i=1}^n p_i f\\big(x_i\\big)$ despite not all \n",
    "$x_i$ being identical.  This completes the proof.  \n",
    "\n",
    "\n",
    "- - - - \n",
    "an equivalent finish is to recall the law of total expecation:\n",
    "\n",
    "$E\\big[X\\big] = \\big(q\\big) E\\big[X \\big\\vert X \\neq x_k\\big] + \\big(1-q\\big) E\\big[X \\big\\vert X = x_k\\big]$ \n",
    "\n",
    "then write our inequality as (where we first apply the definition of strict convexity, and then apply the general non-strict Jensen's Inequality as we interchange the expectation operator and the convex function $f$):    \n",
    "\n",
    "$f\\Big(E\\big[X\\big]\\Big) \\lt \\big(q\\big) f\\big(E\\big[X \\big\\vert X \\neq x_k\\big]\\big) + \\big(1-q\\big) f\\big(E\\big[X \\big\\vert X = x_k\\big]\\big) \\leq   \\big(q\\big) E\\Big[f \\big(X \\big\\vert X \\neq x_k\\big)\\Big] + \\big(1-q\\big)E\\Big[ f\\big(X \\big\\vert X = x_k\\big)\\Big] = E\\Big[f\\big(X\\big)\\Big] $\n",
    "\n",
    "noting that via linearity: \n",
    "\n",
    "$\\big(q\\big) E\\Big[f \\big(X \\big\\vert X \\neq x_k\\big)\\Big] + \\big(1-q\\big)E\\Big[ f\\big(X \\big\\vert X = x_k\\big)\\Big]  = E\\Big[\\big(q\\big) f \\big(X \\big\\vert X \\neq x_k\\big) + \\big(1-q\\big) f\\big(X \\big\\vert X = x_k\\big)\\Big]= E\\Big[f\\big(X\\big)\\Big]$\n",
    "\n",
    "which tells us $f\\Big(E\\big[X\\big]\\Big) \\lt E\\Big[f\\big(X\\big)\\Big]$ and contradicts our hypothesis of equality despite having an $X$ with not all probability concentrated on one value.  \n",
    "\n",
    "While the above notation is not quite satisfactory, it perhaps generalizes a bit better to related cases.  \n",
    "\n",
    "E.g., if we for some reason wanted to use this argument for a continuous random variable (or perhaps a mixed discrete and continuous one), we'd work with the CDF of said variable, and since the variable by assumption does not have all probability on one value, it must have some $v$ where  $0\\lt CDF(v) \\leq 1$, i.e. some $v$ where the CDF is greater than zero and less than 1. From here we'd do the partition such that $E\\big[X\\big] = \\big(q\\big) E\\big[X \\big\\vert X \\leq v\\big] + \\big(1-q\\big) E\\big[X \\big\\vert X \\gt v\\big]$, and then proceed in essentially the same manner.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extension:**  \n",
    "\n",
    "A differentiable strictly convex function cannot have 2 points with the same tangent line \n",
    "\n",
    "(note: this ties in later with some deeper concepts related to Legendre Transforms which come up in an exercise in chapter 9).  \n",
    "\n",
    "\n",
    "I.e. suppose for a contradiction that we have some strictly convex function $f$ and the tangent line at $x_1$ is the same as the tangent line at $x_2$, where $x_1 \\lt x_2$.   \n",
    "\n",
    "Due to continuity (uncountably many) points exist between $x_1$ and $x_2$.  Amongst these points, there is either (a) at least one point $f(\\mu)$ that is also on the same tangent line, or (b) no point $f(\\mu)$ on the same tangent line because all $f(x)$ for $x \\in (x_1, x_2)$  is *above* said tangent line or (c) no point $f(\\mu)$ on the same tangent line because  $f(x)$ for all $x \\in (x_1, x_2)$ is  *below* said tangent line.  \n",
    "\n",
    "We know by the previous exercise that (a) cannot be true-- i.e. we have $\\mu = (p) x_1 +(1-p)x_2$ and as outlined a couple ways in the above, we know that $f(\\mu) \\lt (p)f(x_1) + (1-p)f(x_2)$ which means the 3 points are not on the same line. I.e. (a) results in a contradiction\n",
    "\n",
    "We know that (b) cannot be true for several reasons: most basically it breaks the defintion relating points in the convex hull of a function -- i.e. we have a strictly convex function so we know \n",
    "\n",
    "*key convex statement 1*: $f(\\mu) \\lt (p)f(x_1) + (1-p)f(x_2)$ \n",
    "\n",
    "but if we sketch the tangent line we see that (b) is equivalent to saying \n",
    "\n",
    "$f(\\mu) \\gt \\text{tangent line value for (x :=}\\mu\\text{)   } = (p)f(x_1) + (1-p)f(x_2)$   \n",
    "\n",
    "which is a contradiction \n",
    "\n",
    "\n",
    "Finally we know that (c) cannot be true.  Based on exercise 6.19, we know that the tangent line for a differentiable convex function is a linear lower bound on the function.  Case (c) states that $f(\\mu)$ lies strictly below this lower bound (tangent line), which is a contradiction.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "= = = = = = \n",
    "# 6.17 Three Chord Lemma\n",
    "- - - -\n",
    "\n",
    "commentary: \n",
    "\n",
    "This maps to picture (B) as part of figure 6.1 on page 88.  It is a strikingly simple derivation, though I missed it and am flushing out the notes to the official solution here. This and the following ideas are interesting, however I wonder a bit --these bounds on difference quotients are interesting but how do you know a function is convex? By far the most common case comes from the second derivative test.  Technically the second derivative test isn't needed but it is generally not so easy to prove convexity without it. \n",
    "\n",
    "Even though th problem statement does not say it, this heavily reuses ideas from problem 6.10 -- accordingly I reuse notation for my solution to that problem. \n",
    "- - - -\n",
    "\n",
    "claim: \n",
    "where $f$ is convex\n",
    "\n",
    "$\\frac{f(\\mu) - f(a)}{ \\mu - a}$  $\\leq \\frac{f(x_2) - f(x_1)}{b-a} $  $\\leq \\frac{f(x_2) - f(\\mu)}{x_2 - \\mu} $\n",
    "\n",
    "proof: \n",
    "\n",
    "by convexity, we know \n",
    "\n",
    "$f(\\mu) \\leq (p)f(x_1) + (1-p)f(x_2)$  \n",
    "\n",
    "for the left hand side, subtract $f(x_1)$ from each side, which gives us\n",
    "\n",
    "$f(\\mu) - f(x_1) \\leq (p-1)f(x_1) + (1-p)f(x_2) = -(1-p)f(x_1) + (1-p)f(x_2) = (1-p)\\big(f(x_2) - f(x_1)\\big)$  \n",
    "\n",
    "as in 6.10, \n",
    "\n",
    "$(1-p) := \\frac{(\\mu - x_1)}{(x_2 - x_1)}$\n",
    "\n",
    "\n",
    "$f(\\mu) - f(x_1) \\leq \\frac{(\\mu - x_1)}{(x_2 - x_1)} \\big(f(x_2) - f(x_1)\\big)$ \n",
    "or equivalently\n",
    "\n",
    "$\\frac{f(\\mu) - f(x_1)}{\\mu - x_1} \\leq \\frac{f(x_2) - f(x_1)}{x_2 - x_1} $ \n",
    "\n",
    "This is the left portion of the inequality. \n",
    "\n",
    "- - - - \n",
    "now revisit our inequality:  \n",
    "\n",
    "$f(\\mu) \\leq (p)f(x_1) + (1-p)f(x_2)$  \n",
    "\n",
    "subtract $f(x_2)$ from each side \n",
    "\n",
    "$f(\\mu) - f(x_2) \\leq (p)f(x_1) + -(p)f(x_2) = p\\big(f(x_1) - f(x_2)\\big)$  \n",
    "\n",
    "multiply each side by - 1\n",
    "\n",
    "$ f(x_2) - f(\\mu) \\geq p\\big( f(x_2) - f(x_1) \\big)$  \n",
    "\n",
    "as in 6.10:\n",
    "\n",
    "$p:= \\frac{(x_2 - \\mu)}{(x_2 - x_1)}$\n",
    "\n",
    "$ f(x_2) - f(\\mu) \\geq \\frac{(x_2 - \\mu)}{(x_2 - x_1)} \\big( f(x_2) - f(x_1) \\big)$  \n",
    "\n",
    "$ \\frac{f(x_2) - f(\\mu)}{x_2 - \\mu} \\geq \\frac{ f(x_2) - f(x_1)}{x_2 - x_1} $  \n",
    "\n",
    "This is the right hand side of the inequality.  Put these together, and we get our claim: \n",
    "\n",
    "\n",
    "$ \\frac{f(\\mu) - f(x_1)}{\\mu - x_1} \\leq \\frac{ f(x_2) - f(x_1)}{x_2 - x_1} \\leq  \\frac{f(x_2) - f(\\mu)}{x_2 - \\mu}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** It may be worthwhile to type up 6.18 and 6.19 as they are really quite good exercises **  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log sum inequality\n",
    "\n",
    "This is actually my notes on the proof of Theorem 2.7.1 in Cover and Thomas's *Elements of Information Theory* and also ex 13.3.4 of Bremaud's *Discrete Porbability Models and Methods*, Jensen's Inequality is at the core of the problem, so it seemed appropriate to put the proof in here.\n",
    "\n",
    "for non-negative numbers \n",
    "\n",
    "$a_1, a_2, ... $ and $b_1, b_2, ... $ where \n",
    "\n",
    " $0 \\lt \\sum_i a_i = A \\lt \\infty$   \n",
    " $0 \\lt \\sum_i b_i = B \\lt \\infty$ \n",
    " \n",
    "*note: the original problem was posed over n-tuples.  An open item is to upper bound the LHS and confirm absolute convergence in the infinite series setup*  \n",
    " \n",
    " **claim:  ** \n",
    " \n",
    "$\\sum_i a_i \\log\\big(\\frac{a_i}{b_i}\\big) \\geq A \\log\\big(\\frac{A}{B}\\big)$  \n",
    "\n",
    "where the natural $\\log$ or $\\log_2$ is used \n",
    "\n",
    "with equality **iff** $a_i = \\lambda b_i$  \n",
    "\n",
    "(note the usual information theorectic convention that $0\\log 0 = 0$ is used if one of the non-negative numbers is zero)  \n",
    "\n",
    "**proof:  ** \n",
    "\n",
    "$w_i := \\frac{a_i}{A}$  \n",
    "$q_i := \\frac{b_i}{B}$   \n",
    "\n",
    "first, divide both sides by $A$ \n",
    "\n",
    "$\\sum_i w_i \\log\\big(\\frac{a_i}{b_i}\\big) \\geq  \\log\\big(\\frac{A}{B}\\big)$  \n",
    "\n",
    "$\\sum_i \\Big(w_i \\log\\big(a_i\\big) - w_i \\log \\big(b_i\\big)\\Big) \\geq  \\log \\big(A\\big) - \\log \\big({B}\\big)$  \n",
    "\n",
    "$\\Big(\\sum_i w_i \\log\\big(a_i\\big)\\Big) - \\Big(\\sum_i w_i \\log \\big(b_i\\big)\\Big) \\geq  \\log \\big(A\\big) - \\log \\big({B}\\big)$  \n",
    "\n",
    "\n",
    "$\\Big( \\sum_i w_i \\log\\big(a_i\\big)\\Big)-  \\log \\big(A\\big)  \\geq \\Big(\\sum_i w_i \\log \\big(b_i\\big)\\Big)  - \\log \\big({B}\\big)$  \n",
    "\n",
    "\n",
    "$\\Big( \\sum_i w_i \\log\\big(a_i\\big)\\Big)-  \\Big(\\sum_i w_i\\log \\big(A\\big)\\Big)  \\geq \\Big(\\sum_i w_i \\log \\big(b_i\\big)\\Big)  - \\Big(\\sum_i w_i \\log \\big({B}\\big)\\Big)$   \n",
    "\n",
    "\n",
    "$\\Big( \\sum_i w_i \\log\\big(a_i\\big)-   w_i\\log \\big(A\\big)\\Big)  \\geq \\Big(\\sum_i w_i \\log \\big(b_i\\big) - w_i\\log \\big({B}\\big)\\Big)$   \n",
    "\n",
    "$ \\sum_i w_i \\log\\big(\\frac{a_i}{A}\\big)  \\geq \\sum_i w_i \\log \\big(\\frac{b_i}{B}\\big)$   \n",
    "\n",
    "$ \\sum_i w_i \\log\\big(w_i\\big)  \\geq \\sum_i w_i \\log \\big(q_i\\big)$   \n",
    "\n",
    "or \n",
    "\n",
    "$ \\sum_i w_i \\log\\big(\\frac{w_i}{q_i}\\big)  \\geq 0$   \n",
    "\n",
    "we should immediately recognize this as being the same thing as \n",
    "\n",
    "$D_{\\text{KL}}\\big(\\mathbf w \\big \\Vert \\mathbf q\\big) \\geq 0$ \n",
    "\n",
    "i.e. that KL divergence is positive definite-- equal to zero **iff** the two distributions are identical  -- this is equivalent to saying that $\\frac{a_i}{A} = \\frac{b_i}{B} = \\frac{\\lambda a_i}{\\lambda A}$  \n",
    "\n",
    "as a reminder, we may verify this fact about KL divergence via the strict convexity of the negative logarithm \n",
    "\n",
    "$ \\sum_i w_i \\log\\big(\\frac{w_i}{q_i}\\big)  =   \\sum_i- w_i\\log\\big(\\frac{q_i}{w_i}\\big)= E\\Big[-\\log\\big(\\frac{Q}{W}\\big)\\Big] \\geq -\\log\\Big(E\\Big[\\frac{p(Q)}{p(W)}\\Big]\\big) = -\\log\\Big(\\sum_i w_i \\frac{q_i}{w_i}\\Big) = -\\log\\Big(\\sum_i  q_i \\Big) = -\\log\\big(1\\big) = 0$   \n",
    "\n",
    "by Jensen's Inequality.  Recall that the negative logarithm is strictly convex, hence the inequality becomes an equality if and only if $\\frac{q_i}{w_i} = \\alpha$ for all $i$, where $\\alpha$ is some constant.  However, this implies \n",
    "\n",
    "$q_i = \\alpha w_i$ and summing over all $i$ we see \n",
    "$1 = \\sum_i q_i = \\sum_i \\alpha w_i =  \\alpha \\cdot \\sum_i w_i = \\alpha \\cdot 1 = \\alpha $\n",
    "\n",
    "thus the equality case occurs if and only if $q_i = w_i$ for all $i$. \n",
    "\n",
    "The only lingering nits relating to the above are concerned with zero probability events.  It takes a bit of additional work to verify these edge cases involving possible infinity do not alter the equality cases.  Rather than dwelling on them here, we deal with them in the probability folder in one of the information theory writeups under Pinsker's Inequality.  \n",
    "\n",
    "\n",
    "*additionally the log sum inequality holds for countably infinite discrete distributions.  There are slightly shorter arguments which more carefully deal with potential convergence issues that may arise during the derivation. A pending item is to include a shorter, slightly more careful argument to prove the log sum inequality*  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
