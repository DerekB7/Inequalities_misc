{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus: Covariance Matrices must be Symmetric Positive Semi-Definite**    \n",
    "\n",
    "i.e. covariance matrices are Gramm Matrices -- Cauchy Schwarz (or Sylvester's Determinant Criterion) then immediately gives the result for $7.2$.  In general symmetric positive semi-definite matrices have a rich structure so this proof allows 'an alternative way' of getting to a lot of important and well know results from probability theory.   \n",
    "Note:  All random variables stated below are assumed to have a second moment.   \n",
    "\n",
    "First we can center any random variable $Y_k$ and have $X_k := Y_k - E[Y_k]$ -- equivalently we can assume WLOG that our random variables are zero mean.  \n",
    "\n",
    "Let the covariance matrix (for real valued random variables that have a second moment) be given by:  \n",
    "\n",
    "$\\mathbf \\Sigma = \\begin{bmatrix}\n",
    "E[X_1 X_1] & E[X_1 X_2] &\\dots & E[X_1 X_n]\\\\\n",
    " E[X_2 X_1] &  E[X_2 X_2]& \\dots &  E[X_2 X_n] \\\\\n",
    "\\vdots &\\vdots& \\ddots & \\vdots \\\\\n",
    " E[X_n X_1] &  E[X_n X_2] &\\dots & E[X_n X_n]\\\\\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "and let $\\mathbf S$ be a matrix filled with random variables.  \n",
    "\n",
    "$\\mathbf S := \\begin{bmatrix}\n",
    "X_1 X_1 & X_1 X_2 &\\dots & X_1 X_n\\\\\n",
    "X_2 X_1 &  X_2 X_2& \\dots &  X_2 X_n \\\\\n",
    "\\vdots &\\vdots& \\ddots & \\vdots \\\\\n",
    " X_n X_1 &  X_n X_2 &\\dots & X_n X_n\\\\\n",
    "\\end{bmatrix} = \\mathbf {XX}^T$ \n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf X: =\n",
    "\\begin{bmatrix}\n",
    "X_1 \\\\\n",
    "X_2\\\\\n",
    "\\vdots \\\\\n",
    "X_n\\\\\n",
    "\\end{bmatrix} $  \n",
    "i.e. $\\mathbf X$ is a column vector of random variables \n",
    "\n",
    "perhaps abusing notation slightly, notatice that \n",
    "$E\\big[\\mathbf S\\big] = \\mathbf \\Sigma$ \n",
    "(i.e. we apply the expectations operator entrywise)  \n",
    "\n",
    "hence to provide positive semi-definiteness, we need to prove that for any $\\mathbf u \\in \\mathbb R^n$, then $\\mathbf u^T \\mathbf \\Sigma \\mathbf u \\geq 0$  \n",
    "\n",
    "so to prove this we have \n",
    "\n",
    "$\\mathbf u^T \\mathbf \\Sigma \\mathbf u$  \n",
    "$= \\text{trace}\\big( \\mathbf \\Sigma \\mathbf u\\mathbf u^T\\big)$  \n",
    "$= \\sum_{i=1}^n\\sum_{j=1}^n u_i u_j E[X_i X_j]$  \n",
    "$= E[\\sum_{i=1}^n\\sum_{j=1}^n u_i u_j X_i X_j]$  by linearity of expectations   \n",
    "$= E[\\sum_{i=1}^n u_iX_i \\sum_{j=1}^n  u_j  X_j]$  \n",
    "$= E[(\\sum_{i=1}^n u_iX_i)^2]$   \n",
    "\n",
    "(alternative close)  \n",
    "$= E[\\sum_{i=1}^n\\sum_{j=1}^n u_i u_j X_i X_j]$  \n",
    "$= E\\Big[\\text{trace}\\big( \\mathbf S \\mathbf u\\mathbf u^T\\big)\\Big]$  \n",
    "$= E\\Big[\\text{trace}\\big( \\mathbf {XX}^T \\mathbf u\\mathbf u^T\\big)\\Big]$  \n",
    "$= E\\Big[\\text{trace}\\big( \\mathbf {X}^T \\mathbf u\\mathbf u^T\\mathbf X\\big)\\Big]$  \n",
    "$= E\\Big[\\text{trace}\\big( (\\mathbf {X}^T \\mathbf u)(\\mathbf u^T\\mathbf X)\\big)\\Big]$  \n",
    "$= E\\Big[\\text{trace}\\big( (\\mathbf {X}^T \\mathbf u)^2\\big)\\Big]$  \n",
    "$= E\\Big[\\big(\\mathbf {X}^T \\mathbf u\\big)^2\\Big]$  \n",
    "$\\geq 0$  \n",
    "\n",
    "where $\\big(\\mathbf {X}^T \\mathbf u\\big) $ is a scalar quantity (that is a linear combination of $X_k$'s), hence we know that the combined random variable given by  \n",
    "\n",
    "$\\big(\\mathbf {X}^T \\mathbf u\\big)^2 $ is non-negative.  \n",
    "Taking expectations gives the result.   \n",
    "\n",
    "- - - - - \n",
    "corollary: \n",
    "\n",
    "since we know   \n",
    "$\\mathbf \\Sigma \\succeq 0$  \n",
    "\n",
    "we know by direct application of Cauchy Schwarz (or Sylvester's determinant criterion):  \n",
    "\n",
    "$\\text{var}\\big(X_1\\big)\\text{var}\\big(X_2\\big) = E[X_1^2]E[X_2^2] = E[X_1 X_1]E[X_2 X_2] \\geq E[X_1 X_2]^2 = \\text{Cov}\\big(X_1, X_2\\big)^2$  \n",
    "\n",
    "which is exercise 7.2  \n",
    "\n",
    "- - - - \n",
    "*potential extension:*   \n",
    "Since covariance matrices are ubiquitous, in particular for use in the multivariate Gaussian distribution, we can make claims about most (all?) Gramm matrices as if they were covariance matrices.  The multivariate Gaussian in particular has some nice applications in this regard to deal with KL Divergence (since said distribution has largest entropy for an unbounded distribution that has a 2nd moment).  \n",
    "\n",
    "Some of these extensions are exercises in Cover and Thomas.  In general there seems to be an art form that goes \n",
    "\n",
    "claim about real symmetric positive (semi)definite matrix $\\to$ assume its a Covariance matrix $\\to$ KL Divergence /Info theory property that follows  \n",
    "\n",
    "then translate this back to the original problem \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For avoidance of Doubt**  \n",
    "\n",
    "we know that if all second moment exist, then \n",
    "\n",
    "$ E\\big [\\big \\vert X_j X_k\\big \\vert \\big] \\lt \\infty$   for $j \\neq k$  \n",
    "the argument is that over any sample path, we have  \n",
    "\n",
    "$ \\big \\vert X_j X_k\\big \\vert = \\big \\vert X_j\\big \\vert \\big \\vert X_k\\big \\vert = Y_j Y_k = (Y_j^2)^\\frac{1}{2} (Y_k^2)^\\frac{1}{2} \\leq \\frac{1}{2}Y_j^2 + \\frac{1}{2}Y_k^2 =  \\frac{1}{2}X_j^2 + \\frac{1}{2}X_k^2$   \n",
    "by $\\text{GM } \\leq \\text{AM}$  \n",
    "and taking expectations of each side gives   \n",
    "\n",
    "$ E\\big [\\big \\vert X_j X_k\\big \\vert \\big] \\leq E\\big[\\frac{1}{2}X_j^2 + \\frac{1}{2}X_k^2 \\big] = \\frac{1}{2}E\\big[X_j^2\\big] + \\frac{1}{2}E\\big[X_k^2 \\big]\\lt \\infty $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extension: Gram Matrices are Positive (Semi) Definite**  \n",
    "\n",
    "consider the Gram Matrixes \n",
    "\n",
    "$\\mathbf G := \\begin{bmatrix}\n",
    "\\langle \\mathbf x_1, \\mathbf x_1 \\rangle & \\langle \\mathbf x_1, \\mathbf x_2 \\rangle & \\dots & \\langle \\mathbf x_1, \\mathbf x_n \\rangle\\\\\n",
    "\\langle \\mathbf x_2, \\mathbf x_1 \\rangle & \\langle \\mathbf x_2, \\mathbf x_2 \\rangle & \\dots & \\langle \\mathbf x_2, \\mathbf x_n \\rangle\\\\\n",
    "\\vdots &\\vdots& \\ddots & \\vdots \\\\\n",
    "\\langle \\mathbf x_n, \\mathbf x_1 \\rangle & \\langle \\mathbf x_n, \\mathbf x_2 \\rangle & \\dots & \\langle \\mathbf x_n, \\mathbf x_n \\rangle\\\\\n",
    "\\end{bmatrix}$    \n",
    "\n",
    "note that the $\\mathbf x_k$ up there are any aritrary vectors with some arbitrary inner product.  *They are not necessarily n-dimensional vectors*.  \n",
    "\n",
    "consider the scalar field to be $\\mathbb C$.  We observe by conjugate symmetry that $\\mathbf G = \\mathbf G^*$ i.e. it *is* Hermitian.  \n",
    "\n",
    "now for $\\mathbf u \\in \\mathbb C^n$  we have  \n",
    "\n",
    "$\\mathbf c^* \\mathbf G \\mathbf c $  \n",
    "$= \\sum_{i=1}^n \\sum_{j=1}^n \\bar{c_{i}}c_{j}g_{i,j} $  \n",
    "$= \\sum_{i=1}^n \\bar{c_{i}} \\sum_{j=1}^n c_{j}\\langle \\mathbf x_i, \\mathbf x_j \\rangle $    \n",
    "$= \\sum_{i=1}^n \\bar{c_{i}} \\langle \\mathbf x_i,  \\big(\\sum_{j=1}^n c_{j} \\mathbf x_j \\big) \\rangle \\text{   via bi-linearity of inner product}$   \n",
    "$= \\langle\\big(\\sum_{i=1}^n c_{i} \\mathbf x_i\\big),  \\big(\\sum_{j=1}^n c_{j} \\mathbf x_j \\big) \\rangle \\text{   again via bi-linearity of inner product}$   \n",
    "$= \\langle\\big(\\sum_{j=1}^n c_{j} \\mathbf x_j\\big),  \\big(\\sum_{j=1}^n c_{j} \\mathbf x_j \\big)\\rangle $   \n",
    "$\\geq 0$  \n",
    "\n",
    "via positived-definitess axiom of innerproducts  \n",
    "\n",
    "\n",
    "Hence we know $\\mathbf G \\succeq 0$  i.e. that it is positive semi-definite (which tells us that it cannot have negative eigenvalues and Hermicity guarantees that they are real -- i.e. the two eigenvalues of $\\mathbf G$ must be real non-negative ).  \n",
    "\n",
    "Of course by applying Sylvester's Criterion, or a quadratic form argument, or just considering \n",
    "\n",
    "$\\mathbf G_{\\text{small}} := \\begin{bmatrix}\n",
    "\\langle \\mathbf x_1, \\mathbf x_1 \\rangle & \\langle \\mathbf x_1, \\mathbf x_2 \\rangle \\\\\n",
    "\\langle \\mathbf x_2, \\mathbf x_1 \\rangle & \\langle \\mathbf x_2, \\mathbf x_2 \\rangle \\\\\n",
    "\\end{bmatrix}$    \n",
    "\n",
    "we know that $\\mathbf G_{\\text{small}} = \\mathbf G_{\\text{small}}^*$  and $\\mathbf G_{\\text{small}} \\succeq 0$, thus  \n",
    "\n",
    "$\\det\\big(\\mathbf G_{\\text{small}}\\big) = \\langle \\mathbf x_1, \\mathbf x_1 \\rangle\\langle \\mathbf x_2, \\mathbf x_2 \\rangle -  \\langle \\mathbf x_1, \\mathbf x_2 \\rangle \\langle \\mathbf x_2, \\mathbf x_1 \\rangle = \\langle \\mathbf x_1, \\mathbf x_1 \\rangle\\langle \\mathbf x_2, \\mathbf x_2 \\rangle - \\Big \\vert \\langle \\mathbf x_1, \\mathbf x_2 \\rangle \\Big \\vert^2 = \\lambda_1 \\lambda_2 \\geq 0$  \n",
    "\n",
    "that is, we have a proof that   \n",
    "\n",
    "$ \\Big \\vert \\langle \\mathbf x_1, \\mathbf x_2 \\rangle \\Big \\vert^2 \\leq \\langle \\mathbf x_1, \\mathbf x_1 \\rangle\\langle \\mathbf x_2, \\mathbf x_2 \\rangle$  \n",
    "\n",
    "which is Cauchy-Schwarz  \n",
    "\n",
    "(to see a more belabored discussion of this technique, though not in as abstract a setting, see the discussion in \"Fun with Trace\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**7.2 A Centered Version of Schwarz's Inequality**  \n",
    "\n",
    "*commentary: The problem isn't stated this way, but it really is just asking for confirmation that squared covariance is less than or equal to variance times variance.  The official solution used a more direct inner product approach and then applied Schwarz's inequality.  The below approach is a touch more indirect but is a nice walk-through of probability.*  \n",
    "\n",
    "- - - - \n",
    "note: the correlation coefficient $-1 \\leq \\rho(X,Y) \\leq 1$, which in terms of magnitude can be stated as:  \n",
    "\n",
    "$ 0 \\leq \\big \\vert \\rho(X,Y) \\big \\vert \\leq 1$ \n",
    "\n",
    "This can be interpretted directly in terms of Cauchy Schwarz-\n",
    "\n",
    "$Var(X - Y) \\geq 0$ \n",
    "\n",
    "Now use suitable rescaling such that $Var(\\alpha X) = 1$ and $Var(\\beta Y) = 1$.  (Note: this rescaling factors are the 1 divided by standard deviations of each random variable.)  \n",
    "\n",
    "$Var(\\alpha X - \\beta Y)= Var(\\alpha X)+  Var(\\beta Y) - 2Cov(\\alpha X, \\beta Y)  \\geq 0$ \n",
    "\n",
    "$Var(\\alpha X)+ Var(\\beta Y) = 1 + 1 \\geq  2Cov(\\alpha X, \\beta Y) $\n",
    "\n",
    "$1 \\geq Cov(\\alpha X, \\beta Y) = \\alpha \\beta Cov(X,Y) = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} = \\rho(X,Y)$ \n",
    "\n",
    "if we run the same argument for $Var(\\alpha X + \\beta Y) \\geq 0$, we find $-1 \\leq \\rho(X,Y)$\n",
    "\n",
    "putting these two together, we find $-1 \\leq \\rho(X,Y) \\leq 1$  \n",
    "\n",
    "\n",
    "(b) Now repeat the above argument, this time for\n",
    "\n",
    "$Var(X + Y) \\geq 0$ \n",
    "\n",
    "Again use suitable rescaling such that $Var(\\alpha X) = 1$ and $Var(\\beta Y) = 1$.  \n",
    "\n",
    "$Var(\\alpha X + \\beta Y)= Var(\\alpha X)+  Var(\\beta Y) + 2Cov(\\alpha X, \\beta Y)  \\geq 0$ \n",
    "\n",
    "$Var(\\alpha X) + Var(\\beta Y) = 1 + 1 \\geq  -2Cov(\\alpha X, \\beta Y) $\n",
    "\n",
    "$-1 \\leq Cov(\\alpha X, \\beta Y) = \\alpha \\beta Cov(X,Y) = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} = \\rho(X,Y)$ \n",
    "\n",
    "Putting these two together, we find $-1 \\leq \\rho(X,Y) \\leq 1$  \n",
    "\n",
    "and if we square the above, we get:\n",
    "\n",
    "$\\rho(X,Y)^2 = \\big(\\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\big)^2 \\leq 1$\n",
    "\n",
    "giving us: \n",
    "\n",
    "$\\big(Cov(X,Y)\\big)^2 \\leq var(X)var(Y)$\n",
    "\n",
    "which completes 7.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4 Reciprocal on a Square** \n",
    "\n",
    "**claim:** \n",
    "\n",
    "for $a \\geq 0$ and $b \\geq 0$ one has the bound\n",
    "\n",
    "$\\frac{1}{a + b + 1}\\lt \\int_a^{a+1} \\int_b^{b+1}\\frac{dxdy}{x +y} =  \\int_a^{a+1} dy \\int_b^{b+1}\\frac{dx}{x +y} $ \n",
    "\n",
    "*comment: as in other problems, I'd prefer requiring at least one of a or b to be strictly positive to avoid singularities at the point where are both 0.  There is likely a measure based approach to work around / ignore this, though measure theory is far outside the scope of the text / pre-reqs.*  \n",
    "\n",
    "\n",
    "**proof:**  \n",
    "First notice that we are dealing with the unit square, i.e. $\\int_a^{a+1} dy = 1$, $ \\int_b^{b+1} dx = 1$, and \n",
    "\n",
    "$\\int_a^{a+1} \\int_b^{b+1} dx dy = 1$\n",
    "\n",
    "(of interest is that $x \\perp y$ which perhaps seems so obvious that we may forget about this, but it is a nice reminder about random variables, independence and geometry)\n",
    "\n",
    "\n",
    "There are basically two approach to solve this: The first, official solution uses the single variable form of jensen's inequality, applied twice.  The alternative approach would use the multivariable form of jensen's inequality, though we have *not* directly examined, derived or proven such a thing in this course.  (On the other hand, the single variable form was extensively covered in chapter 6, and a derivation of the integral form was given in chapter 7.)  \n",
    "\n",
    "- - - -\n",
    "*approach 1*  \n",
    "note that $f$ is strictly convex over its domain, where $f(z) = \\frac{1}{\\gamma + z}$ for any fixed $\\gamma \\gt 0$ and variable $z \\geq 0$ , we verify this by noting that $f''(z) = \\frac{2}{(\\gamma + z)^3} \\gt 0$  \n",
    "\n",
    "\n",
    "first piece:   \n",
    "we first evaluate the inner integral over x, for any $y \\geq 0$  \n",
    "$\\frac{1}{y + b + \\frac{1}{2}} = f(b + \\frac{1}{2}) = f(E[x]) \\leq E[f(x)] = \\int_b^{b+1} f(x)dx = \\int_b^{b+1}\\frac{dx}{x +y}  $  \n",
    "\n",
    "second piece:  \n",
    "noting that  \n",
    "$\\gamma = b + \\frac{1}{2} \\gt 0$  \n",
    "\n",
    "we chain on the result from the LHS above, and place it on the RHS, below, now evaluating the outer integral.  \n",
    "\n",
    "$\\frac{1}{a + b + 1} = \\frac{1}{a +\\frac{1}{2} + b +\\frac{1}{2}} = f[E(y)] \\lt E[f(y)] = \\int_a^{a+1} f(y)dy = \\int_a^{a+1} \\frac{1}{y + b + \\frac{1}{2}} dy $\n",
    "\n",
    "putting this all together, gives us \n",
    "\n",
    "$\\frac{1}{a + b + 1}\\lt \\int_a^{a+1} \\int_b^{b+1}\\frac{dxdy}{x +y} =  \\int_a^{a+1} dy \\int_b^{b+1}\\frac{dx}{x +y}$  \n",
    "\n",
    "- - - -\n",
    "*approach 2*\n",
    "\n",
    "let $x_1 := x$ and $x_2 = y$, and \n",
    "\n",
    "$\\mathbf x = \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "x_2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Using the multivariable form of Jensen's Inequality (which over an open convex domain, seems straightforward for a functions that are at least twice differentiable, using taylor polynomials, though your author does not know innards of the proof in this case), we may confirm that a scalar valued function $g\\big(\\mathbf x\\big)$ is strictly convex by the second derivative test, i.e. that its Hessian $\\mathbf H$ is positive definite over our domain, which we may write lazily as\n",
    "\n",
    "$g''\\big(\\mathbf x\\big) \\succ 0$\n",
    "\n",
    "or \n",
    "\n",
    "$\\nabla \\nabla^T g\\big(\\mathbf x\\big) \\succ 0$\n",
    "\n",
    "or \n",
    "\n",
    "$\\mathbf v: =\\big(\\mathbf x - \\mathbf a\\big)$  \n",
    "\n",
    "$\\mathbf v^T \\mathbf H \\mathbf v \\gt 0$  \n",
    "\n",
    "for any $\\mathbf v \\neq \\mathbf 0$\n",
    "\n",
    "for any $\\mathbf x$ in our domain, and any given $\\mathbf a \\neq \\mathbf x$.  \n",
    "\n",
    "In this case, we have \n",
    "\n",
    "$\\mathbf H = \\begin{bmatrix}\n",
    "\\frac{2}{(x_1 + x_2)^3} & \\frac{2}{(x_1 + x_2)^3} \\\\ \n",
    "\\frac{2}{(x_1 + x_2)^3} & \\frac{2}{(x_1 + x_2)^3}\n",
    "\\end{bmatrix} = \\frac{2}{(x_1 + x_2)^3} \\mathbf{11}^T$\n",
    "\n",
    "Noting that over our domain $\\frac{2}{(x_1 + x_2)^3} \\gt 0$, this tells us that $\\mathbf H$ is rank one and must be (at least) positive semi-definite.  As is often the case, evaluating the general case of a multivariable 2nd derivative can be challenging.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subtle point is that our function has its domain rectricted such that each component that all legal coordinates are non-negative.  If we are looking to 'move around' from said legal coordinated (via taylor polynomial truncated at $O(n^2)$ with Lagrange form of the remainder), the space that we 'move around to' still must be in our restricted domain, which is to say that our test should be refined to exclude any $\\mathbf v \\neq \\mathbf 0$, *and* require that each component of $\\mathbf v$ is non-negative, i.e. $v_k \\geq 0$ for $k =\\{1,...,n\\}$  (though in this case $n=2$).  \n",
    "\n",
    "Now, we may examine that eigenpairs of $\\mathbf H$ and see that we have \n",
    "\n",
    "$\\lambda_1 = \\frac{2}{(x_1 + x_2)^3} \\gt 0$ with eigenvector $\\mathbf q_1 \\propto \\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and  \n",
    "\n",
    "$\\lambda_2 = 0$ with eigenvector $\\mathbf q_2 \\propto \\begin{bmatrix}\n",
    "1\\\\ \n",
    "-1\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "\n",
    "our orthogonal eigenvectors work as a coordinate system hence any legal  \n",
    "\n",
    "$\\mathbf 0 \\neq \\mathbf v = \\gamma_1 \\mathbf q_1 + \\gamma_2 \\mathbf q_2$  \n",
    "\n",
    "where each component of $\\mathbf v$ is non-negative.  We could further refine this, but most simply, we recognize that in order for $\\mathbf v$ to be legal, there must be some $\\gamma_1 \\neq 0$.  Hence any for any legal $\\mathbf v$, we have \n",
    "\n",
    "\n",
    "$\\mathbf v^T \\mathbf H \\mathbf v = \\big(\\gamma_1 \\mathbf q_1 + \\gamma_2 \\mathbf q_2\\big)^T \\mathbf H \\big(\\gamma_1 \\mathbf q_1 + \\gamma_2 \\mathbf q_2\\big) = \\gamma_1^2 \\lambda_1 + \\gamma_2^2 \\lambda_2 = \\gamma_1^2 \\big(\\frac{2}{(x_1 + x_2)^3}\\big) + 0 \\gt 0$ \n",
    "\n",
    "because $\\frac{2}{(x_1 + x_2)^3} \\gt 0$ and $\\gamma_1^2 \\gt 0$ \n",
    "\n",
    "Hence, over our domain we do have a positive definite Hessian afterall, which confirms that our function is strictly convex in the multi-variable setup.  \n",
    "\n",
    "Thus by applying strict form of Jensen's Inequality to our problem, we have:  \n",
    "\n",
    "$E\\big[\\mathbf x\\big] = \\begin{bmatrix}\n",
    "a + \\frac{1}{2}\\\\ \n",
    "b + \\frac{1}{2}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\frac{1}{a + b + 1} = \\frac{1}{a + \\frac{1}{2} + b + \\frac{1}{2}} = f\\big(E\\big[\\mathbf x\\big]\\big) \\lt E\\big[f\\big(\\mathbf x\\big)\\big]= \\int_a^{a+1} \\int_b^{b+1} f\\big(\\mathbf x\\big) dx_1 dx_2$  \n",
    "\n",
    "which completes the proof of our problem.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7.7 Triangle Lower Bound ** \n",
    "\n",
    "\n",
    "suppose we have a function $f$ that has a real non-negative domain that maps to real non-negative numbers, and is convex on $[T, \\infty)$.  Show that for all $t \\geq T$ on has the following   \n",
    "\n",
    "**claim**:\n",
    "\n",
    "$\\frac{f(t)}{2}\\frac{f(t)}{\\big \\vert f'(t)\\big \\vert}=\\frac{1}{2} \\big(f(t)\\big)^2  \\big \\vert f'(t)\\big \\vert^{-1} =  \\big \\vert \\frac{1}{2} f(t)^2   f'(t)^{-1}\\big \\vert \\leq \\int_t^{\\infty} f(u) du$  \n",
    "\n",
    "\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$\\delta = \\frac{f(t)}{\\big \\vert f'(t)\\big \\vert }$  \n",
    "\n",
    "then we can restate this as   \n",
    "$\\frac{1}{2}  f(t) \\delta \\leq \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$  \n",
    "- - - - \n",
    "note: $ \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$ is justified by the fact that the function maps to a strictly real non-negative range, so extending the upper bound on integration cannot decrease the integral value.  \n",
    "- - - - \n",
    "\n",
    "A picture representation of  \n",
    "$\\frac{1}{2}  f(t) \\delta \\leq \\int_t^{t+\\delta} f(u) du$   \n",
    "\n",
    "is shown below, where the area in the triangle is represented on the Left Hand Side and the Right Hand Side has the area under the curve\n",
    "\n",
    "\n",
    "![picture of the triangle lower bound](chp7_triangle_lower_bound_.gif)\n",
    "\n",
    "\n",
    "Recall from ex 6.19, that we know (which is given in picture form as Fig 6.1 c on page 88)\n",
    "\n",
    "$f(b) \\geq f(a) + (b-a)\\theta$ for $b \\gt a$ in the domain where $f$ is convex,and $\\theta  \\in [f^{'}_\\_(x),f^{'}_+(x)]$  (i.e. where these are the limits of difference quotients from the left and right respectively, which must be finite for a convex function -- but because our function is at least once differentiable, we can just use $f^{'}(x)$ which is squeezed between those \n",
    "\n",
    "\n",
    "i.e. for our problem we have \n",
    "\n",
    "$f(b) \\geq f(a) + (b-a)f'(a)$ \n",
    "\n",
    "for $b \\gt a$,\n",
    "\n",
    "which is be said to be the lower linear bound associated with a convex function.  Geometrically this means that the tangent line at point $a$ is always below (or at most equal to) the function $f$ evaluated at any point in its domain. \n",
    "\n",
    "To close out this idea, note that the magnitude of the slope from \n",
    "\n",
    "$\\big(t, f(t)\\big) \\to \\big(t + \\delta, 0\\big)  = \\Big \\vert \\frac{\\Delta y}{\\Delta x} \\Big \\vert=  \\Big \\vert\\frac{f(t) - 0}{t - (t + \\delta)} \\Big \\vert = \\Big \\vert -f(t) \\delta^{-1}\\Big \\vert  = \\Big \\vert f(t) \\delta^{-1}\\Big \\vert  = \\Big \\vert f(t)\\big( \\frac{f(t)}{\\big \\vert f'(t))\\big \\vert}\\big)^{-1}\\Big \\vert = \\Big \\vert f'(t) \\Big \\vert$\n",
    "\n",
    "Thus the triangle in the picutre has an area given by $\\frac{1}{2} f(t)\\delta$ and for all points in $u \\in [t,t+\\delta]$ the value of $f(u) \\geq $ the value located on the triangle's hypotenuse at $u$.  \n",
    "\n",
    "Hence the area under $f(u)$ over $[t, t + \\delta]$ which is given by  $\\int_t^{t+\\delta} f(u) du$ is at least as big as the area under the triangle.  \n",
    "\n",
    "which gives us: \n",
    "\n",
    "$\\frac{1}{2} \\big(f(t)\\big)^2  \\big \\vert f'(t)\\big \\vert^{-1} = \\frac{1}{2}  f(t) \\delta \\leq \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$ \n",
    "\n",
    "which completes the exercise.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** overkill additional analysis:**  \n",
    "\n",
    "The absolute value in this was making your author a bit uncomfortable.  \n",
    "\n",
    "The inelegant but more exacting finish is to split $f'(t)$ into 3 cases. \n",
    "\n",
    "first if $f'(t)$ is zero (i.e. at the global minimum of the function), then the inequality's left hand side would seem to be indeterminant while the right hand side is infinite.  \n",
    "\n",
    "Put differently, we have an upper bound (RHS) of infinity in this case and the inequality isn't particularly meaningful.\n",
    "\n",
    "- - - -\n",
    "Now consider the case where $f'(t) \\lt 0$  (i.e. as in the picture)\n",
    "\n",
    "in this case define $\\delta := \\frac{f(t)}{-f'(t)}$.  \n",
    "\n",
    "Here the slope associated with the hypotenuse of the triangle is \n",
    "\n",
    "$\\big(t, f(t)\\big) \\to \\big(t + \\delta, 0\\big)  = \\frac{\\Delta y}{\\Delta x} =  \\frac{f(t) - 0}{t - (t + \\delta)} =  -f(t) \\delta^{-1} =  -f(t)\\big( \\frac{f(t)}{-f'(t)}\\big)^{-1} = f'(t)$\n",
    "\n",
    "and the above argument that the area under the triangle is less than or equal to that under the function applies directly (i.e. it is clean and easy to apply the lower linear bound.)\n",
    "\n",
    "Again we get our final result, \n",
    "\n",
    "$\\frac{1}{2}  f(t) \\delta \\leq \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$\n",
    "- - - -\n",
    "Finally, consider the case where consider the case where $f'(t) \\gt 0$ \n",
    "\n",
    "in this case define $\\delta := \\frac{f(t)}{f'(t)}$, which again makes $\\delta $ real non-negative (though to ignore minor nits we'll say this makes delta positive.) \n",
    "\n",
    "Here the slope associated with the hypotenuse of the triangle is \n",
    "\n",
    "$\\big(t, f(t)\\big) \\to \\big(t + \\delta, 0\\big)  = \\frac{\\Delta y}{\\Delta x} =  \\frac{f(t) - 0}{t - (t + \\delta)} =  -f(t) \\delta^{-1} =  -f(t)\\big( \\frac{f(t)}{f'(t)}\\big)^{-1} = -f'(t)$\n",
    "\n",
    "Pictorially, this still gives a triangle, and we have the area of the triangle.  However this triangle is a subset of the area of the trapezoid that exists between $[t, t+\\delta]$, if we instead used f'(t) (which is $\\gt 0$).  But the area of this trapezoid is upper bounded by the area under the curve of our function, again because of the linear lower bound relation associated with convex functions.  \n",
    "\n",
    "Thus in the case of $f'(t) \\gt 0$, we have \n",
    "\n",
    "\n",
    "$\\frac{1}{2}  f(t) \\delta \\leq \\frac{1}{2}  f(t)\\delta + f(t) \\delta = \\frac{3}{2}  f(t) \\delta  \\leq \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$\n",
    "\n",
    "which clarifies the proof of the claim for all three cases\n",
    "\n",
    "- - - - \n",
    "*commentary:*  \n",
    "\n",
    "Your author worked through the official soltion, which had the crucial hint that $\\delta = \\frac{f(t)}{f'(t)}$, but then erroneously described the triangle as being among the points $\\big \\{\\big(t, f(t)\\big),\\big(t, 0\\big), \\big(t, t + \\delta\\big) \\big \\}$.  Those three points all have the same x coordinate and hence do not describe a triangle, but instead a line segment.  Curriously, as of November 2017, this error is *not* contained in the errata sheet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8**  \n",
    "The slip-in trick as applied to the right side of the standard normal is contained in the interlude to exercise 2 of \"Ross_Pekoz_Chp4.ipynb\" which is contained in the probability folder.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.11 A continuous Carelman-Type Inequality** \n",
    "\n",
    "**claim: ** \n",
    "$\\exp\\Big(\\int_a^b \\log(f(x)) w(x) dx \\Big) \\leq e \\int_a^b f(x) w(x) dx   $\n",
    "\n",
    "where $\\int_a^b w(x) = 1$\n",
    "\n",
    "*commentary: the function f has domain [a,b] and range $[0, \\infty)$ though I'd suggest using $(0, \\infty)$ to avoid singularities and issues with taking the logarithm of zero.  The problem also asks us to solve for an overly loose inequality, something I first deciphered by mapping this to probability -- and applying jensen's inequality-- then confirmed via the 'comments sheet' (but not errata sheet) maintained by the book's author here: \n",
    "http://www-stat.wharton.upenn.edu/~steele/Publications/Books/CSMC/LangeListCSMCTypos.pdf . *\n",
    "\n",
    "\n",
    "**proof: **  \n",
    "$g(x) := \\log(f(x))$, valid over $x \\in [a,b]$, with weighting given by $w(x)$\n",
    "\n",
    "$\\exp\\Big(\\int_a^b \\log(f(x)) w(x) dx \\Big) = \\exp \\Big(E\\big[g(x) \\big]\\Big) \\leq  E\\Big[\\exp\\big(g(x)) \\Big] = E\\Big[\\exp\\big(\\log(f(x))\\big) \\Big] = E\\Big[f(x)\\Big] = \\int_a^b f(x) w(x) dx $ \n",
    "\n",
    "by application of Jensen's Inequality.  \n",
    "\n",
    "Note that the right hand side is positive and bounded above by some version of itself that is scaled by some $\\alpha \\gt 1$, and in this case $\\alpha:= e$.\n",
    "\n",
    "This completes the proof. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.12 Gruess's Inequality  ** \n",
    "\n",
    "This is a walkthrough of my answer for the final problem, albeit with much more of a probability flavor.  All random variables have a second moment.  \n",
    "\n",
    "Note: this is a considerably longer and different approach than what was taken in the official solution.  However, this approach has a lot of intuitive appeal given its direct ties with probability.  \n",
    "\n",
    "- - - -  \n",
    "*Lemma 1:*  \n",
    "\n",
    "for any $\\gamma \\geq 0$ and any $0 \\leq p\\leq 1$  \n",
    "\n",
    "$p(1-p)\\gamma $ is maximized with $p=0.5$ \n",
    "\n",
    "without loss of generality we ignore the positive scalar $\\gamma$ and maximize $p(1-p)$ which can only be maximized if its positive square root is maximized,\n",
    "\n",
    "\n",
    "$\\big(p(1-p)\\big)^{\\frac{1}{2}} \\leq \\frac{1}{2}\\big(p + (1-p)\\big) = 0.5$  \n",
    "which by $GM \\leq AM$ occurs **iff** $p = 1-p$, i.e. $p= 0.5$\n",
    "\n",
    "scale by our positive $\\gamma$ and our bound on the value is \n",
    "\n",
    "$\\gamma \\sqrt{p(1-p)} \\leq \\frac{1}{2}\\gamma$\n",
    "\n",
    "with the maxmium occuring **iff** $p= 0.5$\n",
    "- - - -  \n",
    "\n",
    "*Lemma 2:* \n",
    "\n",
    "For any bimodal distribution (i.e. one that takes on only 2 values), we can use the Bernouli distributions's variance, after suitable shifting and rescaling.  Recall that shifting does not change the variance, so we may assume without loss of generality that our bimodal distribution takes on values of $0$ with probability $1-p$ and some $d \\gt 0$ with probability $p$.  Hence for bimodal $X$ we can have\n",
    "\n",
    "$var\\big(\\frac{1}{d} X\\big) = \\frac{1}{d^2}var\\big(X\\big) = p\\big(1-p\\big)$  \n",
    "\n",
    "giving us \n",
    "\n",
    "$var \\big(X\\big) = \\sigma_X^2 = d^2 p\\big(1-p\\big)$  \n",
    "\n",
    "and of course the standard deviation is given by:  \n",
    "\n",
    "$\\sigma_X = d \\sqrt{p\\big(1-p\\big)}$ \n",
    "\n",
    "- - - -  \n",
    "\n",
    "*Lemma 3: *  \n",
    "\n",
    "(note: I seem to think there is a maximum entropy type of argument that can get us to this result a lot more quickly....)  \n",
    "\n",
    "\n",
    "For any random variable that takes on values in $[a, A]$ with $ a \\lt \\mu \\lt A$, variance can be maximized by allocating all probability to $a$ and $A$.  \n",
    "\n",
    "To do this argument right will require a little bit of fine tuning, and really should use a CDF, $F_X(X)$.  \n",
    "\n",
    "The idea is that we have a greedy exchange here.  \n",
    "\n",
    "First center the random variable / assume without loss of generality that it has has zero mean.  Carefully using the CDF, assume we find some value $v$ where $a \\lt v \\lt A$ and we have $v$ with some probability $q$ where $0 \\lt q \\leq 1$.  (We can fine tune the argument somewhat to accomodate continuous cases by having $v$ be the the upper bound on the magnitude of a collection of values which collectively have positive probability and said collection has values strictly greater than $a$ and strictly less than $A$.) \n",
    "\n",
    "The argument is we can keep the mean intact but increase our variance via allocating instead to the convex combination:  \n",
    "\n",
    "$rA + (s)a = qv$, where $0\\leq r, s\\leq 1$ and $r + s = q$  \n",
    "\n",
    "or in proper convex combination form: $\\frac{r}{q}A + \\frac{s}{q}a = v$  \n",
    "- - - - \n",
    "for avoidance of doubt, note that  \n",
    "$0A + 1a \\lt v \\lt 1A +0a$  \n",
    "hence by continuity over reals, there must be some $0\\lt p \\lt 1$ where\n",
    "\n",
    "$pA + (1-p)a = v$ \n",
    "- - - - \n",
    "\n",
    "via the strict convexity associated with a squaring operation, we know:  \n",
    "$\\frac{r}{q}A^2 + \\frac{s}{q}a^2 \\gt \\big(\\frac{r}{q}A + \\frac{s}{q}a\\big)^2 = v^2$.  Hence we may multiply each side by $q$ and get  \n",
    "\n",
    "$rA^2 + s a^2 \\gt  qv^2$\n",
    "\n",
    "This generates the following exchange argument:  \n",
    "\n",
    "With random variable $X$ taking on values in $[a,A]$, for any given mean $\\mu$ where $a \\lt \\mu \\lt A$,  suppose we have maximized variance and not all probability is concentrated on $a$ and $A$.  Then there must be at least some $v$ and some associated $0 \\lt q \\lt 1$.  \n",
    "\n",
    "But we can find an even higher variance if we allocate the from $q$ to $r, s$ such that $(r)A + (s)a = qv$.  This is a contradicts the assumption we found a configuration with maximum variance.  \n",
    "\n",
    "Hence any configuration with maximum variance, for any given mean must have all probability on $A$ with probability $p$ and on $a$ with probability $1-p$. \n",
    "\n",
    "(For completeness, we may further state the tologically our bounded interval is a compact set, hence we can be confident that the maximum does in fact exist.)  \n",
    "\n",
    "\n",
    "- - - -  \n",
    "\n",
    "**claim:** \n",
    "\n",
    "where $a \\leq X \\leq A$ and $b \\leq Y \\leq B$\n",
    "\n",
    "$\\big \\vert Cov(X,Y)\\big \\vert \\leq \\frac{1}{4}\\big(A-a\\big)\\big(B-b\\big)$  \n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$\\big \\vert \\rho \\big \\vert = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} \\leq 1$   \n",
    "(see **problem 7.2**)   \n",
    "\n",
    "\n",
    "We now introduce $X^*$, which is a random variable with the same mean as $X$ but all probability is allocated to $\\{a, A\\}$ and also $Y^*$ which is a random variable with the same mean as $Y$ but all probability is allocated to $\\{b, B\\}$\n",
    "\n",
    "Multiply each side by $\\sigma_{X} \\sigma_{Y}$  \n",
    "\n",
    "$\\big \\vert Cov(X,Y)\\big \\vert \\leq \\sigma_{X} \\sigma_{Y} \\leq  \\sigma_{X^*} \\sigma_{Y^*}$  \n",
    "\n",
    "by lemma 3. \n",
    "\n",
    "Now using lemma 2, we know \n",
    "\n",
    "$\\sigma_{X^*} = \\sqrt{(A-a)^2 p(1-p)} =  \\big(\\sqrt{p(1-p)}\\big)(A-a)  \\leq  \\frac{1}{2}(A-a)$ \n",
    "\n",
    "by lemma 1.  \n",
    "\n",
    "Equivalently for $Y^*$ we have:  \n",
    "\n",
    "$\\sigma_{Y^*} = \\sqrt{(B-b)^2 p(1-p)} =  \\big(\\sqrt{p(1-p)}\\big)(B-b)  \\leq  \\frac{1}{2}(B-b)$ \n",
    "\n",
    "putting this together, we have:  \n",
    "\n",
    "$\\big \\vert Cov(X,Y)\\big \\vert \\leq \\sigma_{X} \\sigma_{Y} \\leq \\sigma_{X^*} \\sigma_{Y^*} \\leq \\big(\\frac{1}{2}(A-a)\\big)\\big(\\frac{1}{2}(B-b)\\big) = \\frac{1}{4}(A-a)(B-b)$  \n",
    "\n",
    "which completes the proof.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subsequent note:**  \n",
    "\n",
    "With respect to the variance in particular for bounded $X$, \n",
    "Supposing we hold $\\bar X$ constant, then there is a majorization (or stochastic dominance) interpretation of Gruess's Inequality.  \n",
    "\n",
    "To avoid special cases, we assume that this is not a deterministic random variable  i.e. it has non-zero variance.  \n",
    "\n",
    "That is for any centered random variable $Y = X - \\bar{X}$  that is bounded, say in $[a, A]$ (where since centered, it is understood that $a \\lt 0 \\lt A$, if we look at the complementary cdf of $Z = Y^2$, we have \n",
    "\n",
    "$Pr\\{Z^* \\geq z \\} \\geq Pr\\{Z \\geq z\\}$  for any $z \\in [0, \\max(\\vert a\\vert,A)]$  \n",
    "\n",
    "where $Z^*$ is  the optimal random variable that is 1 for every point in $[0, \\max(a,A)]$.  Integrating over this bound gives \n",
    "\n",
    "$\\text{var}(Y^*) = E\\big[Z^*\\big] = \\int_0^{\\max(\\vert a\\vert,A)}  Pr\\{Z^* \\geq z \\}dz \\geq \\int_0^{\\max(\\vert a\\vert,A)}  Pr\\{Z \\geq z\\}dz = E\\big[Z\\big] = \\text{var}(Y)$   \n",
    "\n",
    "since our variable has non-zero variance and it is centered at its mean, this implies that $Y$ takes on positive and negative values -- but the above implies -- at least for a clear maximum case to exist -- they *all* have to be at  $\\max(a,A)$, which implies $\\big\\vert a \\big\\vert = A$.\n",
    "\n",
    "But this implies $X$ takes on values $a + \\mu$ with probability $q$ and $A + \\mu$ with probability $p$, such that p+q = 1, which gives us \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "a+ \\mu & A + \\mu\\\\ \n",
    "1 & 1\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "q\\\\ \n",
    "p\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mu\\\\ \n",
    "1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "notice that since $a \\lt A$ the above matrix has determinant $= (a +\\mu) -( A + \\mu)  = a - A \\lt 0$  and hence is non-singular. Solving this gives us \n",
    "\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "q\\\\ \n",
    "p\n",
    "\\end{bmatrix} = \\left[\\begin{matrix}\\frac{A}{A - a}\\\\\\frac{-a}{A - a}\\end{matrix}\\right]= \\left[\\begin{matrix}\\frac{A}{A + \\vert a\\vert}\\\\\\frac{\\vert a\\vert}{A + \\vert a \\vert}\\end{matrix}\\right]$   \n",
    "\n",
    "(The reader should note that this in essence is the zero mean case of Wald's Equality on absorbtion probabilities of a random walk with two sided boundaries (and no overshoots).)  \n",
    "\n",
    "The sole solution is $p= \\frac{\\vert a\\vert}{A + \\vert a \\vert}$, but we stated that in the maximizing case $ \\vert a\\vert = A$, which implies that in the opitmal case, we must have $p= \\frac{A}{2A }=\\frac{1}{2}$.  \n",
    "\n",
    "There is a bit more thinking and machinery involved here, but the fact that Gruess's Inequality can be interpretted as coming from stochastic dominance / majorization... is a very nice result.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subsequent note:**   \n",
    "There is a rather stunningly simple way of getting to this result, specialized to variance in this link:  \n",
    "\n",
    "https://stats.stackexchange.com/questions/45588/variance-of-a-bounded-random-variable\n",
    "- - - - -  \n",
    "**edit:** This is also problem 53 on page 239 (Chp. 5) of Blitzstein and Hwang \n",
    "\n",
    "- - - - -  \n",
    "since the below gives an upper bound on the variance of a bounded random variable $X$ we can rerun that to bound variance on the random variable $Y$, take square roots of both and multiply the bounds, and use the correlation coefficient / exercise from the beginning of this page  \n",
    "\n",
    "$\\big \\vert Cov(X,Y)\\big \\vert \\leq \\sigma_{X} \\sigma_{Y} \\leq \\sigma_{X^*} \\sigma_{Y^*} \\leq \\text{bounds derived below}$  \n",
    "- - - - \n",
    "\n",
    "First constrain $0\\leq x \\leq 1$  which gives us access to $\\text{GM} \\leq \\text{AM}$ *and* we have $x^2 \\leq x$ for all $X=x$.  That is, we have a point-wise bound.  In conditional expectations form this reads, that we have the following non-negative random variable:    \n",
    "$E\\Big[(X - X^2)\\big \\vert X\\Big] \\geq 0 $    \n",
    "taking expectations gives  \n",
    "$E\\Big[E\\big[(X - X^2)\\big \\vert X\\big]\\Big] \\geq 0$   \n",
    "$E\\Big[(X - X^2)\\Big] \\geq 0  $  \n",
    "$E\\Big[X\\Big] - E\\Big[X^2\\Big] \\geq 0  $  \n",
    "$1 \\geq \\mu = E\\Big[X\\Big] \\geq E\\Big[X^2\\Big] \\geq 0 $  \n",
    "\n",
    "which gives us:  \n",
    "\n",
    "$\\begin{equation}\n",
    "\\text{var}(X)= \\sigma^2 =  E[X^2] - (E[X]^2)   = E[X^2] - \\mu^2 \\leq \\mu - \\mu^2 = \\mu(1-\\mu)\\leq \\frac{1}{4}\n",
    "\\end{equation}$  \n",
    "\n",
    "where we know $\\mu(1-\\mu)\\leq \\frac{1}{4}$ for $\\mu \\in [0,1]$ by $\\text{GM} \\leq \\text{AM}$, with equality iff $\\mu = \\frac{1}{2}$  \n",
    "\n",
    "taking square roots and viewing in terms of standard deviation   \n",
    "$\\text{std dev}(X) = \\big(\\text{var}(X)\\big)^\\frac{1}{2} = \\sigma \\leq \\sqrt{\\mu(1-\\mu)}\\leq \\frac{1}{2}$   \n",
    "\n",
    "Then if the variable is infact bounded in $[a,A]$, we observe homogeneity of the standard deviation with respect to scaling (by positive number $(A-a)$) and look at $X^* = (A-a)X$  \n",
    "\n",
    "from there we can always re-center a variable without changing its standard deviation or variance -- hence if we take $Z = X^*+ a$ it has the same variance as $X^*$ and takes on values in $[a,A]$, as desired)  \n",
    "\n",
    "$\\text{std dev}\\big(Z\\big) = \\text{std dev}\\big(X^*\\big)= \\text{std dev}\\big((A-a)X\\big) = (A-a)\\cdot\\text{std dev}\\big(X\\big)\\leq (A-a)\\sqrt{\\mu(1-\\mu)}\\leq (A-a)\\frac{1}{2}$  \n",
    "and squaring gives  \n",
    "\n",
    "$\\text{var}(Z) = \\text{var}(Y) = \\text{std dev}\\big(Y\\big)^2 \\leq (A-a)^2\\mu(1-\\mu)\\leq (A-a)^2\\frac{1}{4}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
