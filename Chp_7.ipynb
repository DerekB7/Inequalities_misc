{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**7.2 A Centered Version of Schwarz's Inequality**  \n",
    "\n",
    "*commentary: The problem isn't stated this way, but it really is just asking for confirmation that squared covariance is less than or equal to variance times variance.  The official solution used a more direct inner product approach and then applied Schwarz's inequality.  The below approach is a touch more indirect but is a nice walk-through of probability.*  \n",
    "\n",
    "- - - - \n",
    "note: the correlation coefficient $-1 \\leq \\rho(X,Y) \\leq 1$, which in terms of magnitude can be stated as:  \n",
    "\n",
    "$ 0 \\leq \\big \\vert \\rho(X,Y) \\big \\vert \\leq 1$ \n",
    "\n",
    "This can be interpretted directly in terms of Cauchy Schwarz-\n",
    "\n",
    "$Var(X - Y) \\geq 0$ \n",
    "\n",
    "Now use suitable rescaling such that $Var(\\alpha X) = 1$ and $Var(\\beta Y) = 1$.  (Note: this rescaling factors are the 1 divided by standard deviations of each random variable.)  \n",
    "\n",
    "$Var(\\alpha X - \\beta Y)= Var(\\alpha X)+  Var(\\beta Y) - 2Cov(\\alpha X, \\beta Y)  \\geq 0$ \n",
    "\n",
    "$Var(\\alpha X)+ Var(\\beta Y) = 1 + 1 \\geq  2Cov(\\alpha X, \\beta Y) $\n",
    "\n",
    "$1 \\geq Cov(\\alpha X, \\beta Y) = \\alpha \\beta Cov(X,Y) = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} = \\rho(X,Y)$ \n",
    "\n",
    "if we run the same argument for $Var(\\alpha X + \\beta Y) \\geq 0$, we find $-1 \\leq \\rho(X,Y)$\n",
    "\n",
    "putting these two together, we find $-1 \\leq \\rho(X,Y) \\leq 1$  \n",
    "\n",
    "\n",
    "(b) Now repeat the above argument, this time for\n",
    "\n",
    "$Var(X + Y) \\geq 0$ \n",
    "\n",
    "Again use suitable rescaling such that $Var(\\alpha X) = 1$ and $Var(\\beta Y) = 1$.  \n",
    "\n",
    "$Var(\\alpha X + \\beta Y)= Var(\\alpha X)+  Var(\\beta Y) + 2Cov(\\alpha X, \\beta Y)  \\geq 0$ \n",
    "\n",
    "$Var(\\alpha X) + Var(\\beta Y) = 1 + 1 \\geq  -2Cov(\\alpha X, \\beta Y) $\n",
    "\n",
    "$-1 \\leq Cov(\\alpha X, \\beta Y) = \\alpha \\beta Cov(X,Y) = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} = \\rho(X,Y)$ \n",
    "\n",
    "Putting these two together, we find $-1 \\leq \\rho(X,Y) \\leq 1$  \n",
    "\n",
    "and if we square the above, we get:\n",
    "\n",
    "$\\rho(X,Y)^2 = \\big(\\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\big)^2 \\leq 1$\n",
    "\n",
    "giving us: \n",
    "\n",
    "$\\big(Cov(X,Y)\\big)^2 \\leq var(X)var(Y)$\n",
    "\n",
    "which completes 7.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4 Reciprocal on a Square** \n",
    "\n",
    "**claim:** \n",
    "\n",
    "for $a \\geq 0$ and $b \\geq 0$ one has the bound\n",
    "\n",
    "$\\frac{1}{a + b + 1}\\lt \\int_a^{a+1} \\int_b^{b+1}\\frac{dxdy}{x +y} =  \\int_a^{a+1} dy \\int_b^{b+1}\\frac{dx}{x +y} $ \n",
    "\n",
    "*comment: as in other problems, I'd prefer requiring at least one of a or b to be strictly positive to avoid singularities at the point where are both 0.  There is likely a measure based approach to work around / ignore this, though measure theory is far outside the scope of the text / pre-reqs.*  \n",
    "\n",
    "\n",
    "**proof:**  \n",
    "First notice that we are dealing with the unit square, i.e. $\\int_a^{a+1} dy = 1$, $ \\int_b^{b+1} dx = 1$, and \n",
    "\n",
    "$\\int_a^{a+1} \\int_b^{b+1} dx dy = 1$\n",
    "\n",
    "(of interest is that $x \\perp y$ which perhaps seems so obvious that we may forget about this, but it is a nice reminder about random variables, independence and geometry)\n",
    "\n",
    "\n",
    "There are basically two approach to solve this: The first, official solution uses the single variable form of jensen's inequality, applied twice.  The alternative approach would use the multivariable form of jensen's inequality, though we have *not* directly examined, derived or proven such a thing in this course.  (On the other hand, the single variable form was extensively covered in chapter 6, and a derivation of the integral form was given in chapter 7.)  \n",
    "\n",
    "- - - -\n",
    "*approach 1*  \n",
    "note that $f$ is strictly convex over its domain, where $f(z) = \\frac{1}{\\gamma + z}$ for any fixed $\\gamma \\gt 0$ and variable $z \\geq 0$ , we verify this by noting that $f''(z) = \\frac{2}{(\\gamma + z)^3} \\gt 0$  \n",
    "\n",
    "\n",
    "first piece:   \n",
    "we first evaluate the inner integral over x, for any $y \\geq 0$  \n",
    "$\\frac{1}{y + b + \\frac{1}{2}} = f(b + \\frac{1}{2}) = f(E[x]) \\leq E[f(x)] = \\int_b^{b+1} f(x)dx = \\int_b^{b+1}\\frac{dx}{x +y}  $  \n",
    "\n",
    "second piece:  \n",
    "noting that  \n",
    "$\\gamma = b + \\frac{1}{2} \\gt 0$  \n",
    "\n",
    "we chain on the result from the LHS above, and place it on the RHS, below, now evaluating the outer integral.  \n",
    "\n",
    "$\\frac{1}{a + b + 1} = \\frac{1}{a +\\frac{1}{2} + b +\\frac{1}{2}} = f[E(y)] \\lt E[f(y)] = \\int_a^{a+1} f(y)dy = \\int_a^{a+1} \\frac{1}{y + b + \\frac{1}{2}} dy $\n",
    "\n",
    "putting this all together, gives us \n",
    "\n",
    "$\\frac{1}{a + b + 1}\\lt \\int_a^{a+1} \\int_b^{b+1}\\frac{dxdy}{x +y} =  \\int_a^{a+1} dy \\int_b^{b+1}\\frac{dx}{x +y}$  \n",
    "\n",
    "- - - -\n",
    "*approach 2*\n",
    "\n",
    "let $x_1 := x$ and $x_2 = y$, and \n",
    "\n",
    "$\\mathbf x = \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "x_2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Using the multivariable form of Jensen's Inequality (which over an open convex domain, seems straightforward for a functions that are at least twice differentiable, using taylor polynomials, though your author does not know innards of the proof in this case), we may confirm that a scalar valued function $g\\big(\\mathbf x\\big)$ is strictly convex by the second derivative test, i.e. that its Hessian $\\mathbf H$ is positive definite over our domain, which we may write lazily as\n",
    "\n",
    "$g''\\big(\\mathbf x\\big) \\succ 0$\n",
    "\n",
    "or \n",
    "\n",
    "$\\nabla \\nabla^T g\\big(\\mathbf x\\big) \\succ 0$\n",
    "\n",
    "or \n",
    "\n",
    "$\\mathbf v: =\\big(\\mathbf x - \\mathbf a\\big)$  \n",
    "\n",
    "$\\mathbf v^T \\mathbf H \\mathbf v \\gt 0$  \n",
    "\n",
    "for any $\\mathbf v \\neq \\mathbf 0$\n",
    "\n",
    "for any $\\mathbf x$ in our domain, and any given $\\mathbf a \\neq \\mathbf x$.  \n",
    "\n",
    "In this case, we have \n",
    "\n",
    "$\\mathbf H = \\begin{bmatrix}\n",
    "\\frac{2}{(x_1 + x_2)^3} & \\frac{2}{(x_1 + x_2)^3} \\\\ \n",
    "\\frac{2}{(x_1 + x_2)^3} & \\frac{2}{(x_1 + x_2)^3}\n",
    "\\end{bmatrix} = \\frac{2}{(x_1 + x_2)^3} \\mathbf{11}^T$\n",
    "\n",
    "Noting that over our domain $\\frac{2}{(x_1 + x_2)^3} \\gt 0$, this tells us that $\\mathbf H$ is rank one and must be (at least) positive semi-definite.  As is often the case, evaluating the general case of a multivariable 2nd derivative can be challenging.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subtle point is that our function has its domain rectricted such that each component that all legal coordinates are non-negative.  If we are looking to 'move around' from said legal coordinated (via taylor polynomial truncated at $O(n^2)$ with Lagrange form of the remainder), the space that we 'move around to' still must be in our restricted domain, which is to say that our test should be refined to exclude any $\\mathbf v \\neq \\mathbf 0$, *and* require that each component of $\\mathbf v$ is non-negative, i.e. $v_k \\geq 0$ for $k =\\{1,...,n\\}$  (though in this case $n=2$).  \n",
    "\n",
    "Now, we may examine that eigenpairs of $\\mathbf H$ and see that we have \n",
    "\n",
    "$\\lambda_1 = \\frac{2}{(x_1 + x_2)^3} \\gt 0$ with eigenvector $\\mathbf q_1 \\propto \\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and  \n",
    "\n",
    "$\\lambda_2 = 0$ with eigenvector $\\mathbf q_2 \\propto \\begin{bmatrix}\n",
    "1\\\\ \n",
    "-1\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "\n",
    "our orthogonal eigenvectors work as a coordinate system hence any legal  \n",
    "\n",
    "$\\mathbf 0 \\neq \\mathbf v = \\gamma_1 \\mathbf q_1 + \\gamma_2 \\mathbf q_2$  \n",
    "\n",
    "where each component of $\\mathbf v$ is non-negative.  We could further refine this, but most simply, we recognize that in order for $\\mathbf v$ to be legal, there must be some $\\gamma_1 \\neq 0$.  Hence any for any legal $\\mathbf v$, we have \n",
    "\n",
    "\n",
    "$\\mathbf v^T \\mathbf H \\mathbf v = \\big(\\gamma_1 \\mathbf q_1 + \\gamma_2 \\mathbf q_2\\big)^T \\mathbf H \\big(\\gamma_1 \\mathbf q_1 + \\gamma_2 \\mathbf q_2\\big) = \\gamma_1^2 \\lambda_1 + \\gamma_2^2 \\lambda_2 = \\gamma_1^2 \\big(\\frac{2}{(x_1 + x_2)^3}\\big) + 0 \\gt 0$ \n",
    "\n",
    "because $\\frac{2}{(x_1 + x_2)^3} \\gt 0$ and $\\gamma_1^2 \\gt 0$ \n",
    "\n",
    "Hence, over our domain we do have a positive definite Hessian afterall, which confirms that our function is strictly convex in the multi-variable setup.  \n",
    "\n",
    "Thus by applying strict form of Jensen's Inequality to our problem, we have:  \n",
    "\n",
    "$E\\big[\\mathbf x\\big] = \\begin{bmatrix}\n",
    "a + \\frac{1}{2}\\\\ \n",
    "b + \\frac{1}{2}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\frac{1}{a + b + 1} = \\frac{1}{a + \\frac{1}{2} + b + \\frac{1}{2}} = f\\big(E\\big[\\mathbf x\\big]\\big) \\lt E\\big[f\\big(\\mathbf x\\big)\\big]= \\int_a^{a+1} \\int_b^{b+1} f\\big(\\mathbf x\\big) dx_1 dx_2$  \n",
    "\n",
    "which completes the proof of our problem.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7.7 Triangle Lower Bound ** \n",
    "\n",
    "\n",
    "suppose we have a function $f$ that has a real non-negative domain that maps to real non-negative numbers, and is convex on $[T, \\infty)$.  Show that for all $t \\geq T$ on has the following   \n",
    "\n",
    "**claim**:\n",
    "\n",
    "$\\frac{f(t)}{2}\\frac{f(t)}{\\big \\vert f'(t)\\big \\vert}=\\frac{1}{2} \\big(f(t)\\big)^2  \\big \\vert f'(t)\\big \\vert^{-1} =  \\big \\vert \\frac{1}{2} f(t)^2   f'(t)^{-1}\\big \\vert \\leq \\int_t^{\\infty} f(u) du$  \n",
    "\n",
    "\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$\\delta = \\frac{f(t)}{\\big \\vert f'(t)\\big \\vert }$  \n",
    "\n",
    "then we can restate this as   \n",
    "$\\frac{1}{2}  f(t) \\delta \\leq \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$  \n",
    "- - - - \n",
    "note: $ \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$ is justified by the fact that the function maps to a strictly real non-negative range, so extending the upper bound on integration cannot decrease the integral value.  \n",
    "- - - - \n",
    "\n",
    "A picture representation of  \n",
    "$\\frac{1}{2}  f(t) \\delta \\leq \\int_t^{t+\\delta} f(u) du$   \n",
    "\n",
    "is shown below, where the area in the triangle is represented on the Left Hand Side and the Right Hand Side has the area under the curve\n",
    "\n",
    "\n",
    "![picture of the triangle lower bound](chp7_triangle_lower_bound_.gif)\n",
    "\n",
    "\n",
    "Recall from ex 6.19, that we know (which is given in picture form as Fig 6.1 c on page 88)\n",
    "\n",
    "$f(b) \\geq f(a) + (b-a)\\theta$ for $b \\gt a$ in the domain where $f$ is convex,and $\\theta  \\in [f^{'}_\\_(x),f^{'}_+(x)]$  (i.e. where these are the limits of difference quotients from the left and right respectively, which must be finite for a convex function -- but because our function is at least once differentiable, we can just use $f^{'}(x)$ which is squeezed between those \n",
    "\n",
    "\n",
    "i.e. for our problem we have \n",
    "\n",
    "$f(b) \\geq f(a) + (b-a)f'(a)$ \n",
    "\n",
    "for $b \\gt a$,\n",
    "\n",
    "which is be said to be the lower linear bound associated with a convex function.  Geometrically this means that the tangent line at point $a$ is always below (or at most equal to) the function $f$ evaluated at any point in its domain. \n",
    "\n",
    "To close out this idea, note that the magnitude of the slope from \n",
    "\n",
    "$\\big(t, f(t)\\big) \\to \\big(t + \\delta, 0\\big)  = \\Big \\vert \\frac{\\Delta y}{\\Delta x} \\Big \\vert=  \\Big \\vert\\frac{f(t) - 0}{t - (t + \\delta)} \\Big \\vert = \\Big \\vert -f(t) \\delta^{-1}\\Big \\vert  = \\Big \\vert f(t) \\delta^{-1}\\Big \\vert  = \\Big \\vert f(t)\\big( \\frac{f(t)}{\\big \\vert f'(t))\\big \\vert}\\big)^{-1}\\Big \\vert = \\Big \\vert f'(t) \\Big \\vert$\n",
    "\n",
    "Thus the triangle in the picutre has an area given by $\\frac{1}{2} f(t)\\delta$ and for all points in $u \\in [t,t+\\delta]$ the value of $f(u) \\geq $ the value located on the triangle's hypotenuse at $u$.  \n",
    "\n",
    "Hence the area under $f(u)$ over $[t, t + \\delta]$ which is given by  $\\int_t^{t+\\delta} f(u) du$ is at least as big as the area under the triangle.  \n",
    "\n",
    "which gives us: \n",
    "\n",
    "$\\frac{1}{2} \\big(f(t)\\big)^2  \\big \\vert f'(t)\\big \\vert^{-1} = \\frac{1}{2}  f(t) \\delta \\leq \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$ \n",
    "\n",
    "which completes the exercise.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** overkill additional analysis:**  \n",
    "\n",
    "The absolute value in this was making your author a bit uncomfortable.  \n",
    "\n",
    "The inelegant but more exacting finish is to split $f'(t)$ into 3 cases. \n",
    "\n",
    "first if $f'(t)$ is zero (i.e. at the global minimum of the function), then the inequality's left hand side would seem to be indeterminant while the right hand side is infinite.  \n",
    "\n",
    "Put differently, we have an upper bound (RHS) of infinity in this case and the inequality isn't particularly meaningful.\n",
    "\n",
    "- - - -\n",
    "Now consider the case where $f'(t) \\lt 0$  (i.e. as in the picture)\n",
    "\n",
    "in this case define $\\delta := \\frac{f(t)}{-f'(t)}$.  \n",
    "\n",
    "Here the slope associated with the hypotenuse of the triangle is \n",
    "\n",
    "$\\big(t, f(t)\\big) \\to \\big(t + \\delta, 0\\big)  = \\frac{\\Delta y}{\\Delta x} =  \\frac{f(t) - 0}{t - (t + \\delta)} =  -f(t) \\delta^{-1} =  -f(t)\\big( \\frac{f(t)}{-f'(t)}\\big)^{-1} = f'(t)$\n",
    "\n",
    "and the above argument that the area under the triangle is less than or equal to that under the function applies directly (i.e. it is clean and easy to apply the lower linear bound.)\n",
    "\n",
    "Again we get our final result, \n",
    "\n",
    "$\\frac{1}{2}  f(t) \\delta \\leq \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$\n",
    "- - - -\n",
    "Finally, consider the case where consider the case where $f'(t) \\gt 0$ \n",
    "\n",
    "in this case define $\\delta := \\frac{f(t)}{f'(t)}$, which again makes $\\delta $ real non-negative (though to ignore minor nits we'll say this makes delta positive.) \n",
    "\n",
    "Here the slope associated with the hypotenuse of the triangle is \n",
    "\n",
    "$\\big(t, f(t)\\big) \\to \\big(t + \\delta, 0\\big)  = \\frac{\\Delta y}{\\Delta x} =  \\frac{f(t) - 0}{t - (t + \\delta)} =  -f(t) \\delta^{-1} =  -f(t)\\big( \\frac{f(t)}{f'(t)}\\big)^{-1} = -f'(t)$\n",
    "\n",
    "Pictorially, this still gives a triangle, and we have the area of the triangle.  However this triangle is a subset of the area of the trapezoid that exists between $[t, t+\\delta]$, if we instead used f'(t) (which is $\\gt 0$).  But the area of this trapezoid is upper bounded by the area under the curve of our function, again because of the linear lower bound relation associated with convex functions.  \n",
    "\n",
    "Thus in the case of $f'(t) \\gt 0$, we have \n",
    "\n",
    "\n",
    "$\\frac{1}{2}  f(t) \\delta \\leq \\frac{1}{2}  f(t)\\delta + f(t) \\delta = \\frac{3}{2}  f(t) \\delta  \\leq \\int_t^{t+\\delta} f(u) du \\leq \\int_t^{\\infty} f(u) du$\n",
    "\n",
    "which clarifies the proof of the claim for all three cases\n",
    "\n",
    "- - - - \n",
    "*commentary:*  \n",
    "\n",
    "Your author worked through the official soltion, which had the crucial hint that $\\delta = \\frac{f(t)}{f'(t)}$, but then erroneously described the triangle as being among the points $\\big \\{\\big(t, f(t)\\big),\\big(t, 0\\big), \\big(t, t + \\delta\\big) \\big \\}$.  Those three points all have the same x coordinate and hence do not describe a triangle, but instead a line segment.  Curriously, as of November 2017, this error is *not* contained in the errata sheet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.11 A continuous Carelman-Type Inequality** \n",
    "\n",
    "**claim: ** \n",
    "$\\exp\\Big(\\int_a^b \\log(f(x)) w(x) dx \\Big) \\leq e \\int_a^b f(x) w(x) dx   $\n",
    "\n",
    "where $\\int_a^b w(x) = 1$\n",
    "\n",
    "*commentary: the function f has domain [a,b] and range $[0, \\infty)$ though I'd suggest using $(0, \\infty)$ to avoid singularities and issues with taking the logarithm of zero.  The problem also asks us to solve for an overly loose inequality, something I first deciphered by mapping this to probability -- and applying jensen's inequality-- then confirmed via the 'comments sheet' (but not errata sheet) maintained by the book's author here: \n",
    "http://www-stat.wharton.upenn.edu/~steele/Publications/Books/CSMC/LangeListCSMCTypos.pdf . *\n",
    "\n",
    "\n",
    "**proof: **  \n",
    "$g(x) := \\log(f(x))$, valid over $x \\in [a,b]$, with weighting given by $w(x)$\n",
    "\n",
    "$\\exp\\Big(\\int_a^b \\log(f(x)) w(x) dx \\Big) = \\exp \\Big(E\\big[g(x) \\big]\\Big) \\leq  E\\Big[\\exp\\big(g(x)) \\Big] = E\\Big[\\exp\\big(\\log(f(x))\\big) \\Big] = E\\Big[f(x)\\Big] = \\int_a^b f(x) w(x) dx $ \n",
    "\n",
    "by application of Jensen's Inequality.  \n",
    "\n",
    "Note that the right hand side is positive and bounded above by some version of itself that is scaled by some $\\alpha \\gt 1$, and in this case $\\alpha:= e$.\n",
    "\n",
    "This completes the proof. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.12 Gruess's Inequality  ** \n",
    "\n",
    "This is a walkthrough of my answer for the final problem, albeit with much more of a probability flavor.  All random variables have a second moment.  \n",
    "\n",
    "Note: this is a considerably longer and different approach than what was taken in the official solution.  However, this approach has a lot of intuitive appeal given its direct ties with probability.  \n",
    "\n",
    "- - - -  \n",
    "*Lemma 1:*  \n",
    "\n",
    "for any $\\gamma \\geq 0$ and any $0 \\leq p\\leq 1$  \n",
    "\n",
    "$p(1-p)\\gamma $ is maximized with $p=0.5$ \n",
    "\n",
    "without loss of generality we ignore the positive scalar $\\gamma$ and maximize $p(1-p)$ which can only be maximized if its positive square root is maximized,\n",
    "\n",
    "\n",
    "$\\big(p(1-p)\\big)^{\\frac{1}{2}} \\leq \\frac{1}{2}\\big(p + (1-p)\\big) = 0.5$  \n",
    "which by $GM \\leq AM$ occurs **iff** $p = 1-p$, i.e. $p= 0.5$\n",
    "\n",
    "scale by our positive $\\gamma$ and our bound on the value is \n",
    "\n",
    "$\\gamma \\sqrt{p(1-p)} \\leq \\frac{1}{2}\\gamma$\n",
    "\n",
    "with the maxmium occuring **iff** $p= 0.5$\n",
    "- - - -  \n",
    "\n",
    "*Lemma 2:* \n",
    "\n",
    "For any bimodal distribution (i.e. one that takes on only 2 values), we can use the Bernouli distributions's variance, after suitable shifting and rescaling.  Recall that shifting does not change the variance, so we may assume without loss of generality that our bimodal distribution takes on values of $0$ with probability $1-p$ and some $d \\gt 0$ with probability $p$.  Hence for bimodal $X$ we can have\n",
    "\n",
    "$var\\big(\\frac{1}{d} X\\big) = \\frac{1}{d^2}var\\big(X\\big) = p\\big(1-p\\big)$  \n",
    "\n",
    "giving us \n",
    "\n",
    "$var \\big(X\\big) = \\sigma_X^2 = d^2 p\\big(1-p\\big)$  \n",
    "\n",
    "and of course the standard deviation is given by:  \n",
    "\n",
    "$\\sigma_X = d \\sqrt{p\\big(1-p\\big)}$ \n",
    "\n",
    "- - - -  \n",
    "\n",
    "*Lemma 3: *  \n",
    "\n",
    "(note: I seem to think there is a maximum entropy type of argument that can get us to this result a lot more quickly....)  \n",
    "\n",
    "\n",
    "For any random variable that takes on values in $[a, A]$ with $ a \\lt \\mu \\lt A$, variance can be maximized by allocating all probability to $a$ and $A$.  \n",
    "\n",
    "To do this argument right will require a little bit of fine tuning, and really should use a CDF, $F_X(X)$.  \n",
    "\n",
    "The idea is that we have a greedy exchange here.  \n",
    "\n",
    "First center the random variable / assume without loss of generality that it has has zero mean.  Carefully using the CDF, assume we find some value $v$ where $a \\lt v \\lt A$ and we have $v$ with some probability $q$ where $0 \\lt q \\leq 1$.  (We can fine tune the argument somewhat to accomodate continuous cases by having $v$ be the the upper bound on the magnitude of a collection of values which collectively have positive probability and said collection has values strictly greater than $a$ and strictly less than $A$.) \n",
    "\n",
    "The argument is we can keep the mean intact but increase our variance via allocating instead to the convex combination:  \n",
    "\n",
    "$rA + (s)a = qv$, where $0\\leq r, s\\leq 1$ and $r + s = q$  \n",
    "\n",
    "or in proper convex combination form: $\\frac{r}{q}A + \\frac{s}{q}a = v$  \n",
    "- - - - \n",
    "for avoidance of doubt, note that  \n",
    "$0A + 1a \\lt v \\lt 1A +0a$  \n",
    "hence by continuity over reals, there must be some $0\\lt p \\lt 1$ where\n",
    "\n",
    "$pA + (1-p)a = v$ \n",
    "- - - - \n",
    "\n",
    "via the strict convexity associated with a squaring operation, we know:  \n",
    "$\\frac{r}{q}A^2 + \\frac{s}{q}a^2 \\gt \\big(\\frac{r}{q}A + \\frac{s}{q}a\\big)^2 = v^2$.  Hence we may multiply each side by $q$ and get  \n",
    "\n",
    "$rA^2 + s a^2 \\gt  qv^2$\n",
    "\n",
    "This generates the following exchange argument:  \n",
    "\n",
    "With random variable $X$ taking on values in $[a,A]$, for any given mean $\\mu$ where $a \\lt \\mu \\lt A$,  suppose we have maximized variance and not all probability is concentrated on $a$ and $A$.  Then there must be at least some $v$ and some associated $0 \\lt q \\lt 1$.  \n",
    "\n",
    "But we can find an even higher variance if we allocate the from $q$ to $r, s$ such that $(r)A + (s)a = qv$.  This is a contradicts the assumption we found a configuration with maximum variance.  \n",
    "\n",
    "Hence any configuration with maximum variance, for any given mean must have all probability on $A$ with probability $p$ and on $a$ with probability $1-p$. \n",
    "\n",
    "(For completeness, we may further state the tologically our bounded interval is a compact set, hence we can be confident that the maximum does in fact exist.)  \n",
    "\n",
    "\n",
    "- - - -  \n",
    "\n",
    "**claim:** \n",
    "\n",
    "where $a \\leq X \\leq A$ and $b \\leq Y \\leq B$\n",
    "\n",
    "$\\big \\vert Cov(X,Y)\\big \\vert \\leq \\frac{1}{4}\\big(A-a\\big)\\big(B-b\\big)$  \n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$\\big \\vert \\rho \\big \\vert = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} \\leq 1$   \n",
    "(see **problem 7.2**)   \n",
    "\n",
    "\n",
    "We now introduce $X^*$, which is a random variable with the same mean as $X$ but all probability is allocated to $\\{a, A\\}$ and also $Y^*$ which is a random variable with the same mean as $Y$ but all probability is allocated to $\\{b, B\\}$\n",
    "\n",
    "Multiply each side by $\\sigma_{X} \\sigma_{Y}$  \n",
    "\n",
    "$\\big \\vert Cov(X,Y)\\big \\vert \\leq \\sigma_{X} \\sigma_{Y} \\leq  \\sigma_{X^*} \\sigma_{Y^*}$  \n",
    "\n",
    "by lemma 3. \n",
    "\n",
    "Now using lemma 2, we know \n",
    "\n",
    "$\\sigma_{X^*} = \\sqrt{(A-a)^2 p(1-p)} =  \\big(\\sqrt{p(1-p)}\\big)(A-a)  \\leq  \\frac{1}{2}(A-a)$ \n",
    "\n",
    "by lemma 1.  \n",
    "\n",
    "Equivalently for $Y^*$ we have:  \n",
    "\n",
    "$\\sigma_{Y^*} = \\sqrt{(B-b)^2 p(1-p)} =  \\big(\\sqrt{p(1-p)}\\big)(B-b)  \\leq  \\frac{1}{2}(B-b)$ \n",
    "\n",
    "putting this together, we have:  \n",
    "\n",
    "$\\big \\vert Cov(X,Y)\\big \\vert \\leq \\sigma_{X} \\sigma_{Y} \\leq \\sigma_{X^*} \\sigma_{Y^*} \\leq \\big(\\frac{1}{2}(A-a)\\big)\\big(\\frac{1}{2}(B-b)\\big) = \\frac{1}{4}(A-a)(B-b)$  \n",
    "\n",
    "which completes the proof.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
