{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Inequalities\n",
    "\n",
    "Here is a companion / sketch of the approach taken in the book.  \n",
    "\n",
    "a few reminders:   \n",
    "1.) This applies to all cases of real roots to some nth degree polynomial.  If the roots are not in reals, then it may or may not apply.  \n",
    "2.) As these symmetric functions map directly to coefficients of the polynomial (except coefficienrts have a $(-1)^k$ scaling them i.e. + or - sign), *and* we are talking about excpected values, i.e $E_k(\\mathbf x) = \\frac{e_k(\\mathbf x)}{\\binom{n}{k}}$), but never the less, talking about a kth symmetric function where $k\\gt n$ is not meaningful.  This leads to the triangular structure in the below outline of the proof, and as is common in math, our main interest is along the diagonal.  \n",
    "3.)  \n",
    "\n",
    "let $\\text{Diag}\\big(\\mathbf x\\big)\\mathbf 1 := \\mathbf x = \\begin{bmatrix}\n",
    "x_1 \\\\ \n",
    "x_2 \\\\ \n",
    "\\vdots\\\\ \n",
    "x_r\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "for all $\\mathbf x \\in \\mathbb R^r$ \n",
    "\n",
    "and with an abuse of notation, \n",
    "\n",
    "let $\\mathbf x^{-1} := \\text{Diag}\\big(\\mathbf x\\big)^{-1}\\mathbf 1 = \\begin{bmatrix}\n",
    "\\frac{1}{x_1} \\\\ \n",
    "\\frac{1}{x_2} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\frac{1}{x_r}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "\n",
    "Overview of the proof.  \n",
    "\n",
    "note that after differentiating this polynomial, we have \n",
    "\n",
    "$Q(t) = P'(t) = \\sum_{k=0}^{n-1} (-1)^k \\binom{n-1}{k}E_k(\\mathbf x)t^{n-k-1} = \\sum_{k=0}^{n-1} (-1)^k \\binom{n-1}{k}E_k(x_1, x_2, ... , x_{n-1}, x_n)t^{n-k-1}$\n",
    "\n",
    "however, by approaching this at the level of the roots to the polynomial, we can also see \n",
    "\n",
    "$Q(t) = \\sum_{k=0}^{n-1} (-1)^k \\binom{n-1}{k}E_k(y_1, y_2, ... , y_{n-1})t^{n-k-1}$\n",
    "\n",
    "where each $y_i$ is real as well.  Thus we match terms and see  \n",
    "\n",
    "$E_k(x_1, x_2, ... , x_{n-1}, x_n) = E_k(y_1, y_2, ... , y_{n-1})$ \n",
    "\n",
    "The main thrust and magic of the proof begins at the bottom of page 181, which is about exploiting/ harnessing this structure.  \n",
    "\n",
    "That is, anything that we can prove holds for all $\\mathbf y \\in \\big[a,b\\big]^{n-1}$ must hold for all $\\mathbf x \\in \\big[a,b\\big]^{n}$ \n",
    "\n",
    "\n",
    "The proof starts in the top left corner of the below.  \n",
    "\n",
    "Note: I've introduced $r = 2$, $r =3$ and so on which is a little clumsy, but hopefully understandable.  To re-write the relation using that terminology, our key bridge is that we know: \n",
    "\n",
    "$E_k(x_1, x_2, ... , x_{r-1}, x_r) = E_k(y_1, y_2, ... , y_{r-1})$ \n",
    "\n",
    "The proof notes that the $r = 1$ case has nothing to prove.  \n",
    "\n",
    "\n",
    "![OverviewOfProofStructure](Chp12_newtonsInequalitiesPlanOfAttack.png)\n",
    "\n",
    "so we start in that top left, light blue cell and prove \n",
    "\n",
    "$E_0(x_1, x_2)E_2(x_1, x_2) \\leq E_1(x_1, x_2)^2$ \n",
    "\n",
    "which is simply stating that $x_1 x_2 \\leq \\big(\\frac{x_1 + x_2}{2}\\big)^2$ which may be verified if we multiply each side by $4$ and then subtract $2x_1x_2$ from each side.  \n",
    "\n",
    "The magic of it is that we now get that entire first column for free. That is, because we proved this for all real $(x_1, x_2)$, these must be the real roots of the derivative of some bigger polynomial with real roots, which must be the real roots of the derivative of some bigger polynomial with real roots, and so on.  \n",
    "\n",
    "Now we attack that next light blue cell on the diagonal, and prove \n",
    "\n",
    "$E_1(\\mathbf x)E_3(\\mathbf x) \\leq E_2(\\mathbf x)^2$ \n",
    "\n",
    "in the case where $r = 3$.   That is we need to prove \n",
    "\n",
    "$\\frac{x_1 + x_2 + x_3}{3}\\big(x_1 x_2 x_3\\big) \\leq \\big(\\frac{x_1 x_2 + x_1 x_3 + x_2 x_3}{3}\\big)^2 $\n",
    "\n",
    "The technique used here is that the bound is trivially true if any $x_i=0$, hence assume none are zero, and multiply each side by $\\big(x_1 x_2 x_3\\big)^{-2}$, which because it is real squared (and not zero) must be positive, and hence not change inequality signs, *and* nicely lines up with the degree of the exponent on the Right Hand Side. This gives us \n",
    "\n",
    "$\\frac{1}{3}\\big(\\frac{1}{x_2 x_3} +\\frac{1}{x_1 x_3} + \\frac{1}{x_1 x_2}\\big) \\leq \\frac{1}{9} \\big(\\frac{1}{x_1 } +\\frac{1}{x_2} + \\frac{1}{x_3}\\big)^2 $\n",
    "\n",
    "multiply each side by 9 and expand the RHS\n",
    "\n",
    "$3\\big(\\frac{1}{x_2 x_3} +\\frac{1}{x_1 x_3} + \\frac{1}{x_1 x_2}\\big) \\leq \\big(\\frac{1}{x_1 } +\\frac{1}{x_2} + \\frac{1}{x_3}\\big)^2 = \\big(\\frac{1}{x_1^2 } +\\frac{1}{x_2^2} + \\frac{1}{x_3^2} + \\frac{2}{x_2 x_3} +\\frac{2}{x_1 x_3} + \\frac{2}{x_1 x_2}\\big)$\n",
    "\n",
    "which simplifies to\n",
    "\n",
    "$\\big(\\frac{1}{x_2 x_3} +\\frac{1}{x_1 x_3} + \\frac{1}{x_1 x_2}\\big) \\leq \\big(\\frac{1}{x_1^2 } +\\frac{1}{x_2^2} + \\frac{1}{x_3^2}\\big)$\n",
    "\n",
    "which should look awfully familiar from the re-arrangement inequality chapter.  In particular, direct application of Cauchy Schwarz gives us \n",
    "\n",
    "$\\big(\\frac{1}{x_2 x_3} +\\frac{1}{x_1 x_3} + \\frac{1}{x_1 x_2}\\big)  = \\big(\\mathbf x^{-1}\\big)^T \\big(\\mathbf P\\mathbf x^{-1}\\big) \\leq \\big \\Vert \\mathbf x^{-1} \\big \\Vert_2 \\big \\Vert \\mathbf {Px}^{-1}\\big \\Vert_2 = \\big \\Vert \\mathbf x^{-1}\\big \\Vert_2 \\big \\Vert \\mathbf {x}^{-1}\\big \\Vert_2 = \\big \\Vert \\mathbf x^{-1}\\big \\Vert_2^2 = \\big(\\frac{1}{x_1^2 } +\\frac{1}{x_2^2} + \\frac{1}{x_3^2}\\big)$  \n",
    "\n",
    "where we used the cyclic permutation matrix $\\mathbf P$. (Note: there is an even more slick way to get to this approach -- something which is dealt with at the end.) \n",
    "\n",
    "We now get all cells below the diagonal in that second column for 'free'.  \n",
    "\n",
    "The final step is, to in general attack all the orange cells along the diagonal.  That is we want to prove \n",
    "\n",
    "$E_{r-2}(\\mathbf x)E_r(\\mathbf x) \\leq E_{r-1}(\\mathbf x)^2$\n",
    "\n",
    "for any natural number $r \\leq n$ \n",
    "\n",
    "The proof seems unwieldly, but in fact uses a similar technique of dividing each side by a squared product.  The *very* clever finish is, after re-arranging and manipulating some inequalities, to then apply what we already have and state the claim must be true because of what we have in that first column also applies to $\\mathbf x^{-1}$, \n",
    "\n",
    "i.e. that $E_{0}(\\mathbf x^{-1})E_2(\\mathbf x^{-1}) \\leq E_{1}(\\mathbf x^{-1})^2$\n",
    "\n",
    "The amount of overlapping subproblems, and symmetry involved in this problem is really quite shocking. \n",
    "\n",
    "\n",
    "There are a couple of ways to interpret this proof.  \n",
    "\n",
    "*The iterative approach*  \n",
    "If we want to prove that all of the lower triangular relations hold, in our above picture, we merely need to prove the top left entry, and march along the diagonal for $n$ iterations in total.  Each time we prove a diagonal entry $j=r$, we automatically get every claim below it in that same column $j$.  Hence once we've done the proof for all $n$ iterations (i.e for each diagonal entry) we have, in effect enumerated, the proof for every component in that lower triangular structure. \n",
    "\n",
    "*The recursive view*  \n",
    "If our ultimate interest is in the bottom row, for any given entry $j$ on that row, we merely need proof that the diagonal entry on $r=j$ (i.e. the entry above it, or equal to it if in the bottom left corner) is true.  Hence by coming up with a general purpose proof for each and every shaded (i.e. diagonal) entry, we have proven everything.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what does this have to do with Maclaurin's Inequalities?  \n",
    "\n",
    "again, with each $x_i \\gt 0$, we can look at our function $E_r(\\mathbf x) $, fixing $\\mathbf x$ and see how it varies with $r$.   \n",
    "\n",
    "In particular, consider $g: \\mathbb R_{\\gt 0} \\longrightarrow \\mathbb R$ given by  \n",
    "$g\\big(\\mathbf x, r\\big) = \\log\\Big(E_r(\\mathbf x)\\Big)$  \n",
    "\n",
    "for natural number $r \\geq 2$  \n",
    "\n",
    "Newton's Inequalities tell us that \n",
    "\n",
    "$\\frac{1}{2}g\\big(\\mathbf x, r-1\\big)  + \\frac{1}{2}g\\big(\\mathbf x, r+1\\big) \\leq  g\\big(\\mathbf x, \\frac{1}{2}(r-1) + \\frac{1}{2}(r+1)\\big)  = g\\big(\\mathbf x, r\\big)$   \n",
    "\n",
    "which should tell us that our continuous function $g$ is J concave, and hence concave in $r$.  There are some technical nits in that $r$ is constrained to be a natural number, however the geometric interpretation as sketching $g$ as a concave function is clear. \n",
    "\n",
    "With this in mind, we can manipulate the inequalities to say:  \n",
    "\n",
    "$\\frac{1}{2}g\\big(\\mathbf x, r+1\\big)- \\frac{1}{2}g\\big(\\mathbf x, r\\big) \\leq \\frac{1}{2}g\\big(\\mathbf x, r\\big) - \\frac{1}{2}g\\big(\\mathbf x, r-1\\big)$   \n",
    "\n",
    "or \n",
    "\n",
    "$g\\big(\\mathbf x, r+1\\big)- g\\big(\\mathbf x, r\\big) \\leq g\\big(\\mathbf x, r\\big) - g\\big(\\mathbf x, r-1\\big)$   \n",
    "\n",
    "\n",
    "One approach from here is to define a function for $r \\in \\mathbb R_{\\geq 1}$, with   \n",
    "$w_2 :=  r-\\lfloor{r}\\rfloor$  \n",
    "$w_1 = 1 - w_2$   \n",
    "i.e. the excess over the rounded down integer value is allocated to $w_2$  \n",
    "\n",
    "so we then define  \n",
    "\n",
    "$h\\big(\\mathbf x, r\\big) :=  w_1 \\cdot g\\big(\\mathbf x, \\left \\lfloor{r}\\right \\rfloor \\big) + w_2 \\cdot g\\big(\\mathbf x, \\left \\lceil{r}\\right \\rceil \\big)$  \n",
    "\n",
    "this is a continuous piecewise linear function that interpolates values of $g$.  Futher, after sketching the graph and understanding how the interpolation works (convex combinations between points), it becomes natural to extend the domain so that we have $h$ with \n",
    "\n",
    "$r \\in \\mathbb R_{\\geq 1}$ where we define $g\\big(\\mathbf x, 0\\big):=0$  \n",
    "\n",
    "$h$ is continuous everywhere.  With a bit of work we can verify that $h$ is concave and verify that Maclaurins Inequalities follow by looking at the certain properities of concave curves, specifically     \n",
    "\n",
    "$\\frac{1}{r+1} g\\big(\\mathbf x, r+1\\big) =\\frac{g\\big(\\mathbf x, r+1\\big) - g\\big(\\mathbf x, 0\\big)}{r+1-0}\\leq \\frac{g\\big(\\mathbf x, r\\big) - g\\big(\\mathbf x, 0\\big)}{r-0} = \\frac{1}{r}g\\big(\\mathbf x, r\\big) $   \n",
    "\n",
    "which is characterization (B) on page 88 of CS Masterclass -- that a function is convex **iff** its sequential secants have non-decreasing slopes -- and in this case of negative convex (aka concave) the sequential secants are non-increasing.  \n",
    "\n",
    "Supposing we verify $h$ is convex, then we look at sequential secants for natural numbers $r$ and we exponentiate out of logspace, then we recover Maclaurin's Inequalities.  \n",
    "\n",
    "- - - -  \n",
    "*an alternative close:*  \n",
    "revisiting:  \n",
    "\n",
    "$g\\big(\\mathbf x, r+1\\big)- g\\big(\\mathbf x, r\\big) \\leq g\\big(\\mathbf x, r\\big) - g\\big(\\mathbf x, r-1\\big)$   \n",
    "\n",
    "with a domain in $r$ of natural numbers   \n",
    "\n",
    "$\\text{main point:  }  $  \n",
    "This tells us we have a deceasing sequence  \n",
    "\n",
    "$... \\leq g\\big(\\mathbf x, 4\\big) - g\\big(\\mathbf x, 3\\big) \\leq  g\\big(\\mathbf x, 3\\big) - g\\big(\\mathbf x, 2\\big) \\leq g\\big(\\mathbf x, 2\\big) - g\\big(\\mathbf x, 1\\big) $  \n",
    "\n",
    "We can close via the use of telescoping    \n",
    "\n",
    "$\\frac{g\\big(\\mathbf x, r+1\\big)}{r+1}$  \n",
    "$= \\frac{1}{r+1}\\Big(\\big\\{g\\big(\\mathbf x, r+1\\big) - g\\big(\\mathbf x, r\\big)\\big\\} + g\\big(\\mathbf x, r\\big) - g\\big(\\mathbf x, r-1\\big)+ ... + g\\big(\\mathbf x, 1\\big) - 0\\Big)$  \n",
    "$=\\big\\{ \\frac{g(\\mathbf x, r+1) - g(\\mathbf x, r)}{r+1}\\big\\} +  \\frac{r}{r+1}\\cdot \\frac{1}{r}\\Big(g\\big(\\mathbf x, r\\big) - g\\big(\\mathbf x, r-1\\big)+ ... + g\\big(\\mathbf x, 1\\big) - 0\\Big)$  \n",
    "$=\\frac{1}{r+1}\\cdot \\big\\{g(\\mathbf x, r+1) - g(\\mathbf x, r)\\big\\} +  \\frac{r}{r+1}\\cdot\\frac{g(\\mathbf x, r)}{r}$  \n",
    "$\\leq \\frac{1}{r+1}\\cdot \\big\\{\\frac{g(\\mathbf x, r)}{r}\\big\\} +  \\frac{r}{r+1}\\cdot\\frac{g(\\mathbf x, r)}{r}$  \n",
    "$= \\frac{g\\big(\\mathbf x, r\\big)}{r}$  \n",
    "\n",
    "Where the inequality needing proven is:  \n",
    "$\\big(g(\\mathbf x, r+1) - g(\\mathbf x, r)\\big) \\leq \\frac{g(\\mathbf x, r)}{r}$  \n",
    "in words: the integer time averaged 'reward' is at least as big as the most recent 'reward'  \n",
    "\n",
    "This can be stated equivalently,   \n",
    "$r\\big(g(\\mathbf x, r+1) - g(\\mathbf x, r)\\big) \\leq g(\\mathbf x, r)$   \n",
    "\n",
    "we can take advantage of positivity and again define $g(\\mathbf x, 0):= 0$  \n",
    "we know, for $0 \\leq i \\leq r$  \n",
    "$g(\\mathbf x, r+1) - g(\\mathbf x, r) \\leq g(\\mathbf x, i+1) - g(\\mathbf x, i)$   \n",
    "by the fact that we have a decreasing sequence    \n",
    "\n",
    "finally, summing over the bound, we have  \n",
    "$r\\big(g(\\mathbf x, r+1) - g(\\mathbf x, r)\\big) = \\sum_{i=0}^{r-1} \\big(g(\\mathbf x, r+1) - g(\\mathbf x, r)\\big) \\leq \\sum_{i=0}^{r-1} \\big(g(\\mathbf x, i+1) - g(\\mathbf x, i)\\big) = g(\\mathbf x, r) - g(\\mathbf x, 0) = g(\\mathbf x, r)$   \n",
    "as required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# deeper look at elementary symmetric functions\n",
    "\n",
    "the function $g$ with $\\mathbf x \\in \\mathbb R^n$ with strictly real non-negative components, given by \n",
    "\n",
    "$g_k\\big(\\mathbf x\\big) = e_k\\big(\\mathbf x\\big)^\\frac{1}{k}$  \n",
    "\n",
    "for $k = \\{2,3, ..., n-1, n\\}$  \n",
    "\n",
    "is a (strictly -- tbc) concave function.  The function of course is linear when $k = 1$ and we ignore the $k=0$ case here as not meaningful.  \n",
    "\n",
    "note the two tests for linearity\n",
    "\n",
    "1.)  $\\alpha g_k\\big(\\mathbf x\\big) = g_k\\big(\\alpha \\mathbf x\\big)$   \n",
    "holds due to homogeneity of the exponent, but \n",
    "\n",
    "2.)  $g_k\\big(\\mathbf x_1\\big) + g_k\\big(\\mathbf x_2\\big) \\leq g_k\\big(\\mathbf x_1 + \\mathbf x_2\\big)$  \n",
    "because the functions are superadditive \n",
    "\n",
    "we have proven this in multiple ways for the $k =n $ case (superadditivity of geometric mean).   \n",
    "For other values of k, we may consult page 35 of Beckenbach and Bellman's *Inequalities*.  (The background to the proof is elementary but requires  considerable symbol manipulation in its build-up -- your author is looking for an elementary but much more succinct approach.)  \n",
    "\n",
    "The key idea, however, is that the above two relationship characterize elementrary symmetric functions as being negative convex, aka concave.  \n",
    "\n",
    "specifically consider real non-negative weights $w_1, w_2$ where $w_1 + w_2 = 1$, the above two properties give us \n",
    "\n",
    "$w_1 g_k\\big(\\mathbf x_1\\big) + w_2 g_k\\big(\\mathbf x_2\\big) = g_k\\big(w_1 \\mathbf x_1\\big) + g_k\\big(w_2 \\mathbf x_2\\big) \\leq g_k\\big(w_1\\mathbf x_1 + w_2 \\mathbf x_2\\big)$  \n",
    "\n",
    "which is our definition for a scalar valued function.  From here we may apply Jensen's Inequality, etc as we please. \n",
    "\n",
    "**sneak peak to chapter 13:**  \n",
    "\n",
    "As the name implies and the function's defintion shows, these are symmetric functions. That is, any permutation of the functions arguments / vector, does not change the result.  \n",
    "\n",
    "so suppose we had  \n",
    "$\\mathbf x_2 = w_1 \\mathbf I \\mathbf x_1 + w_2 \\mathbf P \\mathbf x_1 = w_1 \\mathbf x_1 + w_2 \\mathbf P \\mathbf x_1$   \n",
    "so $\\mathbf x_2  \\preceq \\mathbf x_1$ or we may say or $\\mathbf x_2 \\in H(\\mathbf x_1)$    \n",
    "(i.e. $\\mathbf x_2$ is in the convex hull of $\\mathbf x_1$  )\n",
    "where $\\mathbf P$ is some (non-identity) permutation matrix  \n",
    "\n",
    "- - - -\n",
    "note this may be restated as, there exists some doubly stochastic matrix $\\mathbf D$ where \n",
    "$\\mathbf D := w_1 \\mathbf I + w_2 \\mathbf P$   \n",
    "so $\\mathbf x_2 = \\mathbf D \\mathbf x_1$  \n",
    "- - - -\n",
    "because $g$ is a symmetric function we know $g\\Big(\\big(\\mathbf P \\mathbf x_1\\big)\\Big) = g\\Big(\\mathbf x_1\\Big)$  \n",
    "\n",
    "\n",
    "in the contex of majorization this tells us \n",
    "\n",
    "$ g_k\\Big( \\mathbf x_1 \\Big) = (w_1 + w_2) g_k\\Big( \\mathbf x_1 \\Big) $  \n",
    "$=w_1 g_k\\Big( \\mathbf x_1 \\Big) + w_2 g_k\\Big(\\mathbf x_1\\Big) $  \n",
    "$= w_1 g_k\\Big( \\mathbf x_1 \\Big) + w_2 g_k\\Big( \\big(\\mathbf P \\mathbf x_1\\big)\\Big) $  \n",
    "$= g_k\\Big(w_1 \\mathbf x_1 \\Big) + g_k\\Big(w_2 \\big(\\mathbf P \\mathbf x_1\\big)\\Big)$     \n",
    "$ \\leq g_k\\Big(w_1\\mathbf x_1 + w_2 \\big(\\mathbf P \\mathbf x_1\\big)\\Big) $  \n",
    "$=  g_k\\Big(w_1\\mathbf x_1 + w_2 \\mathbf P \\mathbf x_1\\Big) $  \n",
    "$= g_k\\Big(\\mathbf x_2\\Big)$  \n",
    "\n",
    "thus in the case of majorization, we have \n",
    "\n",
    "$g_k\\big( \\mathbf x_1 \\big) \\leq  g_k\\big(\\mathbf x_2\\big)$  \n",
    "\n",
    "since there is only one term on each side, (again, recalling that everything is real non-negative), we may safely raise each side to the $k$th power giving us  \n",
    "\n",
    "$e_k\\big(\\mathbf x_1\\big) =   g_k\\big( \\mathbf x_1 \\big)^k \\leq  g_k\\big(\\mathbf x_2\\big)^k = e_k\\big(\\mathbf x_2\\big)$  \n",
    "\n",
    "which shows that the kth elementary symmetric function is Schur Concave \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An almost immediate consequence of the above insight is \n",
    "\n",
    "# Minkowki's Determinant Inequality \n",
    "\n",
    "Let $\\mathbf A$ and $\\mathbf B$ be $n$ x $n$  Hermitian positive (semi)definite matrices.  It's immediate that $\\big(\\mathbf A +\\mathbf B\\big)$ is Hermitian and positive(semi) definite.  Let $\\mathbf x_1$, $\\mathbf x_2$ and $\\mathbf x_3$ have the eigenvalues for $\\mathbf A$, $\\mathbf B$, and $\\big(\\mathbf A +\\mathbf B\\big)$ respectively, and ordered in the usual way of $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n$ in each case. \n",
    "\n",
    "There is a majorization relation here, because  \n",
    "$ \\text{trace}\\big(\\mathbf A +\\mathbf B\\big) = \\text{trace}\\big(\\mathbf A\\big) + \\text{trace}\\big(\\mathbf B\\big)$  \n",
    "\n",
    "but two choices are better than one when recovering our eigenvalues via constrained quadratic form maximization problems, or equivalently where $\\mathbf P$ is a projector (i.e. Hermitian and idempotent) of rank $k$, for some fixed $k \\in \\{1,2,...,n\\}$ and  \n",
    "\n",
    "$\\text{trace} \\big(\\mathbf P^*\\mathbf X\\big) =  \\text{max:}\\Big\\{  \\text{trace}\\big(\\mathbf P\\mathbf X\\big)\\Big\\}$   \n",
    "for hermitian positive (semi) definite $\\mathbf X$, then we know  \n",
    "\n",
    "$\\text{trace} \\Big(\\mathbf P_0^*\\big(\\mathbf A +\\mathbf B\\big) \\Big) $  \n",
    "$\\text{max:} \\Big\\{ \\text{trace} \\Big(\\mathbf P_0 \\big(\\mathbf A+ \\mathbf B\\big) \\Big) \\Big\\}$  \n",
    "$\\text{max:} \\Big\\{ \\text{trace} \\Big(\\mathbf P_0 \\mathbf A\\Big) +\\text{trace} \\Big(\\mathbf P_0 \\mathbf B \\Big) \\Big\\}$  \n",
    "$\\leq \\text{max:} \\Big\\{ \\text{trace} \\Big(\\mathbf P_1 \\mathbf A\\Big)\\Big\\} +\\text{max:}\\Big\\{\\text{trace} \\Big(\\mathbf P_2 \\mathbf B \\Big) \\Big\\}$   \n",
    "$=\\text{trace} \\Big(\\mathbf P_1^*\\mathbf A\\Big) +\\text{trace} \\Big(\\mathbf P_2^* \\mathbf B \\Big)$   \n",
    "because two choices are better than one  (and this is a proper majorization because we have equality when $k=n$)  \n",
    "\n",
    "so it is immediate that \n",
    "$\\mathbf x_3 \\preceq \\mathbf x_1 + \\mathbf x_2$  \n",
    "\n",
    "Minkowki's Determinant Inequality is  \n",
    "$\\text{det}\\big(\\mathbf A\\big)^\\frac{1}{n} + \\text{det}\\big(\\mathbf B\\big)^\\frac{1}{n} \\leq \\text{det}\\big(\\mathbf A +\\mathbf B\\big)^\\frac{1}{n}$  \n",
    "\n",
    "or equivalently in term of eigenvalues, we may prove it in one line:  \n",
    "$g_n\\big(\\mathbf x_1\\big) + g_n\\big(\\mathbf x_2\\big) \\leq g_n\\big(\\mathbf x_1 + \\mathbf x_2\\big) \\leq g_n\\big(\\mathbf x_3\\big)$  \n",
    "\n",
    "where the first inequality is true by the super additivity of the geometric mean and the second inequality is true because of the Schur concativity of $e_n$ aka the determinant function --i.e. we could have written it as $g_n\\big(\\mathbf x_1 + \\mathbf x_2\\big)^n \\leq g_n\\big(\\mathbf x_3\\big)^n$ -- *or* as shown above working directly with $g_n$ with Birkhoff's theorem in hand the super additivity of the geometric mean implies this result.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there is a *more elementary approach to proving Minkowki's Inequality* that merely makes use of super additivity of the geometric mean and a clever factorization.  This is shown at the end of \"determinant_addtion_two_matrices_inequality.ipynb\" in the Linear Algebra folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark**   \n",
    "either using an argument similar to the above, or results in the \"Fun_with_Trace...\" notebook immediately under \"Begin Interlude involving singular values and ideas from Majorization\", we also get an immediate proof of the Hadamard Determinant inequality.  \n",
    "\n",
    "i.e. For Hermitian positive (semi)definite matrix  \n",
    "$\\mathbf A = \\mathbf {Q\\Sigma Q}^*$  we have $\\Sigma \\succeq \\mathbf A$ and both have strictly real non-negative items.  Making use of Schur concavity of the nth elementary symmetric function, $e_n$ we know that \n",
    "\n",
    "$\\det \\big(\\mathbf A \\circ \\mathbf I\\big) = \\prod_{i=1}^n a_{i,i} \\geq \\prod_{i=1}^n \\lambda_{i,i} = \\det\\big(\\mathbf A\\big)$  \n",
    "\n",
    "(of course majorization gives us more tools at our disposal, and e.g. we can imediately apply shur concavity of any of the other elementary symmetric polynomials to get inequalities for other terms of the characteristic polynomial of $\\mathbf A$ (after dividing out the sign function.)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Determinant is concave for Hermitian Positive Definite matrices  \n",
    "\n",
    "Let $\\mathbf A$ and $\\mathbf B$ $n$ x $n$ Hermitian positive definite matrices.  \n",
    "\n",
    "consider the function $f$ that is the (natural) log of the determinant of an HPD matrix \n",
    "\n",
    "i.e. \n",
    "\n",
    "$f\\Big(\\mathbf X\\Big) = \\log\\Big( \\det\\big(\\mathbf X\\big)\\Big)$  \n",
    "\n",
    "**claim:**   \n",
    "for any $p \\in (0,1)$ \n",
    "\n",
    "$ p \\cdot f\\Big(\\mathbf A\\Big) + (1-p)\\cdot f\\Big(\\mathbf B\\Big) \\leq f\\Big(p\\cdot \\mathbf A + (1-p)\\cdot \\mathbf B\\Big)  $ \n",
    "\n",
    "hence $f$ is concave \n",
    "\n",
    "where   \n",
    "\n",
    "**tbc**: equality iff $\\mathbf A = \\mathbf B$  \n",
    "\n",
    "(note: this is also problem in Cover and Thomas and is proven there via tools from probability and information theory.  Understanding that proof as well is of considerable interest.)  \n",
    "\n",
    "we prove the above by proving a generalization of $\\text{GM} \\leq \\text{AM}$\n",
    "\n",
    "**claim**  \n",
    "\n",
    "$\\det\\big(\\mathbf A^p \\mathbf B^{(1-p)}\\big) \\leq \\det\\big(p \\mathbf A + (1-p) \\mathbf B\\big)$\n",
    "\n",
    "- - - - \n",
    "for avoidance of doubt, the desired inequality folows by splitting the determinant on LHS to get \n",
    "\n",
    "$\\det\\big(\\mathbf A\\big)^p\\det \\big( \\mathbf B\\big)^{(1-p)}= \\det\\big(\\mathbf A^p\\big)\\det \\big( \\mathbf B^{(1-p)}\\big) \\leq \\det\\big(p \\mathbf A + (1-p) \\mathbf B\\big)$\n",
    "\n",
    "and taking natural logarithms to get \n",
    "\n",
    "$ p \\cdot f\\Big(\\mathbf A\\Big) + (1-p)\\cdot f\\Big(\\mathbf B\\Big) =\\log\\Big(\\det\\big(\\mathbf A\\big)^p\\Big) +\\log\\Big(\\det \\big( \\mathbf B\\big)^{(1-p)}\\Big)  =\\log\\Big(\\det\\big(\\mathbf A^p\\big)\\det \\big( \\mathbf B^{(1-p)}\\big)\\Big)$  $\\leq \\log\\Big(\\det\\big(p \\mathbf A + (1-p) \\mathbf B\\big)\\Big)= f\\Big(p\\cdot \\mathbf A + (1-p)\\cdot \\mathbf B\\Big) $\n",
    "- - - - \n",
    "**proof:** \n",
    "\n",
    "similar to the above problem, let $\\mathbf x_1$, $\\mathbf x_2$ and $\\mathbf x_3$ have the eigenvalues for $\\mathbf A$, $\\mathbf B$, and $\\big(p\\mathbf A + (1-p)\\mathbf B\\big)$  \n",
    "As a reminder, all eigenvalues are strictly positive, and well ordered, such that for $\\mathbf x_k$ we have   \n",
    "$x_{k,1}\\geq x_{k,2} \\geq ...\\geq x_{k, n-1} \\geq x_{k,n}$  \n",
    "\n",
    "\n",
    "by $\\text{GM} \\leq \\text{AM}$ we set up the point-wise bound  \n",
    "\n",
    "$0\\lt x_{1,i}^p  x_{2,i}^{(1-p)} \\leq p\\cdot x_{1,i} + (1-p)x_{2,i}$  \n",
    "\n",
    "taking advantage of positivity, we multiply over the bound and see \n",
    "\n",
    "$0\\lt \\prod_{i=1}^n \\big(x_{1,i}^p  x_{2,i}^{(1-p)}\\big) \\leq \\prod_{i=1}^n \\big(p\\cdot x_{1,i} + (1-p)x_{2,i}\\big)$  \n",
    "\n",
    "or more clearly, in vector notation, where the exponent (and inequality) is evaluated component wise for each row, and $\\circ$ denotes Hadamard product:     \n",
    "\n",
    "$\\big(\\mathbf x_1^p\\big) \\circ \\big(\\mathbf x_2^{(1-p)}\\big) \\leq p\\cdot \\mathbf x_1 + (1-p)\\mathbf x_2$  \n",
    "\n",
    "hence we have \n",
    "\n",
    "$\\det\\Big(\\mathbf A^p \\mathbf B^{(1-p)}\\Big) = \\det\\Big(\\mathbf A^p\\Big)\\det\\Big( \\mathbf B^{(1-p)}\\Big) =e_n\\Big(\\mathbf x_1^p\\Big) \\cdot e_n \\Big(\\mathbf x_2^{(1-p)}\\Big) = e_n\\Big(\\big(\\mathbf x_1^p\\big) \\circ \\big(\\mathbf x_2^{(1-p)}\\big)\\Big) $  \n",
    "$\\leq e_n\\Big(p\\cdot \\mathbf x_1 + (1-p)\\mathbf x_2\\Big) $   \n",
    "$\\leq e_n\\Big(\\mathbf x_3\\Big) = \\det\\Big(p \\mathbf A + (1-p) \\mathbf B\\Big)$   \n",
    "\n",
    "where we've used the fact that \n",
    "\n",
    "$\\big(p\\cdot \\mathbf x_1 + (1-p)\\mathbf x_2 \\big)\\succeq \\mathbf x_3$  \n",
    "\n",
    "and $e_n$ is Schur Concave  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If the reader prefers: we could have written this as the eigenvalues are contained in diagonal matrices \n",
    "\n",
    "$\\mathbf D^{(1)}$, $\\mathbf D^{(2)}$, and $\\mathbf D^{(3)}$, for $\\big(\\mathbf A\\big)$ , $\\big(\\mathbf b\\big)$ and $\\big(p\\cdot \\mathbf A + (1-p)\\mathbf B\\big)$, respectively.  \n",
    "\n",
    "We have an ordering in each of the eigenvalues from largest to smallest \n",
    "\n",
    "$d^{(k)}_{1,1} \\geq d^{(k)}_{2,2} \\geq ... \\geq d^{(k)}_{n,n}$  \n",
    "\n",
    "by $\\text{GM} \\leq \\text{AM}$ we have a pointwise bound:  \n",
    "\n",
    "$\\mathbf 0 \\lt \\big(\\mathbf D^{(1)}\\big)^p  \\big( \\mathbf D^{(2)}\\big)^{(1-p)} \\leq \\big(p\\cdot\\mathbf D^{(1)} + (1-p)\\cdot \\mathbf D^{(2)}\\big)$\n",
    "\n",
    "taking advantage of positivity we can multiply/take determinants over this bound\n",
    "\n",
    "$\\det\\Big(\\mathbf A^p \\mathbf B^{(1-p)}\\Big) = \\det\\Big(\\mathbf A^p\\Big)\\det\\Big( \\mathbf B^{(1-p)}\\Big) =\n",
    "\\det\\Big(\\big(\\mathbf D^{(1)}\\big)^p\\Big)  \\det\\Big( \\big( \\mathbf D^{(2)}\\big)^{(1-p)}\\Big) = \\det\\Big(\\big(\\mathbf D^{(1)}\\big)^p  \\big( \\mathbf D^{(2)}\\big)^{(1-p)}\\Big) $  \n",
    "$\\leq \\det\\big(p\\cdot\\mathbf D^{(1)} + (1-p)\\cdot \\mathbf D^{(2)}\\big)$  \n",
    "$\\leq \\det\\big(\\mathbf D^{(3)}\\big)$  \n",
    "$=\\det\\big(p\\cdot \\mathbf A + (1-p)\\mathbf B\\big) $\n",
    "  \n",
    "where the first inequality follows from our point-wise bound and the second inequality follows because $e_n$ (equivalently in this case: the determinant) is Schur concave.  \n",
    "\n",
    "and we've used the fact \n",
    "\n",
    "$\\big(p\\cdot \\mathbf D^{(1)} + (1-p)\\mathbf D^{(2)}\\big)\\mathbf 1\\succeq \\mathbf D^{(3)}\\mathbf 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below seems to still need some work, unfortunately..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# another look at 12.7\n",
    "*still needs some work*   \n",
    "can I do something like this for the 3 positive variables case to show that the bound isn't tight?\n",
    "\n",
    "$\\frac{1}{4} \\leq x^3 + y^3 + z^3 + 6xyz$  \n",
    "\n",
    "where $x + y + z = 1$\n",
    "\n",
    "$LHS = \\frac{1}{4}\\big(\\frac{1}{GM}\\big)^3 = \\frac{1}{4xyz} \\leq \\frac{x^2}{yz} + \\frac{y^2}{xz} + \\frac{z^2}{xy} + 6 = RHS$  \n",
    "\n",
    "What I want to do with the RHS, hopefully using Muirhead, though I'm not yet sure its justified is do something like, \n",
    "\n",
    "$7 = x + y + z + 6 \\leq \\frac{x^2}{yz} + \\frac{y^2}{xz} + \\frac{z^2}{xy} + 6 = RHS$  \n",
    "\n",
    "and then lower bound the LHS (see power means and Hoelders revisit of it notes)\n",
    "\n",
    "$\\frac{27}{4} = \\frac{1}{4}\\Big(\\frac{3}{1}\\Big)^3 = \\frac{1}{4}\\Big(\\frac{1}{\\frac{1}{3}(x+y+z)}\\Big)^3 \\leq \\frac{1}{4}\\big(\\frac{1}{GM}\\big)^3 = LHS $\n",
    "\n",
    "and then say that \n",
    "$\\frac{27}{4} \\leq \\frac{28}{4} = 7$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.8  \n",
    "\n",
    "The official solution uses a telescoping identity to prove 12.8.  The telescoping identity itself was, unfortunately, not at all obvious to your author.  As such the bulk of this solution is working out and proving the telescoping identity.  The below workthrough is instructive for a few reasons.  However, there probably is a simpler / cleaner approach to proving / deriving the telescoping identity.  Note: as with many of the items in this chapter, there is a distinctive symmetry and permutations flavor to this exercise.  In particular this exercise has cyclic permutations.  I have lingering suspicions that there are more insights to be had via group theory here.  \n",
    "\n",
    "\n",
    "$\\mathbf a := \\begin{bmatrix}\n",
    "a_1 \\\\ \n",
    "a_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "a_n\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "however, the ordering in $\\mathbf b$ is flipped (perhaps via application of the reflection matrix $\\mathbf J$)\n",
    "\n",
    "$\\mathbf b  := \\begin{bmatrix}\n",
    "b_n \\\\ \n",
    "b_{n-1} \\\\ \n",
    "\\vdots \\\\ \n",
    "b_1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "now consider the function given by \n",
    "\n",
    "$f\\big(\\mathbf a, k\\big) = a_1 a_2... a_{k-1}a_k$  \n",
    "\n",
    "which also means \n",
    "\n",
    "$f\\big(\\mathbf b, k\\big) = b_n b_{n-1}... b_{n-k + 2} b_{n-k +1} =  b_{n-k +1}b_{n-k + 2}...b_{n-1} b_{n}$  \n",
    "\n",
    "\n",
    "where we use the conventions that \n",
    "\n",
    "$f\\big(\\mathbf a, 0 \\big) = 1$  and $f\\big(\\mathbf b, 0 \\big) = 1$  \n",
    "- - - - - \n",
    "\n",
    "\n",
    "also consider the Lie Bracket $\\big[y, z\\big] = y - z$ \n",
    "\n",
    "our goal is to prove\n",
    "\n",
    "$a_1 a_2 ... a_n - b_1 b_2 ... b_n = \\sum_{r=1}^n \\big(\\prod_{k=1}^{r-1} a_k\\big) \\big(a_r - b_r\\big)\\big(\\prod_{k=r+1}^n b_k \\big)$  \n",
    "\n",
    "Consider:  \n",
    "\n",
    "$LHS = \\big[f\\big(\\mathbf a, n\\big), f\\big(\\mathbf b, n\\big)\\big]  = \\big(a_1 a_2 ... a_n\\big) - \\big(b_1 b_2 ... b_n\\big) $ \n",
    "\n",
    "in long form we have:  \n",
    "\n",
    "\n",
    "$RHS = \\big[f\\big(\\mathbf a, 1\\big)f\\big(\\mathbf b, n-1\\big), f\\big(\\mathbf a, 0\\big)f\\big(\\mathbf b, n\\big)\\big] + \\big[f\\big(\\mathbf a, 2\\big)f\\big(\\mathbf b, n-2\\big), f\\big(\\mathbf a, 1\\big)f\\big(\\mathbf b, n-1\\big)\\big] + \\big[f\\big(\\mathbf a, 3\\big)f\\big(\\mathbf b, n-3\\big), f\\big(\\mathbf a, 2\\big)f\\big(\\mathbf b, n-2\\big)\\big] + ... + \\big[f\\big(\\mathbf a, n\\big)f\\big(\\mathbf b, 0\\big), f\\big(\\mathbf a, n-1\\big)f\\big(\\mathbf b, 1\\big)\\big]$\n",
    "\n",
    "- - - - \n",
    "and in condensed form we have:  \n",
    "\n",
    "$RHS = \\sum_{r=1}^n \\big[f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big), f\\big(\\mathbf a, r-1\\big)f\\big(\\mathbf b, n-(r-1)\\big)\\big] $ \n",
    "\n",
    "$ RHS = \\sum_{r=1}^n f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big) - f\\big(\\mathbf a, r-1\\big)f\\big(\\mathbf b, n-(r-1)\\big)$ \n",
    "\n",
    "$ RHS= \\Big(\\sum_{r=1}^n f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big) - \\Big(\\sum_{r=1}^n f\\big(\\mathbf a, r-1\\big)f\\big(\\mathbf b, n-(r-1)\\big)\\Big)$\n",
    "\n",
    "switching indices for the second sum\n",
    "\n",
    "$ RHS = \\Big(\\sum_{r=1}^n f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big) - \\Big(\\sum_{r=0}^{n-1} f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big)$ \n",
    "\n",
    "$RHS= f\\big(\\mathbf a, n\\big)f\\big(\\mathbf b, 0\\big) + \\Big(\\sum_{r=1}^{n-1} f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big) - \\Big(\\sum_{r=1}^{n-1} f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big) - f\\big(\\mathbf a, 0\\big)f\\big(\\mathbf b, n\\big) $\n",
    "\n",
    "$RHS= f\\big(\\mathbf a, n\\big)f\\big(\\mathbf b, 0\\big) - f\\big(\\mathbf a, 0\\big)f\\big(\\mathbf b, n\\big)=  \\big[f\\big(\\mathbf a, n\\big), f\\big(\\mathbf b, n\\big)\\big]  = \\big(a_1 a_2 ... a_n\\big) - \\big(b_1 b_2 ... b_n\\big)  = LHS$ \n",
    "\n",
    "\n",
    "We thus have \n",
    "\n",
    "$\\big(a_1 a_2 ... a_n\\big) - \\big(b_1 b_2 ... b_n\\big) = \\sum_{r=1}^n \\big[f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big), f\\big(\\mathbf a, r-1\\big)f\\big(\\mathbf b, n-(r-1)\\big)\\big]  = \\sum_{r=1}^n \\big(\\prod_{k=1}^{r-1} a_k\\big) \\big(a_r - b_r\\big)\\big(\\prod_{k=r+1}^n b_k \\big)$  \n",
    "\n",
    "- - - - \n",
    "*comment: there must be a smoother / easier way to show this telescoping identiy*  \n",
    "\n",
    "with this out of the way, the Weierstrass inequality, which is true over complex numbers where each $\\big \\vert a_k\\big \\vert \\leq 1$ and $\\big \\vert b_k\\big \\vert \\leq 1$\n",
    " \n",
    "is given by:  \n",
    "\n",
    "where, again \n",
    "\n",
    "$LHS = \\big(a_1 a_2 ... a_n\\big) - \\big(b_1 b_2 ... b_n\\big)$ \n",
    "\n",
    "$\\Big \\vert LHS \\Big \\vert = \\Big \\vert \\sum_{r=1}^n \\big(\\prod_{k=1}^{r-1} a_k\\big) \\big(a_r - b_r\\big)\\big(\\prod_{k=r+1}^n b_k \\big) \\Big \\vert \\leq \\sum_{r=1}^n\\Big \\vert  \\big(\\prod_{k=1}^{r-1} a_k\\big)\\Big \\vert  \\Big \\vert \\big(a_r - b_r\\big)\\Big \\vert  \\Big \\vert \\big(\\prod_{k=r+1}^n b_k \\big) \\Big \\vert \\leq \\sum_{r=1}^n \\Big \\vert \\big(a_r - b_r\\big)\\Big \\vert $  \n",
    "\n",
    "where we first apply triangle inequality, and then recall that the product of numbers with magnitude at most 1, itself has a magnitude at most 1.  \n",
    "\n",
    "Thus we've proven\n",
    "\n",
    "$\\Big \\vert a_1 a_2 ... a_n - b_1 b_2 ... b_n \\Big \\vert  \\leq \\sum_{r=1}^n \\Big \\vert a_r - b_r \\Big \\vert $ \n",
    "\n",
    "where each $\\big \\vert a_k\\big \\vert \\leq 1$ and $\\big \\vert b_k\\big \\vert \\leq 1$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Muirhead applied to a probability problem**  \n",
    "https://math.stackexchange.com/questions/3544581/prove-mathbfex2-geq-frac-mathbfex2-alpha-mathbfe-x-alp/3544711?noredirect=1#3544749  \n",
    "\n",
    "Random variable $X \\gt 0$ with $\\mathbb{E}\\big[X\\big] = 1$ and $0 < \\alpha < 1$. $\\mathbb E\\big[X^2\\big] \\lt \\infty$  \n",
    "*claim:*    \n",
    "$\\begin{equation}\n",
    "\\mathbb{E}[X^2] \\geq \\frac{ \\mathbb{E}[X^{2\\alpha}]}{ \\mathbb{E} [X^{\\alpha}]^2}\n",
    "\\end{equation}$    \n",
    "\n",
    "*proof* \n",
    "\n",
    "A common tactic would be to set up a pointwise bound between $\\big(X_1^2 \\cdot X_2^\\alpha\\cdot X_3^\\alpha- X_1^{2\\alpha}\\cdot X_2 \\cdot X_3\\big)\\geq 0$ with iid $X_i$.  But this is too much to ask for and isn't true.  However  we can symmetrize the above and recover the desired bound  \n",
    "\n",
    "$Y:= \\sum_{\\sigma \\in S_3}\\big( X_{\\sigma(1)}^2 \\cdot X_{\\sigma(2)}^\\alpha\\cdot X_{\\sigma(3)}^\\alpha -  X_{\\sigma(1)}^{2\\alpha}\\cdot X_{\\sigma(2)} \\cdot X_{\\sigma(3)}\\big)\\geq 0$. \n",
    "\n",
    "We know $Y$ is a nonnegative r.v due to Muirhead's Inequality.  Taking expectations of $\\frac{1}{6}Y$ gives the result.  \n",
    "\n",
    "note:  \n",
    "$ \\begin{bmatrix} 1 \\\\  1 \\\\  2a \\end{bmatrix}\\preceq \\begin{bmatrix} 2 \\\\  a \\\\  a \\end{bmatrix} $  \n",
    "because looking at the (ordered) partial sums we have, if $a \\leq \\frac{1}{2}$    \n",
    "$1 \\leq 2$  \n",
    "$1+ 1 = 2 \\leq 2 + a$  \n",
    "$1+ 1 + 2a = 2 + 2a = 1+1+ 2a $   \n",
    "\n",
    "and for $a \\in \\big(\\frac{1}{2},1\\big)$ it reads  \n",
    "$2a \\leq 2$  \n",
    "$2a+ 1= a +(a+1) \\leq 1 +(a+1) =  2 + a$  \n",
    "$1+ 1 + 2a = 2 + 2a = 1+1+ 2a $   \n",
    "\n",
    "the desired result follows that  \n",
    "$\\sum_{k=0}^2 w_{k+1} P^{k}\\begin{bmatrix} 2 \\\\  a \\\\  a \\end{bmatrix} =w_1\\begin{bmatrix} 2 \\\\  a \\\\  a \\end{bmatrix} + w_2\\begin{bmatrix} a \\\\  2 \\\\  a \\end{bmatrix} + w_3\\begin{bmatrix} a \\\\  a \\\\  2 \\end{bmatrix} = \\begin{bmatrix} 2a \\\\  1 \\\\  1 \\end{bmatrix}$  \n",
    "with cyclic permutation matrix $P$ (order 3)  \n",
    "by basic results on majorization from chapter 13.  \n",
    "\n",
    "*Now, if we didn't know chapter 13 results on majroization we could still prove the desired claim about convex hulls as follows*  \n",
    "i.e. the long form way of proving  \n",
    "$w_1\\begin{bmatrix} 2 \\\\  a \\\\  a \\end{bmatrix} + w_2\\begin{bmatrix} a \\\\  2 \\\\  a \\end{bmatrix} + w_3\\begin{bmatrix} a \\\\  a \\\\  2 \\end{bmatrix} = \\begin{bmatrix} 2a \\\\  1 \\\\  1 \\end{bmatrix}$  \n",
    "\n",
    "with $a \\in (0,1)$   \n",
    "is to consider the system of equations   \n",
    "$M \\mathbf w = \\mathbf b$   \n",
    "\n",
    "$\\displaystyle \\left[\\begin{matrix}2 & a & a\\\\a & 2 & a\\\\a & a & 2\\end{matrix}\\right] \\mathbf w= \\displaystyle \\left[\\begin{matrix}2 a\\\\1\\\\1\\end{matrix}\\right]$  \n",
    "\n",
    "note: $M$ is invertible (e.g. strictly diagonally dominant)\n",
    "\n",
    "so the solution exists and is unique  \n",
    "\n",
    "it may be convenient to rescale and look at  \n",
    "$D\\mathbf w =\\Big(\\frac{1}{2(a+1)} \\displaystyle \\left[\\begin{matrix}2 & a & a\\\\a & 2 & a\\\\a & a & 2\\end{matrix}\\right]\\Big) \\mathbf w= \\displaystyle\\left[\\begin{matrix}\\frac{2 a}{2(a+1)}\\\\ \\frac{1}{2(a+1)}\\\\ \\frac{1}{2(a+1)}\\end{matrix}\\right]$  \n",
    "\n",
    "for doubly stochastic matrix $D$.  We can then confirm that $\\mathbf w$ sums to one because  \n",
    "$\\mathbf 1^T D \\mathbf w = \\mathbf 1^T\\mathbf w = \\mathbf 1^T \\displaystyle\\left[\\begin{matrix}\\frac{2 a}{2(a+1)}\\\\ \\frac{1}{2(a+1)}\\\\ \\frac{1}{2(a+1)}\\end{matrix}\\right] = \\frac{2a}{2(a+1)}+\\frac{1}{2(a+1)} + \\frac{1}{2(a+1)} = \\frac{2a +2}{2(a+1)} = 1$  \n",
    "\n",
    "it remains to confirm that (component-wise) $\\mathbf w \\geq 0$  i.e. so that we confirm this is a convex combination not just an affine combination.  \n",
    "\n",
    "It is useful to consider a 'boundary condition of some kind':  \n",
    "$B(\\alpha) :=  D$   (i.e. just being explicit that the matrix is a function of $\\alpha$)  \n",
    "\n",
    "and  \n",
    "$B(0)\\mathbf w_{\\alpha=0} =  \\displaystyle\\left[\\begin{matrix}\\frac{0}{2(0+1)}\\\\ \\frac{1}{2(0+1)}\\\\ \\frac{1}{2(0+1)}\\end{matrix}\\right]\\longrightarrow \\mathbf w_{\\alpha=0}=\\displaystyle\\left[\\begin{matrix}0\\\\ \\frac{1}{2}\\\\ \\frac{1}{2}\\end{matrix}\\right]$  \n",
    "a more direct and perhaps careful argument is to attack the maximums and minimum.  \n",
    "\n",
    "A useful approach is to attack the zeros of $\\mathbf w$ for any $\\alpha \\in(0,1)$ we can say if $w_1 = 0$ then we recurse on a smaller subproblem and have (ignoring the re-scaling associated with double stochastic matrix)    \n",
    "\n",
    "$\\displaystyle \\left[\\begin{matrix} 2 & a\\\\a & 2\\end{matrix}\\right] \\mathbf w_{\\text{ \\ w_1}}= \\displaystyle\\left[\\begin{matrix}1\\\\ 1\\\\\\end{matrix}\\right]$  \n",
    "so  \n",
    "$1= \\max 2 \\big(w_2 \\cdot 2, w_3 \\cdot 2\\big) \\leq \\max 2 \\big(w_2 \\cdot 2 + w_3 \\cdot \\alpha,w_2 \\cdot \\alpha + w_3 \\cdot 2\\big) = 1$  \n",
    "so $\\alpha = 0$ and we recover the $B(0)$ case  \n",
    "\n",
    "By symmetry it remains to consider one of $w_2=0$ or $w_3=0$, so supposing $w_3 =0$, this implies in row 3  \n",
    "$1 \\gt a = (w_1 + w_2) a = 1$  \n",
    "since $a \\in(0,1)$ which is a contradiction.  \n",
    "\n",
    "Thus none of the $w_i$ can be zero for any $\\alpha \\in (0,1)$.  But by continuity of matrix multiplication (and inversion) if there was some $\\tau' \\in (0,1)$ where one of the components of $\\mathbf w$ were negative, then there must exist some $\\tau^* \\in (0, \\tau')$ where $B(\\tau^*)\\mathbf w_{\\tau^*}= \\mathbf b_{\\tau^*}$  and $\\mathbf w_{\\tau^*}$ has at least one component that is zero, which is a contradiction.  \n",
    "\n",
    "Hence we conclude that each weight in $\\mathbf w$ is real non-negative and thus we do have an actual convex combination. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# extension:  \n",
    "\n",
    "for a particular challenge problem:  \n",
    "\n",
    "\n",
    "which in general can be written as something like:  \n",
    "\n",
    "$\\prod_{i=1}^n x_i^{\\frac{1}{n}\\sum_{i =1}^n x_i} \\leq \\prod_{i=1}^n x_i^{x_i} $   \n",
    "\n",
    "where each $x_i \\gt 0$ \n",
    "\n",
    "and without loss of generality, we may assume:  \n",
    "$ x_1 \\geq x_2 \\geq ... \\geq x_n \\gt 0$   \n",
    "\n",
    "\n",
    "we can prove it as follows:  \n",
    "\n",
    "$\\prod_{i=1}^n x_i^{\\frac{1}{n}\\sum_{i =1}^n x_i} \\leq\\frac{1}{n!}  \\big(\\sum_{\\sigma \\in S} \\prod_{i=1}^n x_{\\sigma(k)}^{x_i}   \\big) \\leq \\text{maximal product amongst all permutations} = \\prod_{i=1}^n x_i^{x_i} $   \n",
    "\n",
    "while there are some notational challenges, the inequality from left to center *is* Muirhead's Inequality, i.e. the exponents on the LHS are in the convex hull of that in the middle\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\frac{1}{n}\\sum_{i =1}^n x_i\\\\ \n",
    "\\frac{1}{n}\\sum_{i =1}^n x_i\\\\ \n",
    "\\vdots\\\\ \n",
    "\\frac{1}{n}\\sum_{i =1}^n x_i\\\\ \n",
    "\\frac{1}{n}\\sum_{i =1}^n x_i\n",
    "\\end{bmatrix} \\preceq \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "x_2 \\\\ \n",
    "\\vdots\\\\ \n",
    "x_{n-1}\\\\ \n",
    "x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and there is one possible permutation of the LHS, whereas there are $n!$ permutations in the middle.  Per Muirhead's Inequality, we know that this expected sum over all permutations is at least as big as the expected sum over all permutations of that product in its convex hull.  \n",
    "\n",
    "The center to RHS inequality \n",
    "$  \\frac{1}{n!}  \\big(\\sum_{\\sigma \\in S} \\prod_{i=1}^n x_{\\sigma(k)}^{x_i}   \\big) \\leq \\text{maximal product amongst all permutations}$ \n",
    "\n",
    "is stated as the expected value over all permutations of the product is less than or equal to the maximal product.  \n",
    "- - - -\n",
    "*Final leg:*    \n",
    "\n",
    "Here are 3 ways to prove that \n",
    "\n",
    "$\\text{maximal product amongst all permutations} = \\prod_{i=1}^n x_i^{x_i}$.  \n",
    "\n",
    "1.)  \n",
    "\n",
    "It's obvious -- basically via a greedy strategy.  (Ok maybe not quite obvious enough.)\n",
    "\n",
    "2.) \n",
    "\n",
    "taking advantage of positivity, re-write with the exponential map as  \n",
    "$\\prod_{i=1}^n \\big(x_i\\big)^{x_i} = \\prod_{i=1}^n \\big(e^{\\gamma_i}\\big)^{x_i}  = \\prod_{i=1}^n e^{x_i \\gamma_i} = e^{\\sum_{i=1}^n \\gamma_i x_i}$   \n",
    "\n",
    "and recognize that the $\\gamma_i$'s are in the same ordering as the $x_i$'s, hence   \n",
    "$\\sum_{i=1}^n \\gamma_i x_i \\geq \\sum_{i=1}^n \\gamma_{\\sigma(i)} x_i$  \n",
    "by the re-arrangement inequality\n",
    "\n",
    "3.) \n",
    "\n",
    "Use a slightly modified form of KL Divergence.  Specifically, consider the result that for positive $s_k$ and positive $t_k$, \n",
    "\n",
    "where $\\sum_{k} s_k = \\sum_{k} t_k = c \\gt 0$   \n",
    "\n",
    "we have a (modified) divergence of \n",
    "\n",
    "$\\sum_{k} - s_k \\log\\big(\\frac{t_k}{s_k}\\big)\\geq 0$  \n",
    "\n",
    "proof: \n",
    "multiply each side by $\\frac{1}{c}$  \n",
    "\n",
    "$\\sum_{k} - \\frac{s_k}{c} \\log\\big(\\frac{t_k}{s_k}\\big)\\geq 0$  \n",
    "\n",
    "notice that $-\\log$ is a strictly convex function, and $\\big(\\frac{s_k}{c}\\big)$ can be interpreted as a probability.  Applying Jensen's Inequality, we have \n",
    "\n",
    "$\\sum_{k} - \\frac{s_k}{c} \\log\\big(\\frac{t_k}{s_k}\\big) = E\\Big[-\\log \\big(\\frac{T}{S}\\big)\\Big] \\geq -\\log\\Big(E\\big[\\frac{T}{S}\\big]\\Big) = -\\log \\Big(\\sum_{k} \\frac{s_k}{c} \\frac{t_k}{s_k}\\Big) = -\\log\\Big(\\frac{1}{c} \\sum_{k} t_k\\Big) = -\\log\\Big(\\frac{1}{c} c \\Big) = -\\log\\Big(1 \\Big)  = 0 $  \n",
    "\n",
    "with equality iff $t_k = (\\alpha) s_k$   \n",
    "\n",
    "i.e. the fraction $\\frac{t_k}{s_k} = \\alpha$\n",
    "\n",
    "where $\\alpha$ is some constant.  \n",
    "\n",
    "However because their sums are the same, this equality case means \n",
    "\n",
    "$0 \\lt c = \\sum_{k} (t_k) =  \\sum_{k} (\\alpha  s_k)  = \\alpha \\cdot \\big(\\sum_{k} s_k\\big) = \\alpha \\cdot c$ \n",
    "\n",
    "if we divide everything by $c$ or equivalently by $\\big(\\sum_{k} s_k \\big)$, then we see that it must be the case that $\\alpha = 1 $, which means the lower bound of 0 is achievable **iff** $t_k  = s_k$  for each $k$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we re-visit our modified divergence inequality, we see:  \n",
    "\n",
    "\n",
    "$\\sum_{k} - s_k \\log\\big(\\frac{t_k}{s_k}\\big)\\geq 0$  can be re-written as \n",
    "\n",
    "$\\sum_{k} - \\Big(s_k \\log\\big(\\frac{t_k}{s_k}\\big)\\Big) = \\sum_{k} -\\Big(s_k \\log\\big(t_k\\big) - s_k log\\big(s_k\\big)\\Big) = \\Big(\\sum_{k} -s_k \\log\\big(t_k\\big)\\Big) + \\Big(\\sum_{k}  s_k \\log\\big(s_k\\big)\\Big) \\geq 0$\n",
    "\n",
    "which is equivalent to  \n",
    "\n",
    "$\\sum_{k}  \\log\\big(s_k^{s_k} \\big)  = \\sum_{k}  s_k \\log\\big(s_k\\big) \\geq   \\sum_{k} s_k \\log\\big(t_k\\big)= \\sum_{k}  \\log\\big(t_k^{s_k} \\big)$  \n",
    "\n",
    "which is equivalent to  \n",
    "\n",
    "$ \\log\\big(\\prod_{k} s_k^{s_k}\\big) = \\sum_{k}  \\log\\big(s_k^{s_k}\\big) \\geq \\sum_{k} \\log\\big(t_k^{s_k}\\big) =  \\log\\big(\\prod_{k} t_k^{s_k}\\big)$  \n",
    "\n",
    "taking advantage of positivity, if we exponentiate both sides, we see \n",
    "\n",
    "$\\prod_{k} s_k^{s_k} \\geq \\prod_{k} t_k^{s_k}$    \n",
    "\n",
    "again where $s_k \\gt 0$ and $t_k \\gt 0$ and $\\sum_k s_k = \\sum_k t_k $ \n",
    "- - - - \n",
    "Finally, to tie out any loose ends, \n",
    "\n",
    "consider the function $g$ where \n",
    "\n",
    "$g\\big(\\mathbf a , \\mathbf x \\big) = \\prod_{k} a_k^{x_k}$     \n",
    "\n",
    "then we have \n",
    "\n",
    "$g\\Big(\\mathbf x, \\mathbf x \\Big) = \\prod_{k} x_k^{x_k} \\geq  \\prod_{i=1}^n x_{\\sigma(k)}^{x_i}    = g\\Big(\\mathbf {Px}, \\mathbf x \\Big)$  \n",
    "\n",
    "for any permutation matrix $\\mathbf P$  \n",
    "\n",
    "- - - - \n",
    "alternative solution:  \n",
    "\n",
    "a very quick, very nice approach is to more explicitly use some ideas from Majorization: specifically Birkoff's theorem on doubly stochastic matrices, and a re-arrangement inequality.  \n",
    "\n",
    "$\\prod_{i=1}^n x_i^{\\frac{1}{n}\\sum_{i =1}^n x_i} = \\prod_{i=1}^n x_i^{\\mu} \\leq \\prod_{i=1}^n x_i^{x_i} $\n",
    "\n",
    "where taking advantage of positivity:  \n",
    "\n",
    "$e^{\\gamma_i}  := x_i $  \n",
    "\n",
    "$\\prod_{i=1}^n x_i^{\\mu} = \\exp\\big(\\sum_{i=1}^n \\gamma_i \\mu \\big)\\leq  \\exp\\big(\\sum_{i=1}^n \\gamma_i x_i \\big) = \\prod_{i=1}^n x_i^{x_i} $\n",
    "\n",
    "hence we merely need to prove \n",
    "\n",
    "$\\exp\\big(\\sum_{i=1}^n \\gamma_i \\mu \\big)\\leq  \\exp\\big(\\sum_{i=1}^n \\gamma_i x_i \\big) $ \n",
    " \n",
    "or equivalently \n",
    "\n",
    "$\\sum_{i=1}^n \\gamma_i \\mu \\leq  \\sum_{i=1}^n \\gamma_i x_i $\n",
    "\n",
    "this is true by a rearrangement inequality i.e. because $\\mathbf \\mu$ is majorized by $\\mathbf x$ we have a doubly stochastic matrix $A$  \n",
    "\n",
    "where $\\mathbf u = A \\mathbf x$ \n",
    "\n",
    "and by Birkhofff's Theorem we know that $A$ is a convex combination of Permutations matrices (and from there the rearrangement inequality is called)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**work in progress** (p. 161 of Cover and Thomas... perhaps problem 9.36 on p.484 as well)  \n",
    "\n",
    "for a refinement of this, where \n",
    "\n",
    "KL_divergence_and_entropy.ipynb\n",
    "\n",
    "$\\prod_{i=1}^n x_i^{w_i} \\leq \\prod_{i=1}^n x_i^{x_i}$\n",
    "\n",
    "where $0 \\lt w_i$ and $c= \\sum_{i=1}^n w_i = \\sum_{i=1}^n x_i$   \n",
    "and $0 \\lt x_i$   \n",
    "\n",
    "we can assume that WLOG the exponents sum to one.  If they do not, raise each side to the $\\frac{1}{c}$ power.  From there we can also decompose the $x_i$ in the base.  The result is \n",
    "\n",
    "**claim:** \n",
    "$\\prod_{i=1}^n (x_i\\lambda_i)^{w_i} \\leq \\prod_{i=1}^n (x_i\\lambda_i)^{x_i} $  \n",
    "\n",
    "where $0 \\lt \\lambda_i $   \n",
    "\n",
    "**proof:**  \n",
    "taking logs of each side  \n",
    "$\\sum_{i=1}^n w_i \\log(x_i\\lambda_i)  = \\sum_{i=1}^n w_i \\log(x_i\\frac{w_i}{w_i}\\lambda_i)  = \\sum_{i=1}^n w_i \\log(\\frac{x_i}{w_i}) + \\sum_{i=1}^n w_i \\log(w_i) + \\sum_{i=1}^n w_i \\log(\\lambda_i)$ \n",
    "$= - D_{\\text{KL}}\\big( \\mathbf w \\big \\Vert  \\mathbf x\\big) + - H\\big(\\mathbf w\\big) + \\sum_{i=1}^n w_i \\log(\\lambda_i) $  \n",
    "$\\leq - H\\big(\\mathbf w\\big) + \\sum_{i=1}^n w_i \\log(\\lambda_i) $    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
