{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "it may be good to bring in some of the final touches and ideas in this for explicitly extracting Maclaurin's Inequalities from the end of Newton's Inequalities:  \n",
    "\n",
    "https://math.stackexchange.com/questions/1081075/a-generalization-of-arithmetic-and-geometric-means-using-elementary-symmetric-po\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Inequalities\n",
    "\n",
    "Here is a companion / sketch of the approach taken in the book.  \n",
    "\n",
    "a few reminders:   \n",
    "1.) This applies to all cases of real roots to some nth degree polynomial.  If the roots are not in reals, then it may or may not apply.  \n",
    "2.) As these symmetric functions map directly to coefficients of the polynomial (except coefficienrts have a $(-1)^k$ scaling them i.e. + or - sign), *and* we are talking about excpected values, i.e $E_k(\\mathbf x) = \\frac{e_k(\\mathbf x)}{\\binom{n}{k}}$), but never the less, talking about a kth symmetric function where $k\\gt n$ is not meaningful.  This leads to the triangular structure in the below outline of the proof, and as is common in math, our main interest is along the diagonal.  \n",
    "3.)  \n",
    "\n",
    "let $\\text{Diag}\\big(\\mathbf x\\big)\\mathbf 1 := \\mathbf x = \\begin{bmatrix}\n",
    "x_1 \\\\ \n",
    "x_2 \\\\ \n",
    "\\vdots\\\\ \n",
    "x_r\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "for all $\\mathbf x \\in \\mathbb R^r$ \n",
    "\n",
    "and with an abuse of notation, \n",
    "\n",
    "let $\\mathbf x^{-1} := \\text{Diag}\\big(\\mathbf x\\big)^{-1}\\mathbf 1 = \\begin{bmatrix}\n",
    "\\frac{1}{x_1} \\\\ \n",
    "\\frac{1}{x_2} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\frac{1}{x_r}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "\n",
    "Overview of the proof.  \n",
    "\n",
    "note that after differentiating this polynomial, we have \n",
    "\n",
    "$Q(t) = P'(t) = \\sum_{k=0}^{n-1} (-1)^k \\binom{n-1}{k}E_k(\\mathbf x)t^{n-k-1} = \\sum_{k=0}^{n-1} (-1)^k \\binom{n-1}{k}E_k(x_1, x_2, ... , x_{n-1}, x_n)t^{n-k-1}$\n",
    "\n",
    "however, by approaching this at the level of the roots to the polynomial, we can also see \n",
    "\n",
    "$Q(t) = \\sum_{k=0}^{n-1} (-1)^k \\binom{n-1}{k}E_k(y_1, y_2, ... , y_{n-1})t^{n-k-1}$\n",
    "\n",
    "where each $y_i$ is real as well.  Thus we match terms and see  \n",
    "\n",
    "$E_k(x_1, x_2, ... , x_{n-1}, x_n) = E_k(y_1, y_2, ... , y_{n-1})$ \n",
    "\n",
    "The main thrust and magic of the proof begins at the bottom of page 181, which is about exploiting/ harnessing this structure.  \n",
    "\n",
    "That is, anything that we can prove holds for all $\\mathbf y \\in \\big[a,b\\big]^{n-1}$ must hold for all $\\mathbf x \\in \\big[a,b\\big]^{n}$ \n",
    "\n",
    "\n",
    "The proof starts in the top left corner of the below.  \n",
    "\n",
    "Note: I've introduced $r = 2$, $r =3$ and so on which is a little clumsy, but hopefully understandable.  To re-write the relation using that terminology, our key bridge is that we know: \n",
    "\n",
    "$E_k(x_1, x_2, ... , x_{r-1}, x_r) = E_k(y_1, y_2, ... , y_{r-1})$ \n",
    "\n",
    "The proof notes that the $r = 1$ case has nothing to prove.  \n",
    "\n",
    "\n",
    "![OverviewOfProofStructure](Chp12_newtonsInequalitiesPlanOfAttack.png)\n",
    "\n",
    "so we start in that top left, light blue cell and prove \n",
    "\n",
    "$E_0(x_1, x_2)E_2(x_1, x_2) \\leq E_1(x_1, x_2)^2$ \n",
    "\n",
    "which is simply stating that $x_1 x_2 \\leq \\big(\\frac{x_1 + x_2}{2}\\big)^2$ which may be verified if we multiply each side by $4$ and then subtract $2x_1x_2$ from each side.  \n",
    "\n",
    "The magic of it is that we now get that entire first column for free. That is, because we proved this for all real $(x_1, x_2)$, these must be the real roots of the derivative of some bigger polynomial with real roots, which must be the real roots of the derivative of some bigger polynomial with real roots, and so on.  \n",
    "\n",
    "Now we attack that next light blue cell on the diagonal, and prove \n",
    "\n",
    "$E_1(\\mathbf x)E_3(\\mathbf x) \\leq E_2(\\mathbf x)^2$ \n",
    "\n",
    "in the case where $r = 3$.   That is we need to prove \n",
    "\n",
    "$\\frac{x_1 + x_2 + x_3}{3}\\big(x_1 x_2 x_3\\big) \\leq \\big(\\frac{x_1 x_2 + x_1 x_3 + x_2 x_3}{3}\\big)^2 $\n",
    "\n",
    "The technique used here is that the bound is trivially true if any $x_i=0$, hence assume none are zero, and multiply each side by $\\big(x_1 x_2 x_3\\big)^{-2}$, which because it is real squared (and not zero) must be positive, and hence not change inequality signs, *and* nicely lines up with the degree of the exponent on the Right Hand Side. This gives us \n",
    "\n",
    "$\\frac{1}{3}\\big(\\frac{1}{x_2 x_3} +\\frac{1}{x_1 x_3} + \\frac{1}{x_1 x_2}\\big) \\leq \\frac{1}{9} \\big(\\frac{1}{x_1 } +\\frac{1}{x_2} + \\frac{1}{x_3}\\big)^2 $\n",
    "\n",
    "multiply each side by 9 and expand the RHS\n",
    "\n",
    "$3\\big(\\frac{1}{x_2 x_3} +\\frac{1}{x_1 x_3} + \\frac{1}{x_1 x_2}\\big) \\leq \\big(\\frac{1}{x_1 } +\\frac{1}{x_2} + \\frac{1}{x_3}\\big)^2 = \\big(\\frac{1}{x_1^2 } +\\frac{1}{x_2^2} + \\frac{1}{x_3^2} + \\frac{2}{x_2 x_3} +\\frac{2}{x_1 x_3} + \\frac{2}{x_1 x_2}\\big)$\n",
    "\n",
    "which simplifies to\n",
    "\n",
    "$\\big(\\frac{1}{x_2 x_3} +\\frac{1}{x_1 x_3} + \\frac{1}{x_1 x_2}\\big) \\leq \\big(\\frac{1}{x_1^2 } +\\frac{1}{x_2^2} + \\frac{1}{x_3^2}\\big)$\n",
    "\n",
    "which should look awfully familiar from the re-arrangement inequality chapter.  In particular, direct application of Cauchy Schwarz gives us \n",
    "\n",
    "$\\big(\\frac{1}{x_2 x_3} +\\frac{1}{x_1 x_3} + \\frac{1}{x_1 x_2}\\big)  = \\big(\\mathbf x^{-1}\\big)^T \\big(\\mathbf P\\mathbf x^{-1}\\big) \\leq \\big \\Vert \\mathbf x^{-1} \\big \\Vert_2 \\big \\Vert \\mathbf {Px}^{-1}\\big \\Vert_2 = \\big \\Vert \\mathbf x^{-1}\\big \\Vert_2 \\big \\Vert \\mathbf {x}^{-1}\\big \\Vert_2 = \\big \\Vert \\mathbf x^{-1}\\big \\Vert_2^2 = \\big(\\frac{1}{x_1^2 } +\\frac{1}{x_2^2} + \\frac{1}{x_3^2}\\big)$  \n",
    "\n",
    "where we used the cyclic permutation matrix $\\mathbf P$. (Note: there is an even more slick way to get to this approach -- something which is dealt with at the end.) \n",
    "\n",
    "We now get all cells below the diagonal in that second column for 'free'.  \n",
    "\n",
    "The final step is, to in general attack all the orange cells along the diagonal.  That is we want to prove \n",
    "\n",
    "$E_{r-2}(\\mathbf x)E_r(\\mathbf x) \\leq E_{r-1}(\\mathbf x)^2$\n",
    "\n",
    "for any natural number $r \\leq n$ \n",
    "\n",
    "The proof seems unwieldly, but in fact uses a similar technique of dividing each side by a squared product.  The *very* clever finish is, after re-arranging and manipulating some inequalities, to then apply what we already have and state the claim must be true because of what we have in that first column also applies to $\\mathbf x^{-1}$, \n",
    "\n",
    "i.e. that $E_{0}(\\mathbf x^{-1})E_2(\\mathbf x^{-1}) \\leq E_{1}(\\mathbf x^{-1})^2$\n",
    "\n",
    "The amount of overlapping subproblems, and symmetry involved in this problem is really quite shocking. \n",
    "\n",
    "\n",
    "There are a couple of ways to interpret this proof.  \n",
    "\n",
    "*The iterative approach*  \n",
    "If we want to prove that all of the lower triangular relations hold, in our above picture, we merely need to prove the top left entry, and march along the diagonal for $n$ iterations in total.  Each time we prove a diagonal entry $j=r$, we automatically get every claim below it in that same column $j$.  Hence once we've done the proof for all $n$ iterations (i.e for each diagonal entry) we have, in effect enumerated, the proof for every component in that lower triangular structure. \n",
    "\n",
    "*The recursive view*  \n",
    "If our ultimate interest is in the bottom row, for any given entry $j$ on that row, we merely need proof that the diagonal entry on $r=j$ (i.e. the entry above it, or equal to it if in the bottom left corner) is true.  Hence by coming up with a general purpose proof for each and every shaded (i.e. diagonal) entry, we have proven everything.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# deeper look at elementary symmetric functions\n",
    "\n",
    "the function $g$ with $\\mathbf x \\in \\mathbb R^n$ with strictly real non-negative components, given by \n",
    "\n",
    "$g_k\\big(\\mathbf x\\big) = e_k\\big(\\mathbf x\\big)^\\frac{1}{k}$  \n",
    "\n",
    "for $k = \\{2,3, ..., n-1, n\\}$  \n",
    "\n",
    "is a (strictly -- tbc) concave function.  The function of course is linear when $k = 1$ and we ignore the $k=0$ case here as not meaningful.  \n",
    "\n",
    "note the two tests for linearity\n",
    "\n",
    "1.)  $\\alpha g_k\\big(\\mathbf x\\big) = g_k\\big(\\alpha \\mathbf x\\big)$  \n",
    "\n",
    "holds due to homogeneity of the exponent, but \n",
    "\n",
    "2.)  $g_k\\big(\\mathbf x_1\\big) + g_k\\big(\\mathbf x_2\\big) \\leq g_k\\big(\\mathbf x_1 + \\mathbf x_2\\big)$  \n",
    "because the functions are superadditive \n",
    "\n",
    "we have proven this in multiple ways for the $k =n $ case (superadditive of geometric mean).   \n",
    "For other values of k, we may consult page 35 of Beckenbach and Bellman's *Inequalities*.  (The background to the proof is elementary but requires  considerable symbol manipulation in its build-up -- your author is looking for an elementary but much more succinct approach.)  \n",
    "\n",
    "The key idea, however, is that the above two relationship characterize elementrary symmetric functions as being negative convex, aka concave.  \n",
    "\n",
    "specifically consider real non-negative weights $w_1, w_2$ where $w_1 + w_2 = 1$, the above two properties give us \n",
    "\n",
    "$w_1 g_k\\big(\\mathbf x_1\\big) + w_2 g_k\\big(\\mathbf x_2\\big) = g_k\\big(w_1 \\mathbf x_1\\big) + g_k\\big(w_2 \\mathbf x_2\\big) \\leq g_k\\big(w_1\\mathbf x_1 + w_2 \\mathbf x_2\\big)$  \n",
    "\n",
    "which is our definition for a scalar valued function.  From here we may apply Jensen's Inequality, etc as we please. \n",
    "\n",
    "**sneak peak to chapter 13:**  \n",
    "\n",
    "As the name implies and the function's defintion shows, these are symmetric functions. That is, any permutation of the functions arguments / vector, does not change the result.  \n",
    "\n",
    "so suppose we had \n",
    "\n",
    "$\\mathbf x_2 = w_1 \\mathbf I \\mathbf x_1 + w_2 \\mathbf P \\mathbf x_1 = w_1 \\mathbf x_1 + w_2 \\mathbf P \\mathbf x_1$  \n",
    "\n",
    "so $\\mathbf x_2  \\preceq \\mathbf x_1$ or we may say or $\\mathbf x_2 \\in H(\\mathbf x_1)$   \n",
    "\n",
    "(i.e. $\\mathbf x_2$ is in the convex hull of $\\mathbf x_1$  )\n",
    "\n",
    "where $\\mathbf P$ is some (non-identity) permutation matrix  \n",
    "\n",
    "- - - -\n",
    "note this may be restated as, there exists some doubly stochastic matrix $\\mathbf D$ where \n",
    "$\\mathbf D := w_1 \\mathbf I + w_2 \\mathbf P$   \n",
    "\n",
    "so $\\mathbf x_2 = \\mathbf D \\mathbf x_1$  \n",
    "- - - -\n",
    "because $g$ is a symmetric function we know $g\\Big(\\big(\\mathbf P \\mathbf x_1\\big)\\Big) = g\\Big(\\mathbf x_1\\Big)$  \n",
    "\n",
    "\n",
    "in the contex of majorization this tells us \n",
    "\n",
    "$ g_k\\Big( \\mathbf x_1 \\Big) = (w_1 + w_2) g_k\\Big( \\mathbf x_1 \\Big) =w_1 g_k\\Big( \\mathbf x_1 \\Big) + w_2 g_k\\Big(\\mathbf x_1\\Big) = w_1 g_k\\Big( \\mathbf x_1 \\Big) + w_2 g_k\\Big( \\big(\\mathbf P \\mathbf x_1\\big)\\Big) = g_k\\Big(w_1 \\mathbf x_1 \\Big) + g_k\\Big(w_2 \\big(\\mathbf P \\mathbf x_1\\big)\\Big)$   \n",
    "$ \\leq g_k\\Big(w_1\\mathbf x_1 + w_2 \\big(\\mathbf P \\mathbf x_1\\big)\\Big) =  g_k\\Big(w_1\\mathbf x_1 + w_2 \\mathbf P \\mathbf x_1\\Big) = g_k\\Big(\\mathbf x_2\\Big)$  \n",
    "\n",
    "thus in the case of majorization, we have \n",
    "\n",
    "$g_k\\big( \\mathbf x_1 \\big) \\leq  g_k\\big(\\mathbf x_2\\big)$  \n",
    "\n",
    "since there is only one term on each side, (again, recalling that everything is real non-negative), we may safely raise each side to the $k$th power giving us  \n",
    "\n",
    "$e_k\\big(\\mathbf x_1\\big) =   g_k\\big( \\mathbf x_1 \\big)^k \\leq  g_k\\big(\\mathbf x_2\\big)^k = e_k\\big(\\mathbf x_2\\big)$  \n",
    "\n",
    "which shows that the kth elementary symmetric function is Schur Concave \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An almost immediate consequence of the above insight is \n",
    "\n",
    "# Minkowki's Determinant Inequality \n",
    "\n",
    "Let $\\mathbf A$ and $\\mathbf B$ be $n$ x $n$  Hermitian positive (semi)definite matrices.  It's immediate that $\\big(\\mathbf A +\\mathbf B\\big)$ is Hermitian and positive(semi) definite.  Let $\\mathbf x_1$, $\\mathbf x_2$ and $\\mathbf x_3$ have the eigenvalues for $\\mathbf A$, $\\mathbf B$, and $\\big(\\mathbf A +\\mathbf B\\big)$ respectively. \n",
    "\n",
    "The is a majorization relation here, because  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A\\big) + \\text{trace}\\big(\\mathbf B\\big) = \\text{trace}\\big(\\mathbf A +\\mathbf B\\big)$  \n",
    "\n",
    "but two choices are better than one when running recovering our eigenvalues via constrained quadratic form maximization problems, so it is immediate that \n",
    "\n",
    "$\\mathbf x_3 \\preceq \\mathbf x_1 + \\mathbf x_2$  \n",
    "\n",
    "Minkowki's inequality is \n",
    "\n",
    "$\\text{det}\\big(\\mathbf A\\big)^\\frac{1}{n} + \\text{det}\\big(\\mathbf B\\big)^\\frac{1}{n} \\leq \\text{det}\\big(\\mathbf A +\\mathbf B\\big)^\\frac{1}{n}$  \n",
    "\n",
    "or equivalently in term of eigenvalues, we may prove it in one line:  \n",
    "\n",
    "$g_n\\big(\\mathbf x_1\\big) + g_n\\big(\\mathbf x_2\\big) \\leq g_n\\big(\\mathbf x_1 + \\mathbf x_2\\big) \\leq g_n\\big(\\mathbf x_3\\big)$  \n",
    "\n",
    "where the first inequality is true by the super additivity of the geometric mean and the second inequality is true because of the Schur concativity of $e_n$ aka the determinant function --i.e. we could have written it as $g_n\\big(\\mathbf x_1 + \\mathbf x_2\\big)^n \\leq g_n\\big(\\mathbf x_3\\big)^n$ -- *or* as shown above working directly with $g_n$ with Birkhoff's theorem in hand the super additivity of the geometric mean implies this result.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there is an *more elementary approach to proving Minkowki's Inequality* that merely makes use of super additivity of the geometric mean and a clever factorization.  This is shown at the end of \"determinant_addtion_two_matrices_inequality.ipynb\" in the Linear Algebra folder.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Determinant is concave for Hermitian Positive Definite matrices  \n",
    "\n",
    "Let $\\mathbf A$ and $\\mathbf B$ $n$ x $n$ Hermitian positive definite matrices.  \n",
    "\n",
    "consider the function $f$ that is the (natural) log of the determinant of an HPD matrix \n",
    "\n",
    "i.e. \n",
    "\n",
    "$f\\Big(\\mathbf X\\Big) = \\log\\Big( \\det\\big(\\mathbf X\\big)\\Big)$  \n",
    "\n",
    "**claim:**   \n",
    "for any $p \\in (0,1)$ \n",
    "\n",
    "$ p \\cdot f\\Big(\\mathbf A\\Big) + (1-p)\\cdot f\\Big(\\mathbf B\\Big) \\leq f\\Big(p\\cdot \\mathbf A + (1-p)\\cdot \\mathbf B\\Big)  $ \n",
    "\n",
    "hence $f$ is concave \n",
    "\n",
    "where   \n",
    "\n",
    "**tbc**: equality iff $\\mathbf A = \\mathbf B$  \n",
    "\n",
    "(note: this is also problem in Cover and Thomas and is proven there via tools from probability and information theory.  Understanding that proof as well is of considerable interest.)  \n",
    "\n",
    "we prove the above by proving a generalization of $\\text{GM} \\leq \\text{AM}$\n",
    "\n",
    "**claim**  \n",
    "\n",
    "$\\det\\big(\\mathbf A^p \\mathbf B^{(1-p)}\\big) \\leq \\det\\big(p \\mathbf A + (1-p) \\mathbf B\\big)$\n",
    "\n",
    "- - - - \n",
    "for avoidance of doubt, the desired inequality folows by splitting the determinant on LHS to get \n",
    "\n",
    "$\\det\\big(\\mathbf A\\big)^p\\det \\big( \\mathbf B\\big)^{(1-p)}= \\det\\big(\\mathbf A^p\\big)\\det \\big( \\mathbf B^{(1-p)}\\big) \\leq \\det\\big(p \\mathbf A + (1-p) \\mathbf B\\big)$\n",
    "\n",
    "and taking natural logarithms to get \n",
    "\n",
    "$ p \\cdot f\\Big(\\mathbf A\\Big) + (1-p)\\cdot f\\Big(\\mathbf B\\Big) =\\log\\Big(\\det\\big(\\mathbf A\\big)\\Big)^p +\\log\\Big(\\det \\big( \\mathbf B\\big)\\Big)^{(1-p)} =\\log\\Big(\\det\\big(\\mathbf A\\big)^p\\det \\big( \\mathbf B\\big)^{(1-p)}\\Big)$  $\\leq \\log\\Big(\\det\\big(p \\mathbf A + (1-p) \\mathbf B\\big)\\Big)= f\\Big(p\\cdot \\mathbf A + (1-p)\\cdot \\mathbf B\\Big) $\n",
    "- - - - \n",
    "**proof:** \n",
    "\n",
    "similar to the above problem, let $\\mathbf x_1$, $\\mathbf x_2$ and $\\mathbf x_3$ have the eigenvalues for $\\mathbf A$, $\\mathbf B$, and $\\big(p\\mathbf A + (1-p)\\mathbf B\\big)$  \n",
    "As a reminder, all eigenvalues are strictly positive, and well ordered, such that for $\\mathbf x_k$ we have   \n",
    "$x_{k,1}\\geq x_{k,2} \\geq ...\\geq x_{k, n-1} \\geq x_{k,n}$  \n",
    "\n",
    "\n",
    "by $\\text{GM} \\leq \\text{AM}$ we set up the point-wise bound  \n",
    "\n",
    "$0\\lt x_{1,i}^p  x_{2,i}^{(1-p)} \\leq p\\cdot x_{1,i} + (1-p)x_{2,i}$  \n",
    "\n",
    "taking advantage of positivity, we multiply over the bound and see \n",
    "\n",
    "$0\\lt \\prod_{i=1}^n \\big(x_{1,i}^p  x_{2,i}^{(1-p)}\\big) \\leq \\prod_{i=1}^n \\big(p\\cdot x_{1,i} + (1-p)x_{2,i}\\big)$  \n",
    "\n",
    "or more clearly, in vector notation, where the exponent (and inequality) is evaluated component wise for each row, and $\\circ$ denotes hadamard product:     \n",
    "\n",
    "$\\big(\\mathbf x_1^p\\big) \\circ \\big(\\mathbf x_2^{(1-p)}\\big) \\leq p\\cdot \\mathbf x_1 + (1-p)\\mathbf x_2$  \n",
    "\n",
    "hence we have \n",
    "\n",
    "$\\det\\Big(\\mathbf A^p \\mathbf B^{(1-p)}\\Big) = \\det\\Big(\\mathbf A^p\\Big)\\det\\Big( \\mathbf B^{(1-p)}\\Big) =e_n\\Big(\\mathbf x_1^p\\Big) \\cdot e_n \\Big(\\mathbf x_2^{(1-p)}\\Big) = e_n\\Big(\\big(\\mathbf x_1^p\\big) \\circ \\big(\\mathbf x_2^{(1-p)}\\big)\\Big) $  \n",
    "$\\leq e_n\\Big(p\\cdot \\mathbf x_1 + (1-p)\\mathbf x_2\\Big) \\leq e_n\\Big(\\mathbf x_3\\Big) = \\det\\Big(p \\mathbf A + (1-p) \\mathbf B\\Big)$   \n",
    "\n",
    "where we've used the fact that \n",
    "\n",
    "$\\big(p\\cdot \\mathbf x_1 + (1-p)\\mathbf x_2 \\big)\\succeq \\mathbf x_3$  \n",
    "\n",
    "and $e_n$ is Schur Concave  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below seems to still need some work, unfortunately..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# another look at 12.7\n",
    "\n",
    "can I do something like this for the 3 positive variables case to show that the bound isn't tight?\n",
    "\n",
    "$\\frac{1}{4} \\leq x^3 + y^3 + z^3 + 6xyz$  \n",
    "\n",
    "where $x + y + z = 1$\n",
    "\n",
    "$LHS = \\frac{1}{4}\\big(\\frac{1}{GM}\\big)^3 = \\frac{1}{4xyz} \\leq \\frac{x^2}{yz} + \\frac{y^2}{xz} + \\frac{z^2}{xy} + 6 = RHS$  \n",
    "\n",
    "What I want to do with the RHS, hopefully using Muirhead, though I'm not yet sure its justified is do something like, \n",
    "\n",
    "$7 = x + y + z + 6 \\leq \\frac{x^2}{yz} + \\frac{y^2}{xz} + \\frac{z^2}{xy} + 6 = RHS$  \n",
    "\n",
    "and then lower bound the LHS (see power means and Hoelders revisit of it notes, or my POTW answer)\n",
    "\n",
    "$\\frac{27}{4} = \\frac{1}{4}\\Big(\\frac{3}{1}\\Big)^3 = \\frac{1}{4}\\Big(\\frac{1}{\\frac{1}{3}(x+y+z)}\\Big)^3 \\leq \\frac{1}{4}\\big(\\frac{1}{GM}\\big)^3 = LHS $\n",
    "\n",
    "and then say that \n",
    "$\\frac{27}{4} \\leq \\frac{28}{4} = 7$ \n",
    "\n",
    "or something like that... hence the 3 variable case has at least one situation where the inequality is rather loose, though I guess that isn't what I really want here?  \n",
    "\n",
    "it's strange how much I'm spinning my wheels on this problem without satisfactory answer.  Is it really just that tricky?  Or am I not visualizing inequalities very well?  \n",
    "\n",
    "\n",
    "Maybe I should revisit this after locking down majorization and schur convexity?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.8  \n",
    "\n",
    "The official solution uses a telescoping identity to prove 12.8.  The telescoping identity itself was, unfortunately, not at all obvious to your author.  As such the bulk of this solution is working out and proving the telescoping identity.  The below workthrough is instructive for a few reasons.  However, there probably is a simpler / cleaner approach to proving / deriving the telescoping identity.  Note: as with many of the items in this chapter, there is a distinctive symmetry and permutations flavor to this exercise.  In particular this exercise has cyclic permutations.  I have lingering suspicions that there are more insights to be had via group theory here.  \n",
    "\n",
    "\n",
    "$\\mathbf a := \\begin{bmatrix}\n",
    "a_1 \\\\ \n",
    "a_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "a_n\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "however, the ordering in $\\mathbf b$ is flipped (perhaps via application of the reflection matrix $\\mathbf J$)\n",
    "\n",
    "$\\mathbf b  := \\begin{bmatrix}\n",
    "b_n \\\\ \n",
    "b_{n-1} \\\\ \n",
    "\\vdots \\\\ \n",
    "b_1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "now consider the function given by \n",
    "\n",
    "$f\\big(\\mathbf a, k\\big) = a_1 a_2... a_{k-1}a_k$  \n",
    "\n",
    "which also means \n",
    "\n",
    "$f\\big(\\mathbf b, k\\big) = b_n b_{n-1}... b_{n-k + 2} b_{n-k +1} =  b_{n-k +1}b_{n-k + 2}...b_{n-1} b_{n}$  \n",
    "\n",
    "\n",
    "where we use the conventions that \n",
    "\n",
    "$f\\big(\\mathbf a, 0 \\big) = 1$  and $f\\big(\\mathbf b, 0 \\big) = 1$  \n",
    "- - - - - \n",
    "\n",
    "\n",
    "also consider the Lie Bracket $\\big[y, z\\big] = y - z$ \n",
    "\n",
    "our goal is to prove\n",
    "\n",
    "$a_1 a_2 ... a_n - b_1 b_2 ... b_n = \\sum_{r=1}^n \\big(\\prod_{k=1}^{r-1} a_k\\big) \\big(a_r - b_r\\big)\\big(\\prod_{k=r+1}^n b_k \\big)$  \n",
    "\n",
    "Consider:  \n",
    "\n",
    "$LHS = \\big[f\\big(\\mathbf a, n\\big), f\\big(\\mathbf b, n\\big)\\big]  = \\big(a_1 a_2 ... a_n\\big) - \\big(b_1 b_2 ... b_n\\big) $ \n",
    "\n",
    "in long form we have:  \n",
    "\n",
    "\n",
    "$RHS = \\big[f\\big(\\mathbf a, 1\\big)f\\big(\\mathbf b, n-1\\big), f\\big(\\mathbf a, 0\\big)f\\big(\\mathbf b, n\\big)\\big] + \\big[f\\big(\\mathbf a, 2\\big)f\\big(\\mathbf b, n-2\\big), f\\big(\\mathbf a, 1\\big)f\\big(\\mathbf b, n-1\\big)\\big] + \\big[f\\big(\\mathbf a, 3\\big)f\\big(\\mathbf b, n-3\\big), f\\big(\\mathbf a, 2\\big)f\\big(\\mathbf b, n-2\\big)\\big] + ... + \\big[f\\big(\\mathbf a, n\\big)f\\big(\\mathbf b, 0\\big), f\\big(\\mathbf a, n-1\\big)f\\big(\\mathbf b, 1\\big)\\big]$\n",
    "\n",
    "- - - - \n",
    "and in condensed form we have:  \n",
    "\n",
    "$RHS = \\sum_{r=1}^n \\big[f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big), f\\big(\\mathbf a, r-1\\big)f\\big(\\mathbf b, n-(r-1)\\big)\\big] $ \n",
    "\n",
    "$ RHS = \\sum_{r=1}^n f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big) - f\\big(\\mathbf a, r-1\\big)f\\big(\\mathbf b, n-(r-1)\\big)$ \n",
    "\n",
    "$ RHS= \\Big(\\sum_{r=1}^n f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big) - \\Big(\\sum_{r=1}^n f\\big(\\mathbf a, r-1\\big)f\\big(\\mathbf b, n-(r-1)\\big)\\Big)$\n",
    "\n",
    "switching indices for the second sum\n",
    "\n",
    "$ RHS = \\Big(\\sum_{r=1}^n f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big) - \\Big(\\sum_{r=0}^{n-1} f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big)$ \n",
    "\n",
    "$RHS= f\\big(\\mathbf a, n\\big)f\\big(\\mathbf b, 0\\big) + \\Big(\\sum_{r=1}^{n-1} f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big) - \\Big(\\sum_{r=1}^{n-1} f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big)\\Big) - f\\big(\\mathbf a, 0\\big)f\\big(\\mathbf b, n\\big) $\n",
    "\n",
    "$RHS= f\\big(\\mathbf a, n\\big)f\\big(\\mathbf b, 0\\big) - f\\big(\\mathbf a, 0\\big)f\\big(\\mathbf b, n\\big)=  \\big[f\\big(\\mathbf a, n\\big), f\\big(\\mathbf b, n\\big)\\big]  = \\big(a_1 a_2 ... a_n\\big) - \\big(b_1 b_2 ... b_n\\big)  = LHS$ \n",
    "\n",
    "\n",
    "We thus have \n",
    "\n",
    "$\\big(a_1 a_2 ... a_n\\big) - \\big(b_1 b_2 ... b_n\\big) = \\sum_{r=1}^n \\big[f\\big(\\mathbf a, r\\big)f\\big(\\mathbf b, n-r\\big), f\\big(\\mathbf a, r-1\\big)f\\big(\\mathbf b, n-(r-1)\\big)\\big]  = \\sum_{r=1}^n \\big(\\prod_{k=1}^{r-1} a_k\\big) \\big(a_r - b_r\\big)\\big(\\prod_{k=r+1}^n b_k \\big)$  \n",
    "\n",
    "- - - - \n",
    "*comment: there must be a smoother / easier way to show this telescoping identiy*  \n",
    "\n",
    "with this out of the way, the Weierstrass inequality, which is true over complex numbers where each $\\big \\vert a_k\\big \\vert \\leq 1$ and $\\big \\vert b_k\\big \\vert \\leq 1$\n",
    " \n",
    "is given by:  \n",
    "\n",
    "where, again \n",
    "\n",
    "$LHS = \\big(a_1 a_2 ... a_n\\big) - \\big(b_1 b_2 ... b_n\\big)$ \n",
    "\n",
    "$\\Big \\vert LHS \\Big \\vert = \\Big \\vert \\sum_{r=1}^n \\big(\\prod_{k=1}^{r-1} a_k\\big) \\big(a_r - b_r\\big)\\big(\\prod_{k=r+1}^n b_k \\big) \\Big \\vert \\leq \\sum_{r=1}^n\\Big \\vert  \\big(\\prod_{k=1}^{r-1} a_k\\big)\\Big \\vert  \\Big \\vert \\big(a_r - b_r\\big)\\Big \\vert  \\Big \\vert \\big(\\prod_{k=r+1}^n b_k \\big) \\Big \\vert \\leq \\sum_{r=1}^n \\Big \\vert \\big(a_r - b_r\\big)\\Big \\vert $  \n",
    "\n",
    "where we first apply triangle inequality, and then recall that the product of numbers with magnitude at most 1, itself has a magnitude at most 1.  \n",
    "\n",
    "Thus we've proven\n",
    "\n",
    "$\\Big \\vert a_1 a_2 ... a_n - b_1 b_2 ... b_n \\Big \\vert  \\leq \\sum_{r=1}^n \\Big \\vert a_r - b_r \\Big \\vert $ \n",
    "\n",
    "where each $\\big \\vert a_k\\big \\vert \\leq 1$ and $\\big \\vert b_k\\big \\vert \\leq 1$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# extension:  \n",
    "\n",
    "for a particular challenge problem:  \n",
    "\n",
    "\n",
    "which in general can be written as something like:  \n",
    "\n",
    "$\\prod_{i=1}^n x_i^{\\frac{1}{n}\\sum_{i =1}^n x_i} \\leq \\prod_{i=1}^n x_i^{x_i} $   \n",
    "\n",
    "where each $x_i \\gt 0$ \n",
    "\n",
    "and without loss of generality, we may assume:  \n",
    "$ x_1 \\geq x_2 \\geq ... \\geq x_n \\gt 0$   \n",
    "\n",
    "\n",
    "we can prove it as follows:  \n",
    "\n",
    "$\\prod_{i=1}^n x_i^{\\frac{1}{n}\\sum_{i =1}^n x_i} \\leq\\frac{1}{n!}  \\big(\\sum_{\\sigma \\in S} \\prod_{i=1}^n x_{\\sigma(k)}^{x_i}   \\big) \\leq \\text{maximal product amongs all permutations} = \\prod_{i=1}^n x_i^{x_i} $   \n",
    "\n",
    "while there are some notational challenges, the inequality from left to center *is* Muirhead's Inequality, i.e. the exponents on the LHS are in the convex hull of that in the middle\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\frac{1}{n}\\sum_{i =1}^n x_i\\\\ \n",
    "\\frac{1}{n}\\sum_{i =1}^n x_i\\\\ \n",
    "\\vdots\\\\ \n",
    "\\frac{1}{n}\\sum_{i =1}^n x_i\\\\ \n",
    "\\frac{1}{n}\\sum_{i =1}^n x_i\n",
    "\\end{bmatrix} \\preceq \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "x_2 \\\\ \n",
    "\\vdots\\\\ \n",
    "x_{n-1}\\\\ \n",
    "x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and there is one possible permutation of the LHS, whereas there are $n!$ permutations in the middle.  Per Muirhead's Inequality, we know that this expected sum over all permutations is at least as big as the expected sum over all permutations of that product in its convex hull.  \n",
    "\n",
    "The center to RHS inequality \n",
    "$  \\frac{1}{n!}  \\big(\\sum_{\\sigma \\in S} \\prod_{i=1}^n x_{\\sigma(k)}^{x_i}   \\big) \\leq \\text{maximal product amongs all permutations}$ \n",
    "\n",
    "is stated as the expected value over all permutations of the product is less than or equal to the maximal product.  \n",
    "- - - -\n",
    "*Final leg: *    \n",
    "\n",
    "Here are 3 ways to prove that \n",
    "\n",
    "$\\text{maximal product amongs all permutations} = \\prod_{i=1}^n x_i^{x_i}$.  \n",
    "\n",
    "1.)  \n",
    "\n",
    "It's obvious -- basically via a greedy strategy.  (Ok maybe not quite obvious enough.)\n",
    "\n",
    "2.) \n",
    "\n",
    "taking advantange of positivity, re-write with the exponential map as  \n",
    "$\\prod_{i=1}^n \\big(x_i\\big)^{x_i} = \\prod_{i=1}^n \\big(e^{\\gamma_i}\\big)^{x_i}  = \\prod_{i=1}^n e^{x_i \\gamma_i} = e^{\\sum_{i=1}^n \\gamma_i x_i}$   \n",
    "\n",
    "and recognize that the $\\gamma_i$'s are in the same ordering as the $x_i$'s, hence   \n",
    "$\\sum_{i=1}^n \\gamma_i x_i \\geq \\sum_{i=1}^n \\gamma_{\\sigma(i)} x_i$  \n",
    "by the re-arrangment inequality\n",
    "\n",
    "3.) \n",
    "\n",
    "Use a slightly modified form of KL Divergence.  Specifically, consider the result that for positive $s_k$ and positive $t_k$, \n",
    "\n",
    "where $\\sum_{k} s_k = \\sum_{k} t_k = c \\gt 0$   \n",
    "\n",
    "we have a (modified) divergence of \n",
    "\n",
    "$\\sum_{k} - s_k \\log\\big(\\frac{t_k}{s_k}\\big)\\geq 0$  \n",
    "\n",
    "proof: \n",
    "multiply each side by $\\frac{1}{c}$  \n",
    "\n",
    "$\\sum_{k} - \\frac{s_k}{c} \\log\\big(\\frac{t_k}{s_k}\\big)\\geq 0$  \n",
    "\n",
    "notice that $-\\log$ is a strictly convex function, and $\\big(\\frac{s_k}{c}\\big)$ can be interpretted as a probability.  Applying Jensen's Inequality, we have \n",
    "\n",
    "$\\sum_{k} - \\frac{s_k}{c} \\log\\big(\\frac{t_k}{s_k}\\big) = E\\Big[-\\log \\big(\\frac{T}{S}\\big)\\Big] \\geq -\\log\\Big(E\\big[\\frac{T}{S}\\big]\\Big) = -\\log \\Big(\\sum_{k} \\frac{s_k}{c} \\frac{t_k}{s_k}\\Big) = -\\log\\Big(\\frac{1}{c} \\sum_{k} t_k\\Big) = -\\log\\Big(\\frac{1}{c} c \\Big) = -\\log\\Big(1 \\Big)  = 0 $  \n",
    "\n",
    "with equality iff $t_k = (\\alpha) s_k$   \n",
    "\n",
    "i.e. the fraction $\\frac{t_k}{s_k} = \\alpha$\n",
    "\n",
    "where $\\alpha$ is some constant.  \n",
    "\n",
    "However because their sums are the same, this equality case means \n",
    "\n",
    "$0 \\lt c = \\sum_{k} (t_k) =  \\sum_{k} (\\alpha  s_k)  = \\alpha \\cdot \\big(\\sum_{k} s_k\\big) = \\alpha \\cdot c$ \n",
    "\n",
    "if we divide everything by $c$ or equivalently by $\\big(\\sum_{k} s_k \\big)$, then we see that it must be the case that $\\alpha = 1 $, which means the lower bound of 0 is achievable **iff** $t_k  = s_k$  for each $k$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we re-visit our modified divergence inequality, we see:  \n",
    "\n",
    "\n",
    "$\\sum_{k} - s_k \\log\\big(\\frac{t_k}{s_k}\\big)\\geq 0$  can be re-written as \n",
    "\n",
    "$\\sum_{k} - \\Big(s_k \\log\\big(\\frac{t_k}{s_k}\\big)\\Big) = \\sum_{k} -\\Big(s_k \\log\\big(t_k\\big) - s_k log\\big(s_k\\big)\\Big) = \\Big(\\sum_{k} -s_k \\log\\big(t_k\\big)\\Big) + \\Big(\\sum_{k}  s_k \\log\\big(s_k\\big)\\Big) \\geq 0$\n",
    "\n",
    "which is equivalent to  \n",
    "\n",
    "$\\sum_{k}  \\log\\big(s_k^{s_k} \\big)  = \\sum_{k}  s_k \\log\\big(s_k\\big) \\geq   \\sum_{k} s_k \\log\\big(t_k\\big)= \\sum_{k}  \\log\\big(t_k^{s_k} \\big)$  \n",
    "\n",
    "which is equivalent to  \n",
    "\n",
    "$ \\log\\big(\\prod_{k} s_k^{s_k}\\big) = \\sum_{k}  \\log\\big(s_k^{s_k}\\big) \\geq \\sum_{k} \\log\\big(t_k^{s_k}\\big) =  \\log\\big(\\prod_{k} t_k^{s_k}\\big)$  \n",
    "\n",
    "taking advantage of positivitiy, if we exponentiate both sides, we see \n",
    "\n",
    "$\\prod_{k} s_k^{s_k} \\geq \\prod_{k} t_k^{s_k}$    \n",
    "\n",
    "again where $s_k \\gt 0$ and $t_k \\gt 0$ and $\\sum_k s_k = \\sum_k t_k $ \n",
    "- - - - \n",
    "Finally, to tie out any loose ends, \n",
    "\n",
    "consider the function $g$ where \n",
    "\n",
    "$g\\big(\\mathbf a , \\mathbf x \\big) = \\prod_{k} a_k^{x_k}$     \n",
    "\n",
    "then we have \n",
    "\n",
    "$g\\Big(\\mathbf x, \\mathbf x \\Big) = \\prod_{k} x_k^{x_k} \\geq  \\prod_{i=1}^n x_{\\sigma(k)}^{x_i}    = g\\Big(\\mathbf {Px}, \\mathbf x \\Big)$  \n",
    "\n",
    "for any permutation matrix $\\mathbf P$  \n",
    "\n",
    "- - - - \n",
    "alternative solution:  \n",
    "\n",
    "a very quick, very nice approach is to more explicitly use some ideas from Majorization: specifically Birkoff's theorem on doubly stochastic matrices, and a re-arrangement inequality.  \n",
    "\n",
    "$\\prod_{i=1}^n x_i^{\\frac{1}{n}\\sum_{i =1}^n x_i} = \\prod_{i=1}^n x_i^{\\mu} \\leq \\prod_{i=1}^n x_i^{x_i} $\n",
    "\n",
    "where taking advantage of positivity:  \n",
    "\n",
    "$e^{\\gamma_i}  := x_i $  \n",
    "\n",
    "$\\prod_{i=1}^n x_i^{\\mu} = \\exp\\big(\\sum_{i=1}^n \\gamma_i \\mu \\big)\\leq  \\exp\\big(\\sum_{i=1}^n \\gamma_i x_i \\big) = \\prod_{i=1}^n x_i^{x_i} $\n",
    "\n",
    "hence we merely need to prove \n",
    "\n",
    "$\\exp\\big(\\sum_{i=1}^n \\gamma_i \\mu \\big)\\leq  \\exp\\big(\\sum_{i=1}^n \\gamma_i x_i \\big) $ \n",
    " \n",
    "or equivalently \n",
    "\n",
    "$\\sum_{i=1}^n \\gamma_i \\mu \\leq  \\sum_{i=1}^n \\gamma_i x_i $\n",
    "\n",
    "this is true by a rearrangment inequality i.e. because $\\mathbf \\mu$ is majorizated by $\\mathbf x$ we have a doubly stochastic matrix $A$  \n",
    "\n",
    "where $\\mathbf u = A \\mathbf x$ \n",
    "\n",
    "and by Birkhofff's Theorem we know that $A$ is a convex combination of Permutations matrices (and from there the rearrangement inequality is called)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
